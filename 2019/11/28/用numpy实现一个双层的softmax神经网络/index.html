<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans,en,default">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">






  
  
  <link rel="stylesheet" media="all" href="/lib/Han/dist/han.min.css?v=3.3">




<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/blacksheepOnbike_little.ico?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/blacksheepOnbike.svg?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/blacksheepOnbike.png?v=5.1.4">


  <link rel="mask-icon" href="/images/blacksheepOnbike.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="微机课设,numpy,神经网络,MNIST," />










<meta name="description" content="使用的数值化数据集：  链接：https:&#x2F;&#x2F;pan.baidu.com&#x2F;s&#x2F;110dhDKA8eXYV4-kfmevo0g  提取码：x3wu  整体思路使用Numpy和pandas写一个神经网络，进行手写数字识别(MNIST)，样本图如下：  总思路整体神经网络代码只使用Numpy和pandas，根据额外需要使用time和matplotlib，设计思路如下 数据处理-&amp;gt; 建立模型">
<meta name="keywords" content="微机课设,numpy,神经网络,MNIST">
<meta property="og:type" content="article">
<meta property="og:title" content="用numpy实现一个双层的softmax神经网络">
<meta property="og:url" content="https:&#x2F;&#x2F;1305936314.github.io&#x2F;2019&#x2F;11&#x2F;28&#x2F;%E7%94%A8numpy%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E5%8F%8C%E5%B1%82%E7%9A%84softmax%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&#x2F;">
<meta property="og:site_name" content="一个充满感情的捕咕人">
<meta property="og:description" content="使用的数值化数据集：  链接：https:&#x2F;&#x2F;pan.baidu.com&#x2F;s&#x2F;110dhDKA8eXYV4-kfmevo0g  提取码：x3wu  整体思路使用Numpy和pandas写一个神经网络，进行手写数字识别(MNIST)，样本图如下：  总思路整体神经网络代码只使用Numpy和pandas，根据额外需要使用time和matplotlib，设计思路如下 数据处理-&amp;gt; 建立模型">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https:&#x2F;&#x2F;1305936314.github.io&#x2F;2019&#x2F;11&#x2F;28&#x2F;%E7%94%A8numpy%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E5%8F%8C%E5%B1%82%E7%9A%84softmax%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&#x2F;output_13_0.png">
<meta property="og:image" content="https:&#x2F;&#x2F;1305936314.github.io&#x2F;2019&#x2F;11&#x2F;28&#x2F;%E7%94%A8numpy%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E5%8F%8C%E5%B1%82%E7%9A%84softmax%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&#x2F;propagation.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;1305936314.github.io&#x2F;2019&#x2F;11&#x2F;28&#x2F;%E7%94%A8numpy%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E5%8F%8C%E5%B1%82%E7%9A%84softmax%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&#x2F;costs.png">
<meta property="og:image" content="https:&#x2F;&#x2F;1305936314.github.io&#x2F;2019&#x2F;11&#x2F;28&#x2F;%E7%94%A8numpy%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E5%8F%8C%E5%B1%82%E7%9A%84softmax%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&#x2F;accuracy.png">
<meta property="og:image" content="https:&#x2F;&#x2F;1305936314.github.io&#x2F;2019&#x2F;11&#x2F;28&#x2F;%E7%94%A8numpy%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E5%8F%8C%E5%B1%82%E7%9A%84softmax%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&#x2F;propagation.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;1305936314.github.io&#x2F;2019&#x2F;11&#x2F;28&#x2F;%E7%94%A8numpy%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E5%8F%8C%E5%B1%82%E7%9A%84softmax%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&#x2F;cost.png">
<meta property="og:updated_time" content="2019-12-13T13:52:13.095Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https:&#x2F;&#x2F;1305936314.github.io&#x2F;2019&#x2F;11&#x2F;28&#x2F;%E7%94%A8numpy%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E5%8F%8C%E5%B1%82%E7%9A%84softmax%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&#x2F;output_13_0.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"right","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://1305936314.github.io/2019/11/28/用numpy实现一个双层的softmax神经网络/"/>





  <title>用numpy实现一个双层的softmax神经网络 | 一个充满感情的捕咕人</title>
  





  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?83d5c2a43c4d3ff2fa03ee5acbd4845b";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-right page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">一个充满感情的捕咕人</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" target="_blank" rel="noopener" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://1305936314.github.io/2019/11/28/%E7%94%A8numpy%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E5%8F%8C%E5%B1%82%E7%9A%84softmax%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="BlackSheepX">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/Faceimage.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="一个充满感情的捕咕人">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">用numpy实现一个双层的softmax神经网络</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-11-28T00:37:52+08:00">
                2019-11-28
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/deeplearning/" itemprop="url" rel="index">
                    <span itemprop="name">deeplearning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/11/28/%E7%94%A8numpy%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E5%8F%8C%E5%B1%82%E7%9A%84softmax%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/11/28/%E7%94%A8numpy%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E5%8F%8C%E5%B1%82%E7%9A%84softmax%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2019/11/28/%E7%94%A8numpy%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E5%8F%8C%E5%B1%82%E7%9A%84softmax%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" class="leancloud_visitors" data-flag-title="用numpy实现一个双层的softmax神经网络">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        <blockquote>
<p>  使用的数值化数据集：<br>  链接：<a href="https://pan.baidu.com/s/110dhDKA8eXYV4-kfmevo0g" target="_blank" rel="noopener">https://pan.baidu.com/s/110dhDKA8eXYV4-kfmevo0g</a><br>  提取码：x3wu</p>
</blockquote>
<h1 id="整体思路"><a href="#整体思路" class="headerlink" title="整体思路"></a>整体思路</h1><p>使用Numpy和pandas写一个神经网络，进行手写数字识别(MNIST)，样本图如下：</p>
<img src="/2019/11/28/%E7%94%A8numpy%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E5%8F%8C%E5%B1%82%E7%9A%84softmax%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/output_13_0.png" class="" title="手写数字5">
<h1 id="总思路"><a href="#总思路" class="headerlink" title="总思路"></a>总思路</h1><p>整体神经网络代码只使用Numpy和pandas，根据额外需要使用time和matplotlib，设计思路如下</p>
<p>数据处理-&gt; 建立模型 -&gt; 进行训练 -&gt; 进行测试 -&gt; 评估时间和精度<br><a id="more"></a></p>
<h1 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h1><p>数据分为训练集和测试集，分别是785*6w和785*1w的数据，785列中前784列代表了28*28的图片的数值化数据，第785列为0~9的标签，并且第一行为0~784的索引</p>
<ol>
<li>使用pandas读取训练和测试数据</li>
<li>分别把训练和测试数据中的数据和标签分离</li>
<li>将标签进行one-hot编码</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练集有6w, 785列，前784为28*28,最后一列为标签</span></span><br><span class="line">train = pd.read_csv(<span class="string">"MNIST_Training_60K.csv"</span>,index_col=<span class="literal">False</span>,header=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 测试集有1w，784列，28*28的图片</span></span><br><span class="line">test = pd.read_csv(<span class="string">"MNIST_Test_10K.csv"</span>,index_col=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">label = train[:][<span class="string">"784"</span>]</span><br><span class="line">train = train.drop([<span class="string">"784"</span>],axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">X = train.T</span><br><span class="line">Y = np.zeros((<span class="number">10</span>, X.shape[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以下为成像代码</span></span><br><span class="line"><span class="comment"># import matplotlib.pyplot as plt</span></span><br><span class="line"><span class="comment"># x = np.array(X)</span></span><br><span class="line"><span class="comment"># X_train = x.reshape((60000,28,28))</span></span><br><span class="line"><span class="comment"># cur=X_train[0:1]</span></span><br><span class="line"><span class="comment"># plt.imshow(cur[0].reshape(h,w).T,cmap='gray')</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将标签one_hot编码</span></span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> range(X.shape[<span class="number">1</span>]):</span><br><span class="line">    Y[label[n]][n] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 处理test数据</span></span><br><span class="line">label_test = test[:][<span class="string">"784"</span>]</span><br><span class="line">test1 = test.drop([<span class="string">"784"</span>],axis=<span class="number">1</span>)</span><br><span class="line">X_test = test1.T</span><br><span class="line"><span class="comment"># Y_test = np.eye(10)[l]</span></span><br></pre></td></tr></table></figure>
<h1 id="建立模型"><a href="#建立模型" class="headerlink" title="建立模型"></a>建立模型</h1><h2 id="正向传播"><a href="#正向传播" class="headerlink" title="正向传播"></a>正向传播</h2><ol>
<li><p>参数初始化</p>
<p>采用”He”初始化，即使用标准化的服从高斯分布的随机数乘上 2/前一层网络单元数的根号幂初始化权重W，并且使用0初始化对偏差b进行初始化</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">(n_x, n_h, n_y)</span>:</span></span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    W1 = np.random.randn(n_h, n_x) * np.sqrt(<span class="number">2.</span> / n_x)</span><br><span class="line">    b1 = np.zeros((n_h, <span class="number">1</span>))</span><br><span class="line">    W2 = np.random.randn(n_y, n_h)  * np.sqrt(<span class="number">2.</span> / n_h)</span><br><span class="line">    b2 = np.zeros((n_y, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 断言保证这些参数的形状如下，否则报错</span></span><br><span class="line">    <span class="keyword">assert</span>(W1.shape == (n_h, n_x))</span><br><span class="line">    <span class="keyword">assert</span>(b1.shape == (n_h,   <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">assert</span>(W2.shape == (n_y, n_h))</span><br><span class="line">    <span class="keyword">assert</span>(b2.shape == (n_y,   <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将参数封装</span></span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>:W1,</span><br><span class="line">                  <span class="string">"b1"</span>:b1,</span><br><span class="line">                  <span class="string">"W2"</span>:W2,</span><br><span class="line">                  <span class="string">"b2"</span>:b2&#125;</span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure>
<p>注意：</p>
<p>权重w本来的维度应该是(前一层神经元数，本层神经元数)，但是为了方便后续计算省略掉此处的转置操作，在一开始就选择将W的维度设置为(本层神经元数，前一层大小神经元数)</p>
<p>而偏差b由于需要整层网络共享，我们基于Python的广播机制，选择让它的大小设置为(本层神经元数，1)</p>
</li>
</ol>
<ol>
<li><p>正向线性传播</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_forward</span><span class="params">(A0, W1, b1)</span>:</span></span><br><span class="line">    Z1 = np.dot(W1, A0) + b1</span><br><span class="line">    <span class="comment"># 即Z1 = W1 * A0 + b1</span></span><br><span class="line">    <span class="keyword">assert</span>(Z1.shape == (W1.shape[<span class="number">0</span>], A0.shape[<span class="number">1</span>]))</span><br><span class="line">    </span><br><span class="line">    cache = (A0, W1, b1)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> Z1, cache</span><br></pre></td></tr></table></figure>
<p>无特别说明</p>
</li>
</ol>
<ol>
<li><p>正向激活</p>
<p>隐藏层使用ReLU(整流线性单元)函数，输出层使用softmax函数，期望它的值符合0~9十个分类。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu</span><span class="params">(Z1)</span>:</span></span><br><span class="line">    A1 = np.maximum(<span class="number">0</span>, Z1) </span><br><span class="line">    cache = Z1</span><br><span class="line">    <span class="keyword">return</span> A1, cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(Z2)</span>:</span></span><br><span class="line">    A2 = np.exp(Z2) / sum(np.exp(Z2))</span><br><span class="line">    </span><br><span class="line">    cache = Z2</span><br><span class="line">    <span class="keyword">return</span> A2, cache</span><br></pre></td></tr></table></figure>
</li>
</ol>
<ol>
<li><p>正向传播函数</p>
<p>将线性传播部分和激活函数合并</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_forward_relu</span><span class="params">(A0, W1, b1)</span>:</span></span><br><span class="line">    Z1, linear_cache = linear_forward(A0, W1, b1)</span><br><span class="line">    A1, activation_cache = relu(Z1)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(A1.shape == (W.shape[<span class="number">0</span>], A0.shape[<span class="number">1</span>]))</span><br><span class="line">    cache = (linear_cache, activation_cache)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> A1, cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_forward_softmax</span><span class="params">(A1, W2, b2)</span>:</span></span><br><span class="line">    Z2, linear_cache = linear_forward(A1, W2, b2)</span><br><span class="line">    A2, activation_cache = softmax(Z2)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(A2.shape == (W2.shape[<span class="number">0</span>], A1.shape[<span class="number">1</span>]))</span><br><span class="line">    cache = (linear_cache, activation_cache)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> A2, cache</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h2><ol>
<li><p>计算成本</p>
<p>使用交叉熵作为成本函数，其函数的公式为：</p>
<script type="math/tex; mode=display">
\sum_{i}^{m} y_i\log{\hat{y_{i}}}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(A2, Y)</span>:</span></span><br><span class="line">    <span class="comment"># 计算交叉熵</span></span><br><span class="line">    m = Y.shape[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算loss函数</span></span><br><span class="line">    logprobs = <span class="number">-1</span>* sum(np.multiply(np.log(A2), Y))</span><br><span class="line">    <span class="comment"># 成本函数就是将整个训练集上的损失相加取相反数</span></span><br><span class="line">    cost = <span class="number">-1</span>/m * np.sum(logprobs)</span><br><span class="line">    cost = np.squeeze(cost)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cost, logprobs</span><br></pre></td></tr></table></figure>
</li>
</ol>
<ol>
<li><p>反向传播</p>
<p>计算各个参数基于成本的梯度，用于更新参数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_propagation</span><span class="params">(parameters, cache, X, Y)</span>:</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    W1 = parameters[<span class="string">'W1'</span>]</span><br><span class="line">    W2 = parameters[<span class="string">'W2'</span>]</span><br><span class="line">    </span><br><span class="line">    A1 = cache[<span class="string">'A1'</span>]</span><br><span class="line">    A2 = cache[<span class="string">'A2'</span>]</span><br><span class="line">    </span><br><span class="line">    dZ2 = A2 - Y</span><br><span class="line">    dW2 = <span class="number">1</span>/m * np.dot(dZ2, A1.T)</span><br><span class="line">    db2 = <span class="number">1</span>/m * np.sum(dZ2, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    dA1 = np.dot(W2.T, dZ2)</span><br><span class="line">    dZ1 = np.array(dA1, copy=<span class="literal">True</span>)</span><br><span class="line">    dZ1[Z1 &lt;=<span class="number">0</span> ] = <span class="number">0</span> <span class="comment"># 标记一下，操作十分神奇,牛逼！</span></span><br><span class="line">    dW1 = <span class="number">1</span>/m * np.dot(dZ1, X.T)</span><br><span class="line">    db1 = <span class="number">1</span>/m * np.sum(dZ1, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    grads=&#123;<span class="string">"dW1"</span>:dW1,</span><br><span class="line">           <span class="string">"db1"</span>:db1,</span><br><span class="line">           <span class="string">"dW2"</span>:dW2,</span><br><span class="line">           <span class="string">"db2"</span>:db2&#125;</span><br><span class="line">    <span class="keyword">return</span> grads</span><br></pre></td></tr></table></figure>
<p>各参数传播的顺序和算式如下：</p>
<img src="/2019/11/28/%E7%94%A8numpy%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E5%8F%8C%E5%B1%82%E7%9A%84softmax%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/propagation.jpg" class="" title="参数传播">
</li>
</ol>
<pre><code>softmax对输入求导的公式见[CS231n-Week3-Assignment4](https://1305936314.github.io/2019/12/09/CS231n-Week3-Assignment4/)
</code></pre><ol>
<li><p>更新参数</p>
<p>使用mini-batch梯度下降法更新参数，目前仅实现了梯度下降法，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span><span class="params">(parameters, grads, learning_rate=<span class="number">1.2</span>)</span>:</span></span><br><span class="line">    W1 = parameters[<span class="string">'W1'</span>]</span><br><span class="line">    b1 = parameters[<span class="string">'b1'</span>]</span><br><span class="line">    W2 = parameters[<span class="string">'W2'</span>]</span><br><span class="line">    b2 = parameters[<span class="string">'b2'</span>]</span><br><span class="line">    </span><br><span class="line">    dW1 = grads[<span class="string">'dW1'</span>]</span><br><span class="line">    db1 = grads[<span class="string">'db1'</span>]</span><br><span class="line">    dW2 = grads[<span class="string">'dW2'</span>]</span><br><span class="line">    db2 = grads[<span class="string">'db2'</span>]</span><br><span class="line">    </span><br><span class="line">    W1 = W1 - learning_rate* dW1</span><br><span class="line">    b1 = b1 - learning_rate* db1</span><br><span class="line">    W2 = W2 - learning_rate* dW2</span><br><span class="line">    b2 = b2 - learning_rate* db2</span><br><span class="line">    </span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</span><br><span class="line">                  <span class="string">"b1"</span>: b1,</span><br><span class="line">                  <span class="string">"W2"</span>: W2,</span><br><span class="line">                  <span class="string">"b2"</span>: b2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="整体模型"><a href="#整体模型" class="headerlink" title="整体模型"></a>整体模型</h2><p>将上述函数等无缝合并，方便转换</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nn_model</span><span class="params">(X, Y, n_h=<span class="number">1000</span>, learning_rate=<span class="number">0.12</span> ,num_iterations=<span class="number">10000</span>, print_cost=False)</span>:</span></span><br><span class="line">    n_x = <span class="number">784</span></span><br><span class="line">    n_h = <span class="number">1000</span></span><br><span class="line">    n_y = <span class="number">10</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 随机初始化参数</span></span><br><span class="line">    np.random.seed(<span class="number">3</span>)</span><br><span class="line">    W1 = np.random.randn(n_h, n_x) * np.sqrt(<span class="number">2.</span> / n_x)</span><br><span class="line">    b1 = np.zeros((n_h, <span class="number">1</span>))</span><br><span class="line">    W2 = np.random.randn(n_y, n_h)  * np.sqrt(<span class="number">2.</span> / n_h)</span><br><span class="line">    b2 = np.zeros((n_y, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line">        <span class="comment"># 正向传播: 线性-&gt; ReLU -&gt; 线性-&gt; softmax</span></span><br><span class="line">        Z1 = np.dot(W1, X) + b1</span><br><span class="line">        A1 = np.maximum(<span class="number">0</span>, Z1)</span><br><span class="line">        Z2 = np.dot(W2, A1) + b2</span><br><span class="line">        A2 = np.exp(Z2) / sum(np.exp(Z2))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算成本：计算交叉熵成本</span></span><br><span class="line">        logprobs = <span class="number">-1</span>* sum(np.multiply(np.log(A2), Y))</span><br><span class="line">        cost = <span class="number">-1</span>/m * np.sum(logprobs)</span><br><span class="line">        cost = np.squeeze(cost)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 反向传播：梯度下降法</span></span><br><span class="line">        dZ2 = A2 - Y</span><br><span class="line">        dW2 = <span class="number">1</span>/m * np.dot(dZ2, A1.T)</span><br><span class="line">        db2 = <span class="number">1</span>/m * np.sum(dZ2, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">        dA1 = np.dot(W2.T, dZ2)</span><br><span class="line">        dZ1 = np.array(dA1, copy=<span class="literal">True</span>)</span><br><span class="line">        dZ1[Z1 &lt;=<span class="number">0</span> ] = <span class="number">0</span> <span class="comment"># 标记一下，操作十分神奇,牛逼！</span></span><br><span class="line">        dW1 = <span class="number">1</span>/m * np.dot(dZ1, X.T)</span><br><span class="line">        db1 = <span class="number">1</span>/m * np.sum(dZ1, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 更新参数</span></span><br><span class="line">        W1 -= learning_rate * dW1</span><br><span class="line">        b1 -= learning_rate * db1</span><br><span class="line">        W2 -= learning_rate * dW2</span><br><span class="line">        b2 -= learning_rate * db2</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Cost after iteration %i: %f"</span> %(i, cost))</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> W1, b1, W2, b2</span><br></pre></td></tr></table></figure>
<p>此处只为保留一下未mini-batch版本</p>
<h1 id="训练测试"><a href="#训练测试" class="headerlink" title="训练测试"></a>训练测试</h1><p>预测函数如下，即正向传播过程</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(X, W1, b1, W2, b2)</span>:</span></span><br><span class="line">    Z1 = np.dot(W1, X) + b1</span><br><span class="line">    A1 = np.maximum(<span class="number">0</span>, Z1)</span><br><span class="line">    Z2 = np.dot(W2, A1) + b2</span><br><span class="line">    A2 = np.exp(Z2) / sum(np.exp(Z2))</span><br><span class="line">    predictions = A2.argmax(axis=<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 使用argmax使得返回值predictions为和label相同的0~9标签</span></span><br><span class="line">    <span class="keyword">return</span> predictions</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line">tic = time.process_time()</span><br><span class="line">W1, b1, W2, b2= nn_model(X, Y,n_h=<span class="number">300</span>, learning_rate=<span class="number">0.08</span>, num_iterations=<span class="number">10000</span>, print_cost=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line">predictions = predict(X_test, W1, b1, W2, b2)</span><br><span class="line">toc = time.process_time()</span><br><span class="line"><span class="comment"># 时间</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Computation time = "</span> + str((toc - tic)) + <span class="string">"s"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 评估精确度</span></span><br><span class="line">correct_predictions = np.equal(predictions, label_test)</span><br><span class="line">accuracy = np.mean(correct_predictions.astype(np.float32))</span><br><span class="line">print(<span class="string">'Test Accuracy:%f'</span>%(accuracy*<span class="number">100</span>) + <span class="string">'%'</span>)</span><br></pre></td></tr></table></figure>
<h1 id="所有代码合并"><a href="#所有代码合并" class="headerlink" title="所有代码合并"></a>所有代码合并</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="comment"># import matplotlib.pyplot as plt</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练集有6w, 785列，前784为28*28,最后一列为标签</span></span><br><span class="line">train = pd.read_csv(<span class="string">"MNIST_Training_60K.csv"</span>,index_col=<span class="literal">False</span>,header=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 测试集有1w，784列，28*28的图片</span></span><br><span class="line">test = pd.read_csv(<span class="string">"MNIST_Test_10K.csv"</span>,index_col=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">label = train[:][<span class="string">"784"</span>]</span><br><span class="line">train = train.drop([<span class="string">"784"</span>],axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">X = train.T</span><br><span class="line">Y = np.zeros((<span class="number">10</span>, X.shape[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> range(X.shape[<span class="number">1</span>]):</span><br><span class="line">    Y[label[n]][n] = <span class="number">1</span></span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nn_model</span><span class="params">(X, Y, n_h=<span class="number">1000</span>, learning_rate=<span class="number">0.12</span> ,num_iterations=<span class="number">10000</span>, print_cost=False)</span>:</span></span><br><span class="line">    n_x = <span class="number">784</span></span><br><span class="line">    n_y = <span class="number">10</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 随机初始化参数</span></span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    W1 = np.random.randn(n_h, n_x) * np.sqrt(<span class="number">2.</span> / n_x)</span><br><span class="line">    b1 = np.zeros((n_h, <span class="number">1</span>))</span><br><span class="line">    W2 = np.random.randn(n_y, n_h)  * np.sqrt(<span class="number">2.</span> / n_h)</span><br><span class="line">    b2 = np.zeros((n_y, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line">        <span class="comment"># 正向传播: 线性-&gt; ReLU -&gt; 线性-&gt; softmax</span></span><br><span class="line">        Z1 = np.dot(W1, X) + b1</span><br><span class="line">        A1 = np.maximum(<span class="number">0</span>, Z1)</span><br><span class="line">        Z2 = np.dot(W2, A1) + b2</span><br><span class="line">        A2 = np.exp(Z2) / sum(np.exp(Z2))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算成本：计算交叉熵成本</span></span><br><span class="line">        logprobs = <span class="number">-1</span>* sum(np.multiply(np.log(A2), Y))</span><br><span class="line">        cost = <span class="number">-1</span>/m * np.sum(logprobs)</span><br><span class="line">        cost = np.squeeze(cost)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 反向传播：梯度下降法</span></span><br><span class="line">        dZ2 = A2 - Y</span><br><span class="line">        dW2 = <span class="number">1</span>/m * np.dot(dZ2, A1.T)</span><br><span class="line">        db2 = <span class="number">1</span>/m * np.sum(dZ2, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">        dA1 = np.dot(W2.T, dZ2)</span><br><span class="line">        dZ1 = np.array(dA1, copy=<span class="literal">True</span>)</span><br><span class="line">        dZ1[Z1 &lt;=<span class="number">0</span> ] = <span class="number">0</span> <span class="comment"># 标记一下，操作十分神奇,牛逼！</span></span><br><span class="line">        dW1 = <span class="number">1</span>/m * np.dot(dZ1, X.T)</span><br><span class="line">        db1 = <span class="number">1</span>/m * np.sum(dZ1, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 更新参数</span></span><br><span class="line">        W1 -= learning_rate * dW1</span><br><span class="line">        b1 -= learning_rate * db1</span><br><span class="line">        W2 -= learning_rate * dW2</span><br><span class="line">        b2 -= learning_rate * db2</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Cost after iteration %i: %f"</span> %(i, cost))</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> W1, b1, W2, b2</span><br><span class="line"></span><br><span class="line">label_test = test[:][<span class="string">"784"</span>]</span><br><span class="line">test1 = test.drop([<span class="string">"784"</span>],axis=<span class="number">1</span>)</span><br><span class="line">X_test = test1.T</span><br><span class="line"><span class="comment"># Y_test = np.eye(10)[l]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line">tic = time.process_time()</span><br><span class="line">W1, b1, W2, b2= nn_model(X, Y,n_h=<span class="number">300</span>, learning_rate=<span class="number">0.08</span>, num_iterations=<span class="number">10000</span>, print_cost=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line">Z1 = np.dot(W1, X_test) + b1</span><br><span class="line">A1 = np.maximum(<span class="number">0</span>, Z1)</span><br><span class="line">Z2 = np.dot(W2, A1) + b2</span><br><span class="line">A2 = np.exp(Z2) / sum(np.exp(Z2))</span><br><span class="line">predictions = A2.argmax(axis=<span class="number">0</span>)</span><br><span class="line">toc = time.process_time()</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Computation time = "</span> + str((toc - tic)) + <span class="string">"s"</span>)</span><br><span class="line"></span><br><span class="line">correct_predictions = np.equal(predictions, label_test)</span><br><span class="line">accuracy = np.mean(correct_predictions.astype(np.float32))</span><br><span class="line">print(<span class="string">'Test Accuracy:%f'</span>%(accuracy*<span class="number">100</span>) + <span class="string">'%'</span>)</span><br></pre></td></tr></table></figure>
<h1 id="添加mini-batch"><a href="#添加mini-batch" class="headerlink" title="添加mini-batch"></a>添加mini-batch</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练集有6w, 785列，前784为28*28,最后一列为标签</span></span><br><span class="line">train = pd.read_csv(<span class="string">"./MNIST_Training_60K.csv"</span>, index_col=<span class="literal">False</span>, header=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 测试集有1w，784列，28*28的图片</span></span><br><span class="line">test = pd.read_csv(<span class="string">"./MNIST_Test_10K.csv"</span>, index_col=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">label = train[:][<span class="string">"784"</span>]</span><br><span class="line">train = train.drop([<span class="string">"784"</span>], axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">X = train.T</span><br><span class="line">Y = np.zeros((<span class="number">10</span>, X.shape[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> range(X.shape[<span class="number">1</span>]):</span><br><span class="line">    Y[label[n]][n] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nn_model</span><span class="params">(X, Y, n_h=<span class="number">1000</span>, mini_batch_size=<span class="number">100</span>, learning_rate=<span class="number">0.08</span>, epoch=<span class="number">1000</span>, print_cost=False)</span>:</span></span><br><span class="line">    n_x = <span class="number">784</span></span><br><span class="line">    n_y = <span class="number">10</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 随机初始化参数</span></span><br><span class="line">    np.random.seed(<span class="number">3</span>)</span><br><span class="line">    W1 = np.random.randn(n_h, n_x) * np.sqrt(<span class="number">2.</span> / n_x)</span><br><span class="line">    b1 = np.zeros((n_h, <span class="number">1</span>))</span><br><span class="line">    W2 = np.random.randn(n_y, n_h) * np.sqrt(<span class="number">2.</span> / n_h)</span><br><span class="line">    b2 = np.zeros((n_y, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    costs=[]</span><br><span class="line">    complete_numof_mini_batch = m // mini_batch_size</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, epoch):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, complete_numof_mini_batch):</span><br><span class="line">            <span class="comment"># 正向传播: 线性-&gt; ReLU -&gt; 线性-&gt; softmax</span></span><br><span class="line">            mini_batch_X = X.values[:, (j * mini_batch_size): (j + <span class="number">1</span>) * mini_batch_size]</span><br><span class="line">            mini_batch_Y = Y[:, (j * mini_batch_size): (j + <span class="number">1</span>) * mini_batch_size]</span><br><span class="line">            Z1 = np.dot(W1, mini_batch_X) + b1</span><br><span class="line">            A1 = np.maximum(<span class="number">0</span>, Z1)</span><br><span class="line">            Z2 = np.dot(W2, A1) + b2</span><br><span class="line">            A2 = np.exp(Z2) / sum(np.exp(Z2))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 计算成本：计算交叉熵成本</span></span><br><span class="line">            logprobs = <span class="number">-1</span> * sum(np.multiply(np.log(A2), mini_batch_Y))</span><br><span class="line">            cost = <span class="number">-1</span> / mini_batch_size * np.sum(logprobs)</span><br><span class="line">            cost = np.squeeze(cost)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 反向传播：梯度下降法</span></span><br><span class="line">            dZ2 = A2 - mini_batch_Y</span><br><span class="line">            dW2 = <span class="number">1</span> / mini_batch_size * np.dot(dZ2, A1.T)</span><br><span class="line">            db2 = <span class="number">1</span> / mini_batch_size * np.sum(dZ2, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">            dA1 = np.dot(W2.T, dZ2)</span><br><span class="line">            dZ1 = np.array(dA1, copy=<span class="literal">True</span>)</span><br><span class="line">            dZ1[Z1 &lt;= <span class="number">0</span>] = <span class="number">0</span>  <span class="comment"># 标记一下，操作十分神奇,牛逼！</span></span><br><span class="line">            dW1 = <span class="number">1</span> / mini_batch_size * np.dot(dZ1, mini_batch_X.T)</span><br><span class="line">            db1 = <span class="number">1</span> / mini_batch_size * np.sum(dZ1, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 更新参数</span></span><br><span class="line">            W1 -= learning_rate * dW1</span><br><span class="line">            b1 -= learning_rate * db1</span><br><span class="line">            W2 -= learning_rate * dW2</span><br><span class="line">            b2 -= learning_rate * db2</span><br><span class="line">            <span class="keyword">if</span> print_cost <span class="keyword">and</span> (j+<span class="number">1</span>) % <span class="number">500</span>:</span><br><span class="line">                costs.append(cost)</span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> (i+<span class="number">1</span>) % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Cost after epoch [%3d/%3d]: %f"</span> % (i+<span class="number">1</span>, epoch, cost))</span><br><span class="line"></span><br><span class="line">    plt.plot(costs)</span><br><span class="line">    plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'iterations (per 500)'</span>)</span><br><span class="line">    plt.title(<span class="string">"Learning rate = "</span> + str(learning_rate))</span><br><span class="line">    plt.show()</span><br><span class="line">    <span class="keyword">return</span> W1, b1, W2, b2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">label_test = test[:][<span class="string">"784"</span>]</span><br><span class="line">test1 = test.drop([<span class="string">"784"</span>], axis=<span class="number">1</span>)</span><br><span class="line">X_test = test1.T</span><br><span class="line"><span class="comment"># Y_test = np.eye(10)[l]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line">mini_batch_size = <span class="number">100</span></span><br><span class="line">tic = time.process_time()</span><br><span class="line">W1, b1, W2, b2 = nn_model(X, Y, n_h=<span class="number">1000</span>, mini_batch_size=mini_batch_size, learning_rate=<span class="number">0.1</span>, epoch=<span class="number">100</span>, print_cost=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line">complete_numof_mini_batch_test = X_test.shape[<span class="number">1</span>] // mini_batch_size</span><br><span class="line">accuracy = []</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">0</span>, complete_numof_mini_batch_test):</span><br><span class="line">    mini_batch_X_test = X_test.values[:, (k * mini_batch_size):((k+<span class="number">1</span>) * mini_batch_size)]</span><br><span class="line">    mini_batch_label = label_test.values[(k * mini_batch_size): (k+<span class="number">1</span>) * mini_batch_size]</span><br><span class="line">    Z1 = np.dot(W1, mini_batch_X_test) + b1</span><br><span class="line">    A1 = np.maximum(<span class="number">0</span>, Z1)</span><br><span class="line">    Z2 = np.dot(W2, A1) + b2</span><br><span class="line">    A2 = np.exp(Z2) / sum(np.exp(Z2))</span><br><span class="line">    predictions = A2.argmax(axis=<span class="number">0</span>)</span><br><span class="line">    correct_predictions = np.equal(predictions, mini_batch_label)</span><br><span class="line">    accuracy.append(np.mean(correct_predictions.astype(np.float32)))</span><br><span class="line"></span><br><span class="line">accuracy = np.mean(accuracy)</span><br><span class="line">toc = time.process_time()</span><br><span class="line">print(<span class="string">"Computation time = "</span> + str((toc - tic)) + <span class="string">"s"</span>)</span><br><span class="line">print(<span class="string">'Test Accuracy:%f'</span> % (accuracy * <span class="number">100</span>) + <span class="string">'%'</span>)</span><br></pre></td></tr></table></figure>
<img src="/2019/11/28/%E7%94%A8numpy%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E5%8F%8C%E5%B1%82%E7%9A%84softmax%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/costs.png" class="" title="cost图">
<p>代码与cost如上图，目前改进就是如此了，将X分成100样本一小份，然后逐份进行训练，由于参数共享，所以只有反向传播时涉及到了Y，别处都不需要修改太多。后续具体参数还需要继续调整，敬请期待！</p>
<p>最后调整好的准确率和时间如下：</p>
<img src="/2019/11/28/%E7%94%A8numpy%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E5%8F%8C%E5%B1%82%E7%9A%84softmax%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/accuracy.png" class="" title="准确率和时间">
<h1 id="说说我对神经网络的理解"><a href="#说说我对神经网络的理解" class="headerlink" title="说说我对神经网络的理解"></a>说说我对神经网络的理解</h1><p>再看一眼这个参数传播的图：</p>
<img src="/2019/11/28/%E7%94%A8numpy%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E5%8F%8C%E5%B1%82%E7%9A%84softmax%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/propagation.jpg" class="" title="参数传播图">
<h2 id="宏观理解"><a href="#宏观理解" class="headerlink" title="宏观理解"></a>宏观理解</h2><p>根据上面的参数传播图来宏观的理解一下就是：</p>
<p>我们所需要做的分类在数值化后，可以通过拟合几个函数(在这个全连接层里起码是这样的)，通过它们得到最终的值并进行取舍和改进，慢慢的能够得出一个网络，包含了多个拟合好的函数进行计算和选择，以此来做到分类</p>
<p>从X开始，每层选择一个节点，一直到输出层，这样就是一个函数，全连接层就是每层节点与上一层都有关系，而通过上一层的权重与上上层的节点也保持一定的关联，依次类推。</p>
<h2 id="正向传播-1"><a href="#正向传播-1" class="headerlink" title="正向传播"></a>正向传播</h2><p>正向传播中，每次求Z都是一次线性运算，相当于将x放入一个逐次增高的函数中，让x位于的维度越来越高，而w就是其每层幂中对于x的权重，在图像中，相当于是w在决定该维度中的决策边线的角度，b是因为单纯的旋转不足以让需要拟合的曲线到达需要的地方，还需要进行上下调整，因为决策边线是无限长的，所以上下调整配合角度旋转就可以到达任何需要拟合的位置。</p>
<p>而随着网络层数的增加，这个输出会到达较高的维度，这样的维度中决策边线就是曲线了。</p>
<p>ReLU函数(A = max(0, Z))的修正作用一方面使得被激活的函数呈非对称(偶)结构，另一方面优化了计算的步骤，对于Z1是负数的情况不需要考虑，由别的地方来给它解答。</p>
<p>softmax($S_i = \frac{e^{z_i}}{\sum_{j}^{N}e^{z_j}}$)的作用不言而喻，对于最可能的情况给予他最大的权重值，但是别的情况也有相似值可以考虑，这就好像模拟了人的认识过程，随着你越来越懂怎么辨认，你就越来越不会觉得可能是别的情况，但在一开始还是不清楚的。softmax还有一个好处就是它自带标准化，得出的值的和为1，这样我们不妨可以将每个值认作对应类的概率，从而与one-hot后的Y值不谋而合。只要我们优化函数使得最终对应类的值尽可能的接近1即可~</p>
<h2 id="计算成本"><a href="#计算成本" class="headerlink" title="计算成本"></a>计算成本</h2><p>计算成本的函数为交叉熵函数($L = - \sum_{i}^{N} y_i \log{a_{2_i}}$)，其中N指的是需要分辨的类的总数，也是softmax中得出的结果数量。这个函数与KL散度密切相关，当且仅当分布P和Q在离散变量的情况下是相同的分布，或者连续性变量的情况下是几乎“处处相同”时，KL散度为0。最小化成本即最小化交叉熵的过程就是拟合神经网络中的分布与给定数据的分布的过程，使得它们尽量的相似。在数据足够多的情况下，基本可以认为它服从真实的分布。</p>
<p>(原公式中是$y_i\log{\hat{y_i}}$，而在softmax的结果中我们基本可以认为$\hat{y} == a_{2_i}$)</p>
<p>每一批量的成本取值为该批量中的样本计算出的交叉熵的平均值，这样大体上可以模拟该批量中的样本所代表的分布与拟合出的分布之间的差距</p>
<h2 id="反向传播-1"><a href="#反向传播-1" class="headerlink" title="反向传播"></a>反向传播</h2><p>反向传播的意义在于什么呢，在于计算出成本函数关于各个参数(尤其是W，b)的梯度，而W和b的梯度需要借助其他参数得到(因为链式求导法则)，因此，我们需要沿着成本函数反向的计算各个参数关于成本的梯度，从而借此更方便的计算出W和b的梯度</p>
<p>关于这个梯度的作用的话，大家都知道，梯度方向是函数在该店沿着梯度方向变化最快的方向，所以借助梯度我们能更快的改变参数的值，让网络的性能更好。</p>
<h2 id="更新参数"><a href="#更新参数" class="headerlink" title="　更新参数"></a>　更新参数</h2><p>更新参数的过程才是使用梯度下降法的关键，更新参数需要参数减去学习率＊成本关于该参数的梯度。我们将梯度认作是一个让我们下降最快的方向的话，那么学习率就是我们下降的时候一步跨出的步长，比如在一个碗状的函数中，高度值代表着我们的误差值，那么沿着当前点的梯度方向跨出一步，理论上相对而言就是下降较快的一步，而不同的点的梯度不一样，所以并不能说明它是最快的。</p>
<p>并且随着我们越来越接近碗底，我们跨出的步长应该变得越来越小，从而使得我们能够更“小心地”接近最低点。所以学习率衰减是一个好办法。从物理上得出这样的理解，同样从物理上得到灵感的还有<a href="[https://1305936314.github.io/2019/12/01/Adam%E7%9A%84%E5%9B%9E%E9%A1%BE/](https://1305936314.github.io/2019/12/01/Adam的回顾/">动量梯度下降法</a>)，取其加权平均数，一般取$\beta_1$ = 0.9较多，即十个样本的加权平均值。</p>
<h2 id="循环批量-batch-，遍数-epoch"><a href="#循环批量-batch-，遍数-epoch" class="headerlink" title="循环批量(batch)，遍数(epoch)"></a>循环批量(batch)，遍数(epoch)</h2><p>我这里使用小批量(mini-batch)的是为了照顾后面对应使用x86汇编实现，暂时想的是mini-batch取20，因为10及以下容易在计算log的时候算到log0从而后面的值都是无效值。小批量循环至一遍结束后要继续遍历epoch次，我这里由于将第一层的神经元数调小了，遍历次数需要多一些，从100升到了120，有时150</p>
<p>并且，由于使用了小批量时每次向量化计算减少了，for循环需要的次数增多了，计算时间也会被明显拉长</p>
<h1 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h1><p>先训练后得出模型的重要参数W，b后，我们要进行测试，时间的评估以及准确度的评估</p>
<p>测试时将测试集放入参数中进行正向传播后得出A2，即模型的预测值，然后看预测值正确的数量来评估准确度。</p>
<h2 id="调参收获"><a href="#调参收获" class="headerlink" title="调参收获"></a>调参收获</h2><img src="/2019/11/28/%E7%94%A8numpy%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E5%8F%8C%E5%B1%82%E7%9A%84softmax%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/cost.png" class="" title="成本">    
<p>这个是我在调小了第一层的神经元数1000=&gt;50后的成本和准确率，调小了神经元数之后呢，相应的这个需要遍历的次数变的高了不少，我觉得可能第一层的权重w初始化需要小一点比较好，本身初始化大小与节点数成反相关的。并且呢需要拟合的复杂度变高了，毕竟与每个节点的相关度变高了，这就相当于是反向正则化了(并且也确认了一个观点“增加网络的规模有一定正则化作用”是对的)，然后的话mini-batch不能太小，而在cost图中如果阴影部分比较分散的话，我个人觉得可能是需要学习率小一点，如果是多片分开的阴影的话我觉得可能需要神经元数多一点更好。</p>

      
    </div>
    
    
    

    
      <div>
        <div id="wechat_subscriber" style="display: block; padding: 10px 0; margin: 20px auto; width: 100%; text-align: center">
    <img id="wechat_subscriber_qcode" src="/images/wechatQR.png" alt="BlackSheepX wechat" style="width: 200px; max-width: 100%;"/>
    <div>性感捕咕人在线放屁</div>
</div>

      </div>
    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/%E5%BE%AE%E6%9C%BA%E8%AF%BE%E8%AE%BE/" rel="tag"># 微机课设</a>
          
            <a href="/tags/numpy/" rel="tag"># numpy</a>
          
            <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" rel="tag"># 神经网络</a>
          
            <a href="/tags/MNIST/" rel="tag"># MNIST</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/11/21/%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/" rel="next" title="第一篇博客">
                <i class="fa fa-chevron-left"></i> 第一篇博客
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/11/30/CS231n-Week1-Assignment1/" rel="prev" title="CS231n_Week1_Assignment1">
                CS231n_Week1_Assignment1 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="lv-container" data-id="city" data-uid="MTAyMC80NzczMS8yNDIyOQ"></div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/Faceimage.jpg"
                alt="BlackSheepX" />
            
              <p class="site-author-name" itemprop="name">BlackSheepX</p>
              <p class="site-description motion-element" itemprop="description">日常放放屁的一个博客</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives%7C%7C%20archive">
              
                  <span class="site-state-item-count">10</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">16</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/1305936314" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://me.csdn.net/xujiaqi1574587" target="_blank" title="CSDN">
                      
                        <i class="fa fa-fw fa-globe"></i>CSDN</a>
                  </span>
                
            </div>
          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                友情链接：
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://www.baidu.com/" title="万能大神" target="_blank">万能大神</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://youjing999.github.io" title="右京先生" target="_blank">右京先生</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#整体思路"><span class="nav-number">1.</span> <span class="nav-text">整体思路</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#总思路"><span class="nav-number">2.</span> <span class="nav-text">总思路</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#数据处理"><span class="nav-number">3.</span> <span class="nav-text">数据处理</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#建立模型"><span class="nav-number">4.</span> <span class="nav-text">建立模型</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#正向传播"><span class="nav-number">4.1.</span> <span class="nav-text">正向传播</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#反向传播"><span class="nav-number">4.2.</span> <span class="nav-text">反向传播</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#整体模型"><span class="nav-number">4.3.</span> <span class="nav-text">整体模型</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#训练测试"><span class="nav-number">5.</span> <span class="nav-text">训练测试</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#所有代码合并"><span class="nav-number">6.</span> <span class="nav-text">所有代码合并</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#添加mini-batch"><span class="nav-number">7.</span> <span class="nav-text">添加mini-batch</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#说说我对神经网络的理解"><span class="nav-number">8.</span> <span class="nav-text">说说我对神经网络的理解</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#宏观理解"><span class="nav-number">8.1.</span> <span class="nav-text">宏观理解</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#正向传播-1"><span class="nav-number">8.2.</span> <span class="nav-text">正向传播</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#计算成本"><span class="nav-number">8.3.</span> <span class="nav-text">计算成本</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#反向传播-1"><span class="nav-number">8.4.</span> <span class="nav-text">反向传播</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#更新参数"><span class="nav-number">8.5.</span> <span class="nav-text">　更新参数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#循环批量-batch-，遍数-epoch"><span class="nav-number">8.6.</span> <span class="nav-text">循环批量(batch)，遍数(epoch)</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#测试"><span class="nav-number">9.</span> <span class="nav-text">测试</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#调参收获"><span class="nav-number">9.1.</span> <span class="nav-text">调参收获</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>

<span class="author" itemprop="copyrightHolder">BlackSheepX </span>
  

</div>





  <div class="theme-info"></div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  
    <script type="text/javascript">
      (function(d, s) {
        var j, e = d.getElementsByTagName(s)[0];
        if (typeof LivereTower === 'function') { return; }
        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.async = true;
        e.parentNode.insertBefore(j, e);
      })(document, 'script');
    </script>
  










  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item=>{
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: false,
        appId: 'qhaVk95aTWcdRVnm2R66K4Cy-gzGzoHsz',
        appKey: 'jz91FTFBgRzkfJ4dEjll7yli',
        placeholder: 'Just go go',
        avatar:'mm',
        guest_info:guest,
        pageSize:'10' || 10,
    });
  </script>



  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "./public/search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("qhaVk95aTWcdRVnm2R66K4Cy-gzGzoHsz", "jz91FTFBgRzkfJ4dEjll7yli");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
