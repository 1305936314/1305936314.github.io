<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>CS231n_Week1_Assignment1</title>
      <link href="/2019/11/30/CS231n-Week1-Assignment1/"/>
      <url>/2019/11/30/CS231n-Week1-Assignment1/</url>
      
        <content type="html"><![CDATA[<h1 id="作业："><a href="#作业：" class="headerlink" title="作业："></a>作业：</h1><p>(阅读理解题)</p><h2 id="1-图像的数据主要来源有哪些"><a href="#1-图像的数据主要来源有哪些" class="headerlink" title="1. 图像的数据主要来源有哪些"></a>1. 图像的数据主要来源有哪些</h2><p>照相机，手机，摄像头等一系列视觉传感器设备生产出了大量的图像数据</p><h2 id="2-sift-feature，金字塔匹配思想和hog特征分别是什么，可以用来干什么"><a href="#2-sift-feature，金字塔匹配思想和hog特征分别是什么，可以用来干什么" class="headerlink" title="2. sift feature，金字塔匹配思想和hog特征分别是什么，可以用来干什么"></a>2. sift feature，金字塔匹配思想和hog特征分别是什么，可以用来干什么</h2><ol><li><strong>sift feature：</strong>确认目标上在变化中具有表现型和不变性的特征，然后把这些特征与相似的目标进行匹配<a id="more"></a></li><li><strong>金字塔匹配思想：</strong>从图片的各部分的各个像素抽取特征并把它们放到一起作为一个特征描述符，然后再特征描述符上做一个支持向量机</li><li><strong>hog特征：</strong>histogram of gradients方向梯度直方图，将一堆特征放在一起后，研究如何在实际图片中比较合理的设计人体姿态和辨认人体姿态</li></ol><h2 id="3-神经网络早就存在为什么最近才兴起"><a href="#3-神经网络早就存在为什么最近才兴起" class="headerlink" title="3. 神经网络早就存在为什么最近才兴起"></a>3. 神经网络早就存在为什么最近才兴起</h2><ol><li>算力的提升</li><li>高质量的数据集的数量增多</li></ol><h2 id="4-图像任务有哪些，解决什么样的图像问题"><a href="#4-图像任务有哪些，解决什么样的图像问题" class="headerlink" title="4. 图像任务有哪些，解决什么样的图像问题"></a>4. 图像任务有哪些，解决什么样的图像问题</h2><p>图像分类，目标检测，图像描述</p>]]></content>
      
      
      <categories>
          
          <category> CS231n </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CS231n </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>用numpy实现一个双层的softmax神经网络</title>
      <link href="/2019/11/28/%E7%94%A8numpy%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E5%8F%8C%E5%B1%82%E7%9A%84softmax%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
      <url>/2019/11/28/%E7%94%A8numpy%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E5%8F%8C%E5%B1%82%E7%9A%84softmax%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<h1 id="整体思路"><a href="#整体思路" class="headerlink" title="整体思路"></a>整体思路</h1><p>使用Numpy和pandas写一个神经网络，进行手写数字识别(MNIST)，样本图如下：</p><img src="/2019/11/28/%E7%94%A8numpy%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E5%8F%8C%E5%B1%82%E7%9A%84softmax%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/output_13_0.png" class="" title="手写数字5"><h1 id="总思路"><a href="#总思路" class="headerlink" title="总思路"></a>总思路</h1><p>整体神经网络代码只使用Numpy和pandas，根据额外需要使用time和matplotlib，设计思路如下</p><p>数据处理-&gt; 建立模型 -&gt; 进行训练 -&gt; 进行测试 -&gt; 评估时间和精度<br><a id="more"></a></p><h1 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h1><p>数据分为训练集和测试集，分别是785*6w和785*1w的数据，785列中前784列代表了28*28的图片的数值化数据，第785列为0~9的标签，并且第一行为0~784的索引</p><ol><li>使用pandas读取训练和测试数据</li><li>分别把训练和测试数据中的数据和标签分离</li><li>将标签进行one-hot编码</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练集有6w, 785列，前784为28*28,最后一列为标签</span></span><br><span class="line">train = pd.read_csv(<span class="string">"MNIST_Training_60K.csv"</span>,index_col=<span class="literal">False</span>,header=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 测试集有1w，784列，28*28的图片</span></span><br><span class="line">test = pd.read_csv(<span class="string">"MNIST_Test_10K.csv"</span>,index_col=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">label = train[:][<span class="string">"784"</span>]</span><br><span class="line">train = train.drop([<span class="string">"784"</span>],axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">X = train.T</span><br><span class="line">Y = np.zeros((<span class="number">10</span>, X.shape[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以下为成像代码</span></span><br><span class="line"><span class="comment"># import matplotlib.pyplot as plt</span></span><br><span class="line"><span class="comment"># x = np.array(X)</span></span><br><span class="line"><span class="comment"># X_train = x.reshape((60000,28,28))</span></span><br><span class="line"><span class="comment"># w,h=28,28</span></span><br><span class="line"><span class="comment"># totalNum=X_train.shape[0]</span></span><br><span class="line"><span class="comment"># cur=X_train[0:1]</span></span><br><span class="line"><span class="comment"># cnt=0</span></span><br><span class="line"><span class="comment"># fig, axs = plt.subplots(1)</span></span><br><span class="line"><span class="comment"># axs.imshow(cur[cnt].reshape(h,w).T,cmap='gray')</span></span><br><span class="line"><span class="comment"># axs.axis('off')</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将标签one_hot编码</span></span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> range(X.shape[<span class="number">1</span>]):</span><br><span class="line">    Y[label[n]][n] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 处理test数据</span></span><br><span class="line">label_test = test[:][<span class="string">"784"</span>]</span><br><span class="line">test1 = test.drop([<span class="string">"784"</span>],axis=<span class="number">1</span>)</span><br><span class="line">X_test = test1.T</span><br><span class="line"><span class="comment"># Y_test = np.eye(10)[l]</span></span><br></pre></td></tr></table></figure><h1 id="建立模型"><a href="#建立模型" class="headerlink" title="建立模型"></a>建立模型</h1><h2 id="正向传播"><a href="#正向传播" class="headerlink" title="正向传播"></a>正向传播</h2><ol><li><p>参数初始化</p><p>采用”He”初始化，即使用标准化的服从高斯分布的随机数乘上 2/前一层网络单元数的根号幂初始化权重W，并且使用0初始化对偏差b进行初始化</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">(n_x, n_h, n_y)</span>:</span></span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    W1 = np.random.randn(n_h, n_x) * np.sqrt(<span class="number">2.</span> / n_x)</span><br><span class="line">    b1 = np.zeros((n_h, <span class="number">1</span>))</span><br><span class="line">    W2 = np.random.randn(n_y, n_h)  * np.sqrt(<span class="number">2.</span> / n_h)</span><br><span class="line">    b2 = np.zeros((n_y, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 断言保证这些参数的形状如下，否则报错</span></span><br><span class="line">    <span class="keyword">assert</span>(W1.shape == (n_h, n_x))</span><br><span class="line">    <span class="keyword">assert</span>(b1.shape == (n_h,   <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">assert</span>(W2.shape == (n_y, n_h))</span><br><span class="line">    <span class="keyword">assert</span>(b2.shape == (n_y,   <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将参数封装</span></span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>:W1,</span><br><span class="line">                  <span class="string">"b1"</span>:b1,</span><br><span class="line">                  <span class="string">"W2"</span>:W2,</span><br><span class="line">                  <span class="string">"b2"</span>:b2&#125;</span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>注意：</p><p>权重w本来的维度应该是(前一层神经元数，本层神经元数)，但是为了方便后续计算省略掉此处的转置操作，在一开始就选择将W的维度设置为(本层神经元数，前一层大小神经元数)</p><p>而偏差b由于需要整层网络共享，我们基于Python的广播机制，选择让它的大小设置为(本层神经元数，1)</p></li></ol><ol><li><p>正向线性传播</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_forward</span><span class="params">(A0, W1, b1)</span>:</span></span><br><span class="line">    Z1 = np.dot(W1, A0) + b1</span><br><span class="line">    <span class="comment"># 即Z1 = W1 * A0 + b1</span></span><br><span class="line">    <span class="keyword">assert</span>(Z1.shape == (W1.shape[<span class="number">0</span>], A0.shape[<span class="number">1</span>]))</span><br><span class="line">    </span><br><span class="line">    cache = (A0, W1, b1)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> Z1, cache</span><br></pre></td></tr></table></figure><p>无特别说明</p></li></ol><ol><li><p>正向激活</p><p>隐藏层使用ReLU(整流线性单元)函数，输出层使用softmax函数，期望它的值符合0~9十个分类。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu</span><span class="params">(Z1)</span>:</span></span><br><span class="line">    A1 = np.maximum(<span class="number">0</span>, Z1) </span><br><span class="line">    cache = Z1</span><br><span class="line">    <span class="keyword">return</span> A1, cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(Z2)</span>:</span></span><br><span class="line">    A2 = np.exp(Z2) / sum(np.exp(Z2))</span><br><span class="line">    </span><br><span class="line">    cache = Z2</span><br><span class="line">    <span class="keyword">return</span> A2, cache</span><br></pre></td></tr></table></figure></li></ol><ol><li><p>正向传播函数</p><p>将线性传播部分和激活函数合并</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_forward_relu</span><span class="params">(A0, W1, b1)</span>:</span></span><br><span class="line">    Z1, linear_cache = linear_forward(A0, W1, b1)</span><br><span class="line">    A1, activation_cache = relu(Z1)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(A1.shape == (W.shape[<span class="number">0</span>], A0.shape[<span class="number">1</span>]))</span><br><span class="line">    cache = (linear_cache, activation_cache)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> A1, cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_forward_softmax</span><span class="params">(A1, W2, b2)</span>:</span></span><br><span class="line">    Z2, linear_cache = linear_forward(A1, W2, b2)</span><br><span class="line">    A2, activation_cache = softmax(Z2)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(A2.shape == (W2.shape[<span class="number">0</span>], A1.shape[<span class="number">1</span>]))</span><br><span class="line">    cache = (linear_cache, activation_cache)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> A2, cache</span><br></pre></td></tr></table></figure></li></ol><h2 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h2><ol><li><p>计算成本</p><p>使用交叉熵作为成本函数，其函数的公式为：</p><script type="math/tex; mode=display">\sum_{i}^{m} y_i\log{\hat{y_{i}}}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(A2, Y)</span>:</span></span><br><span class="line">    <span class="comment"># 计算交叉熵</span></span><br><span class="line">    m = Y.shape[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算loss函数</span></span><br><span class="line">    logprobs = <span class="number">-1</span>* sum(np.multiply(np.log(A2), Y))</span><br><span class="line">    <span class="comment"># 成本函数就是将整个训练集上的损失相加取相反数</span></span><br><span class="line">    cost = <span class="number">-1</span>/m * np.sum(logprobs)</span><br><span class="line">    cost = np.squeeze(cost)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cost, logprobs</span><br></pre></td></tr></table></figure></li></ol><ol><li><p>反向传播</p><p>计算各个参数基于成本的梯度，用于更新参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_propagation</span><span class="params">(parameters, cache, X, Y)</span>:</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    W1 = parameters[<span class="string">'W1'</span>]</span><br><span class="line">    W2 = parameters[<span class="string">'W2'</span>]</span><br><span class="line">    </span><br><span class="line">    A1 = cache[<span class="string">'A1'</span>]</span><br><span class="line">    A2 = cache[<span class="string">'A2'</span>]</span><br><span class="line">    </span><br><span class="line">    dZ2 = A2 - Y</span><br><span class="line">    dW2 = <span class="number">1</span>/m * np.dot(dZ2, A1.T)</span><br><span class="line">    db2 = <span class="number">1</span>/m * np.sum(dZ2, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    dA1 = np.dot(W2.T, dZ2)</span><br><span class="line">    dZ1 = np.array(dA1, copy=<span class="literal">True</span>)</span><br><span class="line">    dZ1[Z1 &lt;=<span class="number">0</span> ] = <span class="number">0</span> <span class="comment"># 标记一下，操作十分神奇,牛逼！</span></span><br><span class="line">    dW1 = <span class="number">1</span>/m * np.dot(dZ1, X.T)</span><br><span class="line">    db1 = <span class="number">1</span>/m * np.sum(dZ1, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    grads=&#123;<span class="string">"dW1"</span>:dW1,</span><br><span class="line">           <span class="string">"db1"</span>:db1,</span><br><span class="line">           <span class="string">"dW2"</span>:dW2,</span><br><span class="line">           <span class="string">"db2"</span>:db2&#125;</span><br><span class="line">    <span class="keyword">return</span> grads</span><br></pre></td></tr></table></figure><p>各参数传播的顺序和算式如下：</p><img src="/2019/11/28/%E7%94%A8numpy%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E5%8F%8C%E5%B1%82%E7%9A%84softmax%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/propagation.jpg" class="" title="参数传播"></li><li><p>更新参数</p><p>使用mini-batch梯度下降法更新参数，目前仅实现了梯度下降法，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span><span class="params">(parameters, grads, learning_rate=<span class="number">1.2</span>)</span>:</span></span><br><span class="line">    W1 = parameters[<span class="string">'W1'</span>]</span><br><span class="line">    b1 = parameters[<span class="string">'b1'</span>]</span><br><span class="line">    W2 = parameters[<span class="string">'W2'</span>]</span><br><span class="line">    b2 = parameters[<span class="string">'b2'</span>]</span><br><span class="line">    </span><br><span class="line">    dW1 = grads[<span class="string">'dW1'</span>]</span><br><span class="line">    db1 = grads[<span class="string">'db1'</span>]</span><br><span class="line">    dW2 = grads[<span class="string">'dW2'</span>]</span><br><span class="line">    db2 = grads[<span class="string">'db2'</span>]</span><br><span class="line">    </span><br><span class="line">    W1 = W1 - learning_rate* dW1</span><br><span class="line">    b1 = b1 - learning_rate* db1</span><br><span class="line">    W2 = W2 - learning_rate* dW2</span><br><span class="line">    b2 = b2 - learning_rate* db2</span><br><span class="line">    </span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</span><br><span class="line">                  <span class="string">"b1"</span>: b1,</span><br><span class="line">                  <span class="string">"W2"</span>: W2,</span><br><span class="line">                  <span class="string">"b2"</span>: b2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure></li></ol><h2 id="整体模型"><a href="#整体模型" class="headerlink" title="整体模型"></a>整体模型</h2><p>将上述函数等无缝合并，方便转换</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nn_model</span><span class="params">(X, Y, n_h=<span class="number">1000</span>, learning_rate=<span class="number">0.12</span> ,num_iterations=<span class="number">10000</span>, print_cost=False)</span>:</span></span><br><span class="line">    n_x = <span class="number">784</span></span><br><span class="line">    n_h = <span class="number">1000</span></span><br><span class="line">    n_y = <span class="number">10</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 随机初始化参数</span></span><br><span class="line">    np.random.seed(<span class="number">3</span>)</span><br><span class="line">    W1 = np.random.randn(n_h, n_x) * np.sqrt(<span class="number">2.</span> / n_x)</span><br><span class="line">    b1 = np.zeros((n_h, <span class="number">1</span>))</span><br><span class="line">    W2 = np.random.randn(n_y, n_h)  * np.sqrt(<span class="number">2.</span> / n_h)</span><br><span class="line">    b2 = np.zeros((n_y, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line">        <span class="comment"># 正向传播: 线性-&gt; ReLU -&gt; 线性-&gt; softmax</span></span><br><span class="line">        Z1 = np.dot(W1, X) + b1</span><br><span class="line">        A1 = np.maximum(<span class="number">0</span>, Z1)</span><br><span class="line">        Z2 = np.dot(W2, A1) + b2</span><br><span class="line">        A2 = np.exp(Z2) / sum(np.exp(Z2))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算成本：计算交叉熵成本</span></span><br><span class="line">        logprobs = <span class="number">-1</span>* sum(np.multiply(np.log(A2), Y))</span><br><span class="line">        cost = <span class="number">-1</span>/m * np.sum(logprobs)</span><br><span class="line">        cost = np.squeeze(cost)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 反向传播：梯度下降法</span></span><br><span class="line">        dZ2 = A2 - Y</span><br><span class="line">        dW2 = <span class="number">1</span>/m * np.dot(dZ2, A1.T)</span><br><span class="line">        db2 = <span class="number">1</span>/m * np.sum(dZ2, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">        dA1 = np.dot(W2.T, dZ2)</span><br><span class="line">        dZ1 = np.array(dA1, copy=<span class="literal">True</span>)</span><br><span class="line">        dZ1[Z1 &lt;=<span class="number">0</span> ] = <span class="number">0</span> <span class="comment"># 标记一下，操作十分神奇,牛逼！</span></span><br><span class="line">        dW1 = <span class="number">1</span>/m * np.dot(dZ1, X.T)</span><br><span class="line">        db1 = <span class="number">1</span>/m * np.sum(dZ1, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 更新参数</span></span><br><span class="line">        W1 -= learning_rate * dW1</span><br><span class="line">        b1 -= learning_rate * db1</span><br><span class="line">        W2 -= learning_rate * dW2</span><br><span class="line">        b2 -= learning_rate * db2</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Cost after iteration %i: %f"</span> %(i, cost))</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> W1, b1, W2, b2</span><br></pre></td></tr></table></figure><p>此处只为保留一下未mini-batch版本</p><h1 id="训练测试"><a href="#训练测试" class="headerlink" title="训练测试"></a>训练测试</h1><p>预测函数如下，即正向传播过程</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(X, W1, b1, W2, b2)</span>:</span></span><br><span class="line">    Z1 = np.dot(W1, X) + b1</span><br><span class="line">    A1 = np.maximum(<span class="number">0</span>, Z1)</span><br><span class="line">    Z2 = np.dot(W2, A1) + b2</span><br><span class="line">    A2 = np.exp(Z2) / sum(np.exp(Z2))</span><br><span class="line">    predictions = A2.argmax(axis=<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 使用argmax使得返回值predictions为和label相同的0~9标签</span></span><br><span class="line">    <span class="keyword">return</span> predictions</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line">tic = time.process_time()</span><br><span class="line">W1, b1, W2, b2= nn_model(X, Y,n_h=<span class="number">300</span>, learning_rate=<span class="number">0.08</span>, num_iterations=<span class="number">10000</span>, print_cost=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line">predictions = predict(X_test, W1, b1, W2, b2)</span><br><span class="line">toc = time.process_time()</span><br><span class="line"><span class="comment"># 时间</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Computation time = "</span> + str((toc - tic)) + <span class="string">"s"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 评估精确度</span></span><br><span class="line">correct_predictions = np.equal(predictions, label_test)</span><br><span class="line">accuracy = np.mean(correct_predictions.astype(np.float32))</span><br><span class="line">print(<span class="string">'Test Accuracy:%f'</span>%(accuracy*<span class="number">100</span>) + <span class="string">'%'</span>)</span><br></pre></td></tr></table></figure><h1 id="所有代码合并"><a href="#所有代码合并" class="headerlink" title="所有代码合并"></a>所有代码合并</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="comment"># import matplotlib.pyplot as plt</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练集有6w, 785列，前784为28*28,最后一列为标签</span></span><br><span class="line">train = pd.read_csv(<span class="string">"MNIST_Training_60K.csv"</span>,index_col=<span class="literal">False</span>,header=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 测试集有1w，784列，28*28的图片</span></span><br><span class="line">test = pd.read_csv(<span class="string">"MNIST_Test_10K.csv"</span>,index_col=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">label = train[:][<span class="string">"784"</span>]</span><br><span class="line">train = train.drop([<span class="string">"784"</span>],axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">X = train.T</span><br><span class="line">Y = np.zeros((<span class="number">10</span>, X.shape[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> range(X.shape[<span class="number">1</span>]):</span><br><span class="line">    Y[label[n]][n] = <span class="number">1</span></span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nn_model</span><span class="params">(X, Y, n_h=<span class="number">1000</span>, learning_rate=<span class="number">0.12</span> ,num_iterations=<span class="number">10000</span>, print_cost=False)</span>:</span></span><br><span class="line">    n_x = <span class="number">784</span></span><br><span class="line">    n_y = <span class="number">10</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 随机初始化参数</span></span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    W1 = np.random.randn(n_h, n_x) * np.sqrt(<span class="number">2.</span> / n_x)</span><br><span class="line">    b1 = np.zeros((n_h, <span class="number">1</span>))</span><br><span class="line">    W2 = np.random.randn(n_y, n_h)  * np.sqrt(<span class="number">2.</span> / n_h)</span><br><span class="line">    b2 = np.zeros((n_y, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line">        <span class="comment"># 正向传播: 线性-&gt; ReLU -&gt; 线性-&gt; softmax</span></span><br><span class="line">        Z1 = np.dot(W1, X) + b1</span><br><span class="line">        A1 = np.maximum(<span class="number">0</span>, Z1)</span><br><span class="line">        Z2 = np.dot(W2, A1) + b2</span><br><span class="line">        A2 = np.exp(Z2) / sum(np.exp(Z2))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算成本：计算交叉熵成本</span></span><br><span class="line">        logprobs = <span class="number">-1</span>* sum(np.multiply(np.log(A2), Y))</span><br><span class="line">        cost = <span class="number">-1</span>/m * np.sum(logprobs)</span><br><span class="line">        cost = np.squeeze(cost)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 反向传播：梯度下降法</span></span><br><span class="line">        dZ2 = A2 - Y</span><br><span class="line">        dW2 = <span class="number">1</span>/m * np.dot(dZ2, A1.T)</span><br><span class="line">        db2 = <span class="number">1</span>/m * np.sum(dZ2, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">        dA1 = np.dot(W2.T, dZ2)</span><br><span class="line">        dZ1 = np.array(dA1, copy=<span class="literal">True</span>)</span><br><span class="line">        dZ1[Z1 &lt;=<span class="number">0</span> ] = <span class="number">0</span> <span class="comment"># 标记一下，操作十分神奇,牛逼！</span></span><br><span class="line">        dW1 = <span class="number">1</span>/m * np.dot(dZ1, X.T)</span><br><span class="line">        db1 = <span class="number">1</span>/m * np.sum(dZ1, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 更新参数</span></span><br><span class="line">        W1 -= learning_rate * dW1</span><br><span class="line">        b1 -= learning_rate * db1</span><br><span class="line">        W2 -= learning_rate * dW2</span><br><span class="line">        b2 -= learning_rate * db2</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Cost after iteration %i: %f"</span> %(i, cost))</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> W1, b1, W2, b2</span><br><span class="line"></span><br><span class="line">label_test = test[:][<span class="string">"784"</span>]</span><br><span class="line">test1 = test.drop([<span class="string">"784"</span>],axis=<span class="number">1</span>)</span><br><span class="line">X_test = test1.T</span><br><span class="line"><span class="comment"># Y_test = np.eye(10)[l]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line">tic = time.process_time()</span><br><span class="line">W1, b1, W2, b2= nn_model(X, Y,n_h=<span class="number">300</span>, learning_rate=<span class="number">0.08</span>, num_iterations=<span class="number">10000</span>, print_cost=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line">Z1 = np.dot(W1, X_test) + b1</span><br><span class="line">A1 = np.maximum(<span class="number">0</span>, Z1)</span><br><span class="line">Z2 = np.dot(W2, A1) + b2</span><br><span class="line">A2 = np.exp(Z2) / sum(np.exp(Z2))</span><br><span class="line">predictions = A2.argmax(axis=<span class="number">0</span>)</span><br><span class="line">toc = time.process_time()</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Computation time = "</span> + str((toc - tic)) + <span class="string">"s"</span>)</span><br><span class="line"></span><br><span class="line">correct_predictions = np.equal(predictions, label_test)</span><br><span class="line">accuracy = np.mean(correct_predictions.astype(np.float32))</span><br><span class="line">print(<span class="string">'Test Accuracy:%f'</span>%(accuracy*<span class="number">100</span>) + <span class="string">'%'</span>)</span><br></pre></td></tr></table></figure><h1 id="添加mini-batch"><a href="#添加mini-batch" class="headerlink" title="添加mini-batch"></a>添加mini-batch</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练集有6w, 785列，前784为28*28,最后一列为标签</span></span><br><span class="line">train = pd.read_csv(<span class="string">"./MNIST_Training_60K.csv"</span>, index_col=<span class="literal">False</span>, header=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 测试集有1w，784列，28*28的图片</span></span><br><span class="line">test = pd.read_csv(<span class="string">"./MNIST_Test_10K.csv"</span>, index_col=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">label = train[:][<span class="string">"784"</span>]</span><br><span class="line">train = train.drop([<span class="string">"784"</span>], axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">X = train.T</span><br><span class="line">Y = np.zeros((<span class="number">10</span>, X.shape[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> range(X.shape[<span class="number">1</span>]):</span><br><span class="line">    Y[label[n]][n] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nn_model</span><span class="params">(X, Y, n_h=<span class="number">1000</span>, mini_batch_size=<span class="number">100</span>, learning_rate=<span class="number">0.08</span>, epoch=<span class="number">1000</span>, print_cost=False)</span>:</span></span><br><span class="line">    n_x = <span class="number">784</span></span><br><span class="line">    n_y = <span class="number">10</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 随机初始化参数</span></span><br><span class="line">    np.random.seed(<span class="number">3</span>)</span><br><span class="line">    W1 = np.random.randn(n_h, n_x) * np.sqrt(<span class="number">2.</span> / n_x)</span><br><span class="line">    b1 = np.zeros((n_h, <span class="number">1</span>))</span><br><span class="line">    W2 = np.random.randn(n_y, n_h) * np.sqrt(<span class="number">2.</span> / n_h)</span><br><span class="line">    b2 = np.zeros((n_y, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    costs=[]</span><br><span class="line">    complete_numof_mini_batch = m // mini_batch_size</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, epoch):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, complete_numof_mini_batch):</span><br><span class="line">            <span class="comment"># 正向传播: 线性-&gt; ReLU -&gt; 线性-&gt; softmax</span></span><br><span class="line">            mini_batch_X = X.values[:, (j * mini_batch_size): (j + <span class="number">1</span>) * mini_batch_size]</span><br><span class="line">            mini_batch_Y = Y[:, (j * mini_batch_size): (j + <span class="number">1</span>) * mini_batch_size]</span><br><span class="line">            Z1 = np.dot(W1, mini_batch_X) + b1</span><br><span class="line">            A1 = np.maximum(<span class="number">0</span>, Z1)</span><br><span class="line">            Z2 = np.dot(W2, A1) + b2</span><br><span class="line">            A2 = np.exp(Z2) / sum(np.exp(Z2))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 计算成本：计算交叉熵成本</span></span><br><span class="line">            logprobs = <span class="number">-1</span> * sum(np.multiply(np.log(A2), mini_batch_Y))</span><br><span class="line">            cost = <span class="number">-1</span> / mini_batch_size * np.sum(logprobs)</span><br><span class="line">            cost = np.squeeze(cost)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 反向传播：梯度下降法</span></span><br><span class="line">            dZ2 = A2 - mini_batch_Y</span><br><span class="line">            dW2 = <span class="number">1</span> / mini_batch_size * np.dot(dZ2, A1.T)</span><br><span class="line">            db2 = <span class="number">1</span> / mini_batch_size * np.sum(dZ2, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">            dA1 = np.dot(W2.T, dZ2)</span><br><span class="line">            dZ1 = np.array(dA1, copy=<span class="literal">True</span>)</span><br><span class="line">            dZ1[Z1 &lt;= <span class="number">0</span>] = <span class="number">0</span>  <span class="comment"># 标记一下，操作十分神奇,牛逼！</span></span><br><span class="line">            dW1 = <span class="number">1</span> / mini_batch_size * np.dot(dZ1, mini_batch_X.T)</span><br><span class="line">            db1 = <span class="number">1</span> / mini_batch_size * np.sum(dZ1, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 更新参数</span></span><br><span class="line">            W1 -= learning_rate * dW1</span><br><span class="line">            b1 -= learning_rate * db1</span><br><span class="line">            W2 -= learning_rate * dW2</span><br><span class="line">            b2 -= learning_rate * db2</span><br><span class="line">            <span class="keyword">if</span> print_cost <span class="keyword">and</span> (j+<span class="number">1</span>) % <span class="number">500</span>:</span><br><span class="line">                costs.append(cost)</span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> (i+<span class="number">1</span>) % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Cost after epoch [%3d/%3d]: %f"</span> % (i+<span class="number">1</span>, epoch, cost))</span><br><span class="line"></span><br><span class="line">    plt.plot(costs)</span><br><span class="line">    plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'iterations (per 500)'</span>)</span><br><span class="line">    plt.title(<span class="string">"Learning rate = "</span> + str(learning_rate))</span><br><span class="line">    plt.show()</span><br><span class="line">    <span class="keyword">return</span> W1, b1, W2, b2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">label_test = test[:][<span class="string">"784"</span>]</span><br><span class="line">test1 = test.drop([<span class="string">"784"</span>], axis=<span class="number">1</span>)</span><br><span class="line">X_test = test1.T</span><br><span class="line"><span class="comment"># Y_test = np.eye(10)[l]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line">mini_batch_size = <span class="number">100</span></span><br><span class="line">tic = time.process_time()</span><br><span class="line">W1, b1, W2, b2 = nn_model(X, Y, n_h=<span class="number">1000</span>, mini_batch_size=mini_batch_size, learning_rate=<span class="number">0.1</span>, epoch=<span class="number">100</span>, print_cost=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line">complete_numof_mini_batch_test = X_test.shape[<span class="number">1</span>] // mini_batch_size</span><br><span class="line">accuracy = []</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">0</span>, complete_numof_mini_batch_test):</span><br><span class="line">    mini_batch_X_test = X_test.values[:, (k * mini_batch_size):((k+<span class="number">1</span>) * mini_batch_size)]</span><br><span class="line">    mini_batch_label = label_test.values[(k * mini_batch_size): (k+<span class="number">1</span>) * mini_batch_size]</span><br><span class="line">    Z1 = np.dot(W1, mini_batch_X_test) + b1</span><br><span class="line">    A1 = np.maximum(<span class="number">0</span>, Z1)</span><br><span class="line">    Z2 = np.dot(W2, A1) + b2</span><br><span class="line">    A2 = np.exp(Z2) / sum(np.exp(Z2))</span><br><span class="line">    predictions = A2.argmax(axis=<span class="number">0</span>)</span><br><span class="line">    correct_predictions = np.equal(predictions, mini_batch_label)</span><br><span class="line">    accuracy.append(np.mean(correct_predictions.astype(np.float32)))</span><br><span class="line"></span><br><span class="line">accuracy = np.mean(accuracy)</span><br><span class="line">toc = time.process_time()</span><br><span class="line">print(<span class="string">"Computation time = "</span> + str((toc - tic)) + <span class="string">"s"</span>)</span><br><span class="line">print(<span class="string">'Test Accuracy:%f'</span> % (accuracy * <span class="number">100</span>) + <span class="string">'%'</span>)</span><br></pre></td></tr></table></figure><img src="/2019/11/28/%E7%94%A8numpy%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E5%8F%8C%E5%B1%82%E7%9A%84softmax%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/costs.png" class="" title="cost图"><p>代码与cost如上图，目前改进就是如此了，将X分成100样本一小份，然后逐份进行训练，由于参数共享，所以只有反向传播时涉及到了Y，别处都不需要修改太多。后续具体参数还需要继续调整，敬请期待！</p><p>最后调整好的准确率和时间如下：</p><img src="/2019/11/28/%E7%94%A8numpy%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E5%8F%8C%E5%B1%82%E7%9A%84softmax%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/accuracy.png" class="" title="准确率和时间">]]></content>
      
      
      <categories>
          
          <category> deeplearning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 微机课设 </tag>
            
            <tag> numpy </tag>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>第一篇博客</title>
      <link href="/2019/11/21/%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/"/>
      <url>/2019/11/21/%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/</url>
      
        <content type="html"><![CDATA[<h1 id="0-最近事务"><a href="#0-最近事务" class="headerlink" title="0. 最近事务"></a>0. 最近事务</h1><h2 id="0-1-平时课程上"><a href="#0-1-平时课程上" class="headerlink" title="0.1 平时课程上"></a>0.1 平时课程上</h2><h3 id="0-1-1-微机原理"><a href="#0-1-1-微机原理" class="headerlink" title="0.1.1 微机原理"></a>0.1.1 微机原理</h3><p>[ ] 需要我<u>读完CAAE的论文</u>简介然后完善好最后的<strong>文献报告</strong><br>[x] 写一下softmax神经网络的python代码<br>[ ] 考完试开始康康汇编，学着用它写神经网络<br>[ ] CS231n的日常学习</p><h3 id="0-1-2-考试开始了"><a href="#0-1-2-考试开始了" class="headerlink" title="0.1.2 考试开始了"></a>0.1.2 考试开始了</h3><p>[ ] 编译原理期末考试</p><p>[x]概率论期末考试</p><hr><h2 id="0-2-项目里面"><a href="#0-2-项目里面" class="headerlink" title="0.2 项目里面"></a>0.2 项目里面</h2><h3 id="0-2-1-专利"><a href="#0-2-1-专利" class="headerlink" title="0.2.1 专利"></a>0.2.1 专利</h3><h3 id="0-2-2-原本代码的复现完善"><a href="#0-2-2-原本代码的复现完善" class="headerlink" title="0.2.2 原本代码的复现完善"></a>0.2.2 原本代码的复现完善</h3><h3 id="0-2-3-读新的论文，并结合来做失踪人口回归的项目"><a href="#0-2-3-读新的论文，并结合来做失踪人口回归的项目" class="headerlink" title="0.2.3 读新的论文，并结合来做失踪人口回归的项目"></a>0.2.3 读新的论文，并结合来做失踪人口回归的项目</h3>]]></content>
      
      
      <categories>
          
          <category> Todo </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 随便写写 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
