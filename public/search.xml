<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>CS231n-Week3-Assignment4</title>
      <link href="/2019/12/09/CS231n-Week3-Assignment4/"/>
      <url>/2019/12/09/CS231n-Week3-Assignment4/</url>
      
        <content type="html"><![CDATA[<h1 id="知识点"><a href="#知识点" class="headerlink" title="知识点"></a>知识点</h1><h2 id="上节内容"><a href="#上节内容" class="headerlink" title="上节内容"></a>上节内容</h2><p>图像分类问题，机器看到的图像是像素值矩阵</p><p>机器识别图像的困难在于语义鸿沟，即识别图片中的事物即含义</p><p>两个算法：KNN算法和线性分类器，KNN算法相对线性分类器而言没有可优化的权重矩阵W</p><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><h3 id="损失函数的通用表达式"><a href="#损失函数的通用表达式" class="headerlink" title="损失函数的通用表达式"></a>损失函数的通用表达式</h3><script type="math/tex; mode=display">L(W) = \frac{1}{N} \sum_{i=1}^{N}L_i(f(x_i, W), y_i)</script><p>损失函数用于衡量W的优良</p><a id="more"></a><h3 id="多分类SVM损失函数-Hinge-loss-合页损失函数"><a href="#多分类SVM损失函数-Hinge-loss-合页损失函数" class="headerlink" title="多分类SVM损失函数(Hinge loss 合页损失函数)"></a>多分类SVM损失函数(Hinge loss 合页损失函数)</h3><script type="math/tex; mode=display">L_i = \sum_{j \neq y_i} \max{(0,s_j-s_{y_i}+1)}</script><p>该函数存在一个问题：L=0时的w不唯一，使用正则项可以解决这个问题</p><h3 id="Softmax-分类器-Multinomial-Logistic-Regression"><a href="#Softmax-分类器-Multinomial-Logistic-Regression" class="headerlink" title="Softmax 分类器(Multinomial Logistic Regression)"></a>Softmax 分类器(Multinomial Logistic Regression)</h3><script type="math/tex; mode=display">P(Y=k \mid X=x_i) = \frac{e_{s_k}}{\sum_j e_{s_j}} \quad  where \quad s = f(x_i;W)</script><h2 id="优化方法"><a href="#优化方法" class="headerlink" title="优化方法"></a>优化方法</h2><h3 id="随即搜索-不可取"><a href="#随即搜索-不可取" class="headerlink" title="随即搜索(不可取)"></a>随即搜索(不可取)</h3><p>从众多的随机数中挑选组成参数w,并使用其计算loss，挑选其中loss最小的</p><h3 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h3><p>需要掌握：<br>梯度的含义<br>学习率<br>梯度下降法<br>随机梯度下降法(minibatch取值)<br>带动量的梯度下降<br>Adam</p><h3 id="图像分类中的两步走方法"><a href="#图像分类中的两步走方法" class="headerlink" title="图像分类中的两步走方法"></a>图像分类中的两步走方法</h3><p>先提取图像的某种<strong>特征</strong>，然后进行模型的学习<br><strong>特征：</strong>颜色分布，HOG， SIFT特征，bag of words</p><h1 id="网课学习"><a href="#网课学习" class="headerlink" title="网课学习"></a>网课学习</h1><h2 id="损失函数-1"><a href="#损失函数-1" class="headerlink" title="损失函数"></a>损失函数</h2><p>使用损失函数来衡量W的好坏</p><h3 id="多分类支持向量机损失-Hinge-Loss"><a href="#多分类支持向量机损失-Hinge-Loss" class="headerlink" title="多分类支持向量机损失(Hinge Loss)"></a>多分类支持向量机损失(Hinge Loss)</h3><p>损失函数公式如下：</p><script type="math/tex; mode=display">L_i = \sum_{j \neq y_i} \max{(0,s_j-s_{y_i}+1)} \quad  where \quad s = f(x_i;W)</script><p>含义：s的含义就是在分类器$f(x_i, W)$得到的<strong>关于第i个类的得分</strong>，而左侧的含义就是，所有错误类的得分与这个正确类得分的差超过-1,的话就将其(它们的差+1)加入损失中，因为不超过-1的话可以勉强算这个类别的分数不会影响到正确类的评判。</p><p>公式的图像如下：</p><img src="/2019/12/09/CS231n-Week3-Assignment4/Hingeloss.png" class="" title="HingeLoss图"><p>函数存在一个问题：令L=0的W不唯一，解决办法：在后面加上一个正则项</p><script type="math/tex; mode=display">L(W) = \frac{1}{N} \sum_{i=1}^{N}L_i(f(x_i, W), y_i) + \lambda R(W)</script><p>$\lambda$超参数用于平衡正则项与原损失函数</p><p>常用正则项：</p><p>L2 正则项：$R(W) = \sum_k \sum_l W^2_{k,l}$</p><p>L1正则项：$R(W) = \sum_k \sum_l \lvert W_{k,l} \rvert$</p><p>L1+L2：    $R(W) = \sum_k \sum_l (\beta W^2_{k,l} + \lvert W_{k,l} \rvert)$</p><p>后续：最大规范正则化， 随机失活 =&gt; 批量归一化，随即深度</p><h3 id="Softmax-分类器-Multinomial-Logistic-Regression-1"><a href="#Softmax-分类器-Multinomial-Logistic-Regression-1" class="headerlink" title="Softmax 分类器(Multinomial Logistic Regression)"></a>Softmax 分类器(Multinomial Logistic Regression)</h3><p>概率函数：</p><script type="math/tex; mode=display">P(Y=k \mid X=x_i) = \frac{e_{s_k}}{\sum_j e_{s_j}} \quad  where \quad s = f(x_i;W)</script><p>损失函数：</p><script type="math/tex; mode=display">L_i = -\log{P(Y = y_i \mid X = x_i )}</script><p>这个损失函数的可解释性更强，因为所有类别的分数都可以称作是概率，而所有类的概率加起来等于1，而我们的优化就是需要去找到一个W，来匹配真实的目标概率分布，即正确类的概率接近1，别的接近0</p><h3 id="Softmax-Loss-vs-Hinge-Loss"><a href="#Softmax-Loss-vs-Hinge-Loss" class="headerlink" title="Softmax Loss vs Hinge Loss"></a>Softmax Loss vs Hinge Loss</h3><p> Hinge Loss 终究关心的是正确类比错误类的分值高出一个安全边界，所以在正确类的分支稍微做出改变时不会有任何影响，而Softmax需要正确类的概率一直升高，目标是占比无穷大</p><h3 id="Softmax损失对输入推导："><a href="#Softmax损失对输入推导：" class="headerlink" title="Softmax损失对输入推导："></a>Softmax损失对输入推导：</h3><script type="math/tex; mode=display">\frac{\vartheta{L}}{\vartheta{x_i}} {= {-\sum_k y_k \frac{\vartheta{\log p_k}}{\vartheta{x_i}}}\\ ={-\sum_k y_k \frac{1}{p_k} \frac{\vartheta{p_k}}{\vartheta{x_i}}}  \\=-y_i (1-p_i)- \sum_{k \neq i} y_k \frac{1}{p_k}(-p_kp_i) \\ ={-y_i (1-p_i)+ \sum_{k \neq i}y_k(p_i) }  \\={-y_i + y_ip_i + \sum_{k \neq i}y_k(p_i)} \\ ={p_i(\sum_k y_k) - y_i}  ={p_i - y_i}}</script>]]></content>
      
      
      <categories>
          
          <category> CS231n </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CS231n </tag>
            
            <tag> Softmax </tag>
            
            <tag> Loss </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CS231n-Week2-WeekSummary</title>
      <link href="/2019/12/08/CS231n-Week2-WeekSummary/"/>
      <url>/2019/12/08/CS231n-Week2-WeekSummary/</url>
      
        <content type="html"><![CDATA[<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><h2 id="知识点"><a href="#知识点" class="headerlink" title="知识点"></a>知识点</h2><p>CIFAR-10数据集，5w训练，1w测试，10class，32x32像素</p><p>距离度量方法：</p><p><strong>L1范数</strong>：$ d_1 (I_1, I_2) = \sum_p \lvert I_1^p - I_2^P\rvert$</p><a id="more"></a><p><strong>L2范数</strong>：$d_2(I_1, I_2 = \sqrt{\sum_p(I_1^p - I_2^p)^2}$</p><p>设置超参数较好的方法：</p><p>１．将数据分成训练集，验证集和测试集，使用多组超参数在训练集中运行，并在验证集上进行验证， 并且使用在验证集中表现最好的，放入测试集中运行，看看效果(要保证验证集和测试集完全分隔)</p><p>这样得到的数据才是应该写入报告的测试结果，才能展现出训练的算法在未知数据中的表现</p><p>２.  交叉验证法：(小数据集中常见)，将数据集分成多个小份，每次换超参数时使用不同的一份作为验证集</p><p>线性分类器为什么可以看成一种模板匹配方法：</p><p>线性分类器中的权重矩阵W就是找到一种使得对于各个训练数据与权重得到的结果中样本对应的类的值较大的一个方法，这就好像寻找并匹配数据中存在的分布，使得模板与数据相乘后符合这样的分布。</p>]]></content>
      
      
      <categories>
          
          <category> CS231n </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CS231n </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CS231n-Week2-Assignment3</title>
      <link href="/2019/12/05/CS231n-Week2-Assignment3/"/>
      <url>/2019/12/05/CS231n-Week2-Assignment3/</url>
      
        <content type="html"><![CDATA[<h1 id="学习："><a href="#学习：" class="headerlink" title="学习："></a>学习：</h1><h2 id="知识点："><a href="#知识点：" class="headerlink" title="知识点："></a>知识点：</h2><p>KNN算法：</p><ol><li>距离度量：<br>根据给定的度量，在训练集合中找出与x距离最近的k个点(k=1时即近邻算法)</li><li>分类决策规则(k&gt;1时为KNN)<br>根据分类决策规则(如多数表决，加权方法等)决定x的类别y<br>总结：KNN算法三要素：K值的选择，距离度量，分类决策规则</li></ol><p>复杂度：<br>训练复杂度: KNN算法只是将训练数据及训练数据及标签预先保存，并不涉及某些计算，复杂度为o(1)<br>测试复杂度： 对于某个样本x，需要与标签数据逐一进行对比，计算量与数据大小有关，复杂度为o(N)</p><h2 id="线性分类器"><a href="#线性分类器" class="headerlink" title="线性分类器"></a>线性分类器</h2><p>线性分类器是最简单的一种参数模型</p><p>f(x,W)是最简单的单纯的权重矩阵W(eight)与数据x相乘，并且加上一个偏置向量b(ias)，得到的结果根据大小说明分别于那一类而言的可能性</p><p>这里的线性分类器中的Bias不与训练数据交互，只是一开始人为的添加基于先验信息的偏好设置，比如训练集中某一类的数据较多，那么会是这一类的概率较大，可以设置对应的b值较大</p><p>线性分类器中的权重矩阵W就是找到一种使得对于各个训练数据与权重得到的结果中样本对应的类的值较大的一个方法，这就好像寻找并匹配数据中存在的分布，使得模板与数据相乘后符合这样的分布。</p><h1 id="作业："><a href="#作业：" class="headerlink" title="作业："></a>作业：</h1><h2 id="f-x-w-线性分类器与knn的区别"><a href="#f-x-w-线性分类器与knn的区别" class="headerlink" title="f(x, w)线性分类器与knn的区别"></a>f(x, w)线性分类器与knn的区别</h2><p>线性分类器是训练过程中的复杂度较高而预测过程的复杂度很低，而KNN的复杂度则反过来，这使得线性分类器可以在低耗的设备上运行</p><p>KNN的计算基本只基于距离度量，而线性分类器的计算主要基于W矩阵的匹配</p><h2 id="线性分类器为什么可以看成一种模板匹配方法"><a href="#线性分类器为什么可以看成一种模板匹配方法" class="headerlink" title="线性分类器为什么可以看成一种模板匹配方法"></a>线性分类器为什么可以看成一种模板匹配方法</h2><p>线性分类器中的权重矩阵W就是找到一种使得对于各个训练数据与权重得到的结果中样本对应的类的值较大的一个方法，这就好像寻找并匹配数据中存在的分布，使得模板与数据相乘后符合这样的分布。</p><h2 id="线性分类器的缺点也就是存在的问题"><a href="#线性分类器的缺点也就是存在的问题" class="headerlink" title="线性分类器的缺点也就是存在的问题"></a>线性分类器的缺点也就是存在的问题</h2><img src="/2019/12/05/CS231n-Week2-Assignment3/%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8%E7%9A%84%E5%9B%B0%E5%A2%83.png" class="" title="线性分类器的问题"><ol><li>异或问题/奇偶问题</li><li>多分类问题</li><li>L2距离范围固定的分类问题</li></ol><h2 id="分析代码实现的任务是什么，每个函数模块实现的功能是什么，代码运行的逻辑"><a href="#分析代码实现的任务是什么，每个函数模块实现的功能是什么，代码运行的逻辑" class="headerlink" title="分析代码实现的任务是什么，每个函数模块实现的功能是什么，代码运行的逻辑"></a>分析代码实现的任务是什么，每个函数模块实现的功能是什么，代码运行的逻辑</h2><h2 id="编程作业"><a href="#编程作业" class="headerlink" title="编程作业"></a>编程作业</h2><h3 id="遇到的问题："><a href="#遇到的问题：" class="headerlink" title="遇到的问题："></a>遇到的问题：</h3><ol><li><p>第一个cell运行就报错了，data_utils.py中的<code>from scipy.misc import imread</code>这句话中找不到imread，因为在新版本的scipy中,imread已经被移除了，取而代之的使用<code>from imageio import imread</code></p></li><li><p>紧接着下一个cell又遇见了问题，找不到数据集，这个通过运行datasets文件夹中的”get_datasets.sh”，如果无效，则复制其中的网址手动下载放入该文件夹下后，将网址那行代码加井号注释掉再运行，就会将tar.gz压缩文件解压好，此时再运行第二cell就没问题了</p></li><li><p>第五个cell，即加载KNN分类器时又报错了，因为没有past模块，这个模块需要自己安装，令人啼笑皆非的是需要装的竟然叫<strong>future</strong></p></li><li><p>算法基础不好的缺点显现了，找出数组中出现次数最多的一种元素，网上借鉴的有两种方法：</p><ol><li><p><code>np.argmax(np.bincount(closest_y))</code></p></li><li><p><code>from collections import Counter</code></p><p><code>Counter.most_common(1)[0][0]</code></p></li></ol></li><li><p>two_loops的代码总是非常的慢，后来发现用<code>np.sum</code>代替<code>sum</code>会节省大量的时间，果然矩阵运算还是厉害的不行</p></li><li><p>最后一个问题就在我的能力之外了，我调试了很多次，哪怕把别人的代码贴过来都没整成功，我的准确率总是不随k值改变的，令人头秃的问题</p></li></ol><h3 id="模块"><a href="#模块" class="headerlink" title="模块"></a>模块</h3><ol><li>加载需要的包，设置画布的属性，并添加一些需要的函数</li><li>加载数据集并打印其训练和测试集的形状</li><li>将数据集的部分数据可视化呈现出来</li><li>随机抽取一定数量的数据做成新的训练测试集，并压缩成数量上堆叠起来的一维向量</li><li>导入KNN分类器并用随机抽取的训练样本进行训练</li><li>使用两层循环的计算距离的代码进行测试</li><li>对测试结果进行可视化(亮的地方代表距离较大，暗的地方距离较小)</li><li>运行预测标签程序，使k=1并计算准确率以测试你的L2距离的函数写的是否正确</li><li>测试k=5时的准确率是否较高</li><li>分别测试你的单层循环和无循环的距离度量函数代码结果是否准确</li><li>分别计算两层循环，单层循环和无循环函数花费的时间，体现无循环对计算的优化</li><li>交叉验证过程的函数</li><li>交叉验证准确率的可视化</li><li>测试最佳k时预测的准确率</li></ol>]]></content>
      
      
      <categories>
          
          <category> CS231n </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CS231n </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>汇编学习</title>
      <link href="/2019/12/04/%E6%B1%87%E7%BC%96%E5%AD%A6%E4%B9%A0/"/>
      <url>/2019/12/04/%E6%B1%87%E7%BC%96%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<blockquote><p>  本文为学习了jiftle大神翻译并写下的通俗易懂的《<a href="https://www.cnblogs.com/jiftle/p/8453106.html" target="_blank" rel="noopener">X86汇编快速入门</a>》后的记录  </p></blockquote><h1 id="寄存器部分"><a href="#寄存器部分" class="headerlink" title="寄存器部分"></a>寄存器部分</h1><img src="/2019/12/04/%E6%B1%87%E7%BC%96%E5%AD%A6%E4%B9%A0/x86-registers.png" class="" title="x86处理器的主要寄存器"><p>如图，EAX用于计算，ECX用于循环变量计数，ESP指示栈指针(用于指示栈顶位置)，EBP是基址指针(用于指示子程序或者函数调用的基址指针)。</p><p>并且，EAX,EBX, ECX和EDX的前两个高位字节和后两个低位字节可以独立使用，其中两位低字节又被独立分为H和L部分，这样做是考虑兼容16位程序。</p><p>应用寄存器时，名称大小写不敏感，eax与EAX无差别</p><h1 id="内存和寻址方式"><a href="#内存和寻址方式" class="headerlink" title="内存和寻址方式"></a>内存和寻址方式</h1><h2 id="声明静态数据区"><a href="#声明静态数据区" class="headerlink" title="声明静态数据区"></a>声明静态数据区</h2><p>X86汇编语言中可以使用”.DATA”指令声明静态数据区(类似于全局变量)，数据以单字节(<strong>DB</strong>)，双字节(<strong>DW</strong>)或4字节(<strong>DD</strong>)存放，分别用DB,DW,DD指令表示声明内存的长度。在汇编语言中，相邻定义的标签在内存中连续存放</p><p>额外的，还可以声明连续的数据和数组，声明数组时使用<strong>DUP</strong>关键字</p><div class="table-container"><table><thead><tr><th>.DATA</th><th></th><th></th></tr></thead><tbody><tr><td>标签</td><td>指令</td><td>作用</td></tr><tr><td>var</td><td>DB 64</td><td>声明一个字节，并将数值64放入此字节</td></tr><tr><td>var2</td><td>DB ?</td><td>声明一个未初始化的字节</td></tr><tr><td></td><td>DB 10</td><td>声明一个没有标签的字节，值为10</td></tr><tr><td>X</td><td>DW ?</td><td>声明一个未初始化的双字节</td></tr><tr><td>Y</td><td>DD 30000</td><td>声明一个4字节，其值为30000</td></tr><tr><td>Z</td><td>DD 1, 2, 3</td><td>声明三个4字节的值，分别初始化为1,2,3，地址为Z+8的值会是3</td></tr><tr><td>bytes</td><td>DB 10 DUP(?)</td><td>在bytes的位置处开始声明10个未初始化的字节</td></tr><tr><td>arr</td><td>DD 100 DUP(0)</td><td>在arr位置处开始声明100个4字节字符，全部初始化为0</td></tr><tr><td>str</td><td>DB ‘hello’, 0</td><td>在str地址处开始声明6个字节,初始化为hello字母的ASCII值和一个0</td></tr></tbody></table></div><h2 id="寻址方式"><a href="#寻址方式" class="headerlink" title="寻址方式"></a>寻址方式</h2><p>现代的x86处理器具有$2^{32}$字节的寻址空间，在上面的例子中，我们用标签(label)表示内存区域，这些标签在实际汇编时，均被32位的实际地址代替。</p><p>除了支持这种直接的内存区域描述，x86还提供了一种灵活的内存寻址方式，即利用最多两个32位的寄存器和一个32位的有符号常数相加计算一个内存地址，其中一个寄存器可以左移1、2或3位以表述更大的空间。例：</p><div class="table-container"><table><thead><tr><th>指令</th><th>含义</th></tr></thead><tbody><tr><td>mov eax, [ebx]</td><td>将ebx值指示的内存地址中的4个字节传送到eax中</td></tr><tr><td>mov [var], ebx</td><td>将ebx的内容传送到[var]的值指示的内存地址中</td></tr><tr><td>mov eax, [esi-4]</td><td>将esi-4值指示的内存地址中的4个字节传送到eax中</td></tr><tr><td>mov [esi+eax], cl</td><td>将cl的值传送到esi + eax的值指示的内存地址中</td></tr><tr><td>mov  edx, [esi + 4*ebx]</td><td>将esi + 4*ebx值指示的内存中的4个字节传送到edx</td></tr></tbody></table></div><p>注意：方括号中的两个标签之间只能使用加法，并且最多只能有两个寄存器参与运算</p><h2 id="长度规定"><a href="#长度规定" class="headerlink" title="长度规定"></a>长度规定</h2><p>汇编中，一般用DB,DW,DD声明内存空间的大小</p><p>而在mov [edx], 2中，如果没有特殊的标识，就不确定常数2是单字节、双字节还是4字节，x86提供了三个指示规则标记，分别为BYTE PTR, WORD PTR,和DWORD PTR,上面的例子可以写成：</p><p>mov BYTE PTR [ebx], 2   ; mov WORD PTR [ebx], 2 ;   mov DWORD PTR [ebx], 2，此时意思就非常清晰</p><h1 id="汇编指令"><a href="#汇编指令" class="headerlink" title="汇编指令"></a>汇编指令</h1><p>汇编指令通常可以分为<strong>数据传送指令、逻辑计算指令和控制流指令</strong>。以下标记分别表示寄存器，内存和常数</p><div class="table-container"><table><thead><tr><th>指令</th><th>含义</th></tr></thead><tbody><tr><td><code>&lt;reg32&gt;</code></td><td>32位寄存器(EAX, EBX, ECX, EDX, ESI, EDI, ESP or EBP)</td></tr><tr><td><code>&lt;reg16&gt;</code></td><td>16位寄存器(AX, BX, CX, or DX)</td></tr><tr><td><code>&lt;reg 8 &gt;</code></td><td>8位寄存器(AH, BH, CH, DH, AL, BL, CL or DL)</td></tr><tr><td><code>&lt;reg&gt;</code></td><td>任何寄存器</td></tr><tr><td></td><td></td></tr><tr><td><code>&lt;mem&gt;</code></td><td>内存地址(例如 [eax], [var + 4]  或者dword ptr [eax+ebx]</td></tr><tr><td><code>&lt;con32&gt;</code></td><td>32位常数</td></tr><tr><td><code>&lt;con16&gt;</code></td><td>16位常数</td></tr><tr><td><code>&lt;con8&gt;</code></td><td>8位常数</td></tr><tr><td><code>&lt;con&gt;</code></td><td>任何8位，16位或32位常数</td></tr></tbody></table></div><h2 id="数据传送指令"><a href="#数据传送指令" class="headerlink" title="数据传送指令"></a>数据传送指令</h2><h3 id="mov-——移动-复制"><a href="#mov-——移动-复制" class="headerlink" title="mov ——移动(复制)"></a>mov ——移动(复制)</h3><p>(操作码：88, 89,8A,8B,8C,8E,…)</p><p>mov指令将第二个操作数(可以是寄存器的内容、内存中的内容或值)复制到第一个操作数(寄存器或内存)。mov 不能用于直接从内存复制到内存，其语法如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mov &lt;reg&gt;, &lt;reg&gt;</span><br><span class="line">mov &lt;reg&gt;, &lt;mem&gt;</span><br><span class="line">mov &lt;mem&gt;,&lt;reg&gt;</span><br><span class="line">mov &lt;reg&gt;,&lt;const&gt;</span><br><span class="line">mov &lt;mem&gt;,&lt;const&gt;</span><br><span class="line"></span><br><span class="line">比如：</span><br><span class="line">mov eax,ebx —— 将ebx的值拷贝到eax</span><br><span class="line">mov byte ptr[var],5 ——将5保存到var指示内存中的一个字节中</span><br></pre></td></tr></table></figure><h3 id="push——入栈"><a href="#push——入栈" class="headerlink" title="push——入栈"></a>push——入栈</h3><p>(操作码：FF,89.8A,8B,8C,8E,…)</p><p>push指令将操作数压入内存的栈中，栈主要用于函数调用过程中，其中ESP只是栈顶。在压栈前，首先将ESP值减4(X86栈增长方向与内存地址编号增长方向相反），然后将操作数内容压入ESP指示的位置。语法如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">push &lt;reg32&gt;</span><br><span class="line">push &lt;mem&gt;</span><br><span class="line">push &lt;con32&gt;</span><br><span class="line"></span><br><span class="line">比如：</span><br><span class="line">push eax ——将eax的内容压栈</span><br><span class="line">push [var] ——将var指示的4字节内容压栈</span><br></pre></td></tr></table></figure><h3 id="pop-——-出栈"><a href="#pop-——-出栈" class="headerlink" title="pop  —— 出栈"></a>pop  —— 出栈</h3><p>pop指令与push指令相反，它执行的是出栈的工作。它首先将ESP指示的地址中的内容出栈，然后将ESP值加4，语法如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">pop &lt;reg32&gt;</span><br><span class="line">pop &lt;mem&gt;</span><br><span class="line"></span><br><span class="line">比如：</span><br><span class="line">pop edi —— 将栈顶的元素出栈到edi中</span><br><span class="line">pop [edx] ——将栈顶元素出栈到从EBX位置开始的4个字节处</span><br></pre></td></tr></table></figure><h3 id="lea-——载入有效地址"><a href="#lea-——载入有效地址" class="headerlink" title="lea   ——载入有效地址"></a>lea   ——载入有效地址</h3><p>lea实际上是一个载入有效地址指令，将第二个操作数表示的地址载入到第一个操作数(寄存器)中，其语法如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">lea &lt;reg32&gt;,&lt;mem&gt;</span><br><span class="line"></span><br><span class="line">比如：</span><br><span class="line">lea eax,[var]——将var指示的地址载入eax中</span><br><span class="line">lea edi,[ebx+4*esi]——将ebx+4*esi表示的地址载入到edi中，这实际上是上面所说的寻址模式的一种表示方式</span><br></pre></td></tr></table></figure><h2 id="算术和逻辑指令"><a href="#算术和逻辑指令" class="headerlink" title="算术和逻辑指令"></a>算术和逻辑指令</h2><h3 id="add-——-整数加法"><a href="#add-——-整数加法" class="headerlink" title="add —— 整数加法"></a>add —— 整数加法</h3><p>add指令将两个操作数相加，且将相加后的结果保存到第一个操作数中。语法如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">add &lt;reg&gt;,&lt;reg&gt;</span><br><span class="line">add &lt;reg&gt;,&lt;mem&gt;</span><br><span class="line">add &lt;mem&gt;,&lt;reg&gt;</span><br><span class="line">add &lt;reg&gt;,&lt;con&gt;</span><br><span class="line">add &lt;mem&gt;,&lt;con&gt;</span><br><span class="line"></span><br><span class="line">比如：</span><br><span class="line">add eax,10 —— EAX ←EAX + 10</span><br><span class="line">add BYTE PTR [var], 10 —— 10与var指示的内存中的一个byte的值相加，并将结果保存在指示的内存中</span><br></pre></td></tr></table></figure><h3 id="sub-——-整数减法"><a href="#sub-——-整数减法" class="headerlink" title="sub —— 整数减法"></a>sub —— 整数减法</h3><p>sub指令指示第一个操作数减去第二个操作数，并将相减后的值保存在第一个操作数，其语法如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">sub &lt;reg&gt;,&lt;reg&gt;</span><br><span class="line">sub &lt;reg&gt;,&lt;mem&gt;</span><br><span class="line">sub &lt;mem&gt;,&lt;reg&gt;</span><br><span class="line">sub &lt;reg&gt;,&lt;con&gt;</span><br><span class="line">sub &lt;mem&gt;,&lt;con&gt;</span><br><span class="line"></span><br><span class="line">比如：</span><br><span class="line">sub al, ah —— AL←AL-AH</span><br><span class="line">sub eax, 216 —— eax中的值减26，并将计算值保存在eax中</span><br></pre></td></tr></table></figure><h3 id="inc-dec——自增自减"><a href="#inc-dec——自增自减" class="headerlink" title="inc, dec——自增自减"></a>inc, dec——自增自减</h3><p>inc, dec分别表示将操作数自加1，自减1，其语法如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">inc &lt;reg&gt;</span><br><span class="line">inc &lt;mem&gt;</span><br><span class="line">dec &lt;reg&gt;</span><br><span class="line">dec &lt;mem&gt;</span><br><span class="line"></span><br><span class="line">比如：</span><br><span class="line">dec eax —— eax中的值自减1</span><br><span class="line">inc DWORD PTR [var] —— var指示内存中的一个4字节值自加1</span><br></pre></td></tr></table></figure><h3 id="imul-——-整数相乘"><a href="#imul-——-整数相乘" class="headerlink" title="imul —— 整数相乘"></a>imul —— 整数相乘</h3><p>整数相乘指令，它有两种指令格式，一种位两个操作数，将两个操作数的值相乘，并将结果保存在第一个操作数中，第一个操作数必须为寄存器；第二种格式为三个操作数，其语义为：将第二个和第三个操作数相乘，并将结果保存在第一个操作数中，第一个操作数必须为寄存器，其语法如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">imul &lt;reg32&gt;,&lt;reg32&gt;</span><br><span class="line">imul &lt;reg32&gt;,&lt;mem&gt;</span><br><span class="line">imul &lt;reg32&gt;,&lt;reg32&gt;,&lt;con&gt;</span><br><span class="line">imul &lt;reg32&gt;,&lt;mem&gt;,&lt;con&gt;</span><br><span class="line"></span><br><span class="line">比如：</span><br><span class="line">imul eax, [var]——eax→eax * [var]</span><br><span class="line">imul esi,edi, 25——ESI→EDI * 25</span><br></pre></td></tr></table></figure><h3 id="idiv-——-整数除法"><a href="#idiv-——-整数除法" class="headerlink" title="idiv —— 整数除法"></a>idiv —— 整数除法</h3>]]></content>
      
      
      
        <tags>
            
            <tag> x86汇编 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CS231n_Week1_Assignment2</title>
      <link href="/2019/12/03/CS231n-Week1-Assignment2/"/>
      <url>/2019/12/03/CS231n-Week1-Assignment2/</url>
      
        <content type="html"><![CDATA[<h1 id="学习："><a href="#学习：" class="headerlink" title="学习："></a>学习：</h1><h2 id="数据驱动方法"><a href="#数据驱动方法" class="headerlink" title="数据驱动方法"></a>数据驱动方法</h2><p>一张图像在计算机的眼中就是一堆数字矩阵，大小为长x宽x通道数，其中每个数字在0~255之间代表像素。<br>图像即训练数据，而在分类任务中，图像的种类即label标签。<br>图像分类存在的问题与挑战：角度，光照，形变和遮挡，图片背景混乱，类内差异等</p><p>过去的尝试：<br>硬编码：计算图像边缘然后将边角进行分类，然后写一些规则对猫进行分类</p><p>现在：数据驱动方法(比深度学习更加广义)：</p><ol><li>收集大量的图片和标签数据集</li><li>使用机器学习来训练一个分类器</li><li>在新的图片上对分类器进行评估<a id="more"></a>CIFAR-10数据集：<br>拥有10种类别(:airplane:, :car:，:bird:，:cat:, :deer:, :dog:, :frog:, :horse:,  :ship:, :truck:)<br>5w训练图片，1w测试图片</li></ol><p>距离度量方法：</p><p><strong>L1范数</strong>：$ d_1 (I_1, I_2) = \sum_p \lvert I_1^p - I_2^P\rvert$</p><img src="/2019/12/03/CS231n-Week1-Assignment2/L1distance.png" class="" title="L1距离的运算演示"><p><strong>L2范数</strong>：$d_2(I_1, I_2 = \sqrt{\sum_p(I_1^p - I_2^p)^2}$</p><p>L1范数取决于你选择的坐标系统，当你转动坐标系统时将会改变它们之间的L1距离，而L2范式不会受此影响<br>因此，如果你输入的特征向量中的值对你的任务有一些重要的意义，则L1更适合一些<br>而对于比较通用的向量而言，L2会更自然一些</p><p>在实际演示中，分别使用L1，L2范数的结果中，L1的结果更受坐标轴的影响，而L2中只需要将坐标轴放在较为自然的地方即可<br><img src="/2019/12/03/CS231n-Week1-Assignment2/L1vsL2.png" class="" title="L1vsL2"></p><h2 id="K-NN-K-最近邻算法"><a href="#K-NN-K-最近邻算法" class="headerlink" title="K-NN: K-最近邻算法"></a>K-NN: K-最近邻算法</h2><p>NN算法复杂度：<br>训练时O(1)， 预测时O(N)<br>缺点：</p><ol><li>实用中由于部署设备不同，我们更希望分类器在预测时快，训练时慢是可以的</li><li>最邻近算法根据最近的点来计算决策边缘，这非常容易受到噪声等因素的影响</li></ol><p>NN与KNN的区别：<br>KNN会投票与其最近的多个点，从而使得周围的决策边缘变得平滑，即对噪声产生更大的鲁棒性。</p><p>设置超参数</p><p>分别提到四种想法：</p><ol><li><p>使用在数据集中准确率最好的超参数 </p><p>这样是不行的，因为会造成严重的过拟合，面对未知数据的性能会非常差</p></li><li><p>将数据分割成训练和测试集，并使用在测试集中性能最好的超参数</p><p>这样也是不好的，因为这样的超参数容易使分类器只在测试集中性能良好</p></li><li><p>将数据分成训练集，验证集和测试集，使用多组超参数在训练集中运行，并在验证集上进行验证， 并且使用在验证集中表现最好的，放入测试集中运行，看看效果(要保证验证集和测试集完全分隔)</p><p>这样得到的数据才是应该写入报告的测试结果，才能展现出训练的算法在未知数据中的表现</p></li><li><p>交叉验证法：(小数据集中常见)，将数据集分成多个小份，每次换超参数时使用不同的一份作为验证集</p><img src="/2019/12/03/CS231n-Week1-Assignment2/cross-validation.png" class="" title="交叉验证集的设置"></li></ol><p>KNN算法在图像分类中很少用到的原因：</p><ol><li><p>它在测试时运算时间较长，与我们的需求不符</p></li><li><p>L1，L2范数的衡量标准建立在向量上的距离函数不太适合表示图像视觉上的相似度</p><p>例子中分别遮挡，平移和改变颜色了图片后，L2范数值不变，所以并不是很适合用于表示图像之间视觉感知的差异</p></li><li><p>维度灾难，KNN算法需要的数据分布较为密集，这就对数据集的要求较高，对于图片而言就需要图片的数量较多，当分布处于较高维度时需要的样本量呈指数倍增长</p></li></ol><h1 id="作业："><a href="#作业：" class="headerlink" title="作业："></a>作业：</h1><h2 id="1-图像分类数据和label分别是什么，图像分类存在的问题与挑战"><a href="#1-图像分类数据和label分别是什么，图像分类存在的问题与挑战" class="headerlink" title="1. 图像分类数据和label分别是什么，图像分类存在的问题与挑战"></a>1. 图像分类数据和label分别是什么，图像分类存在的问题与挑战</h2><p>一张图像在计算机的眼中就是一堆数字矩阵，大小为长*宽*通道数，其中每个数字在0~255之间代表像素。<br>图像即训练数据，而在分类任务中，图像的种类即label标签。<br>图像分类存在的问题与挑战：角度，光照，形变和遮挡，图片背景混乱，类内差异等</p><h2 id="2-使用python加载一张彩色图片，观察像素值"><a href="#2-使用python加载一张彩色图片，观察像素值" class="headerlink" title="2. 使用python加载一张彩色图片，观察像素值"></a>2. 使用python加载一张彩色图片，观察像素值</h2><p>图片原图为：<br><img src="/2019/12/03/CS231n-Week1-Assignment2/prettygirl.jpg" class="" title="原图"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">img_v = plt.imread(<span class="string">'prettygirl.jpg'</span>)</span><br><span class="line">plt.imshow(img_v)</span><br><span class="line">print(img_v)</span><br></pre></td></tr></table></figure><img src="/2019/12/03/CS231n-Week1-Assignment2/problem2.png" class="" title="效果如图"><p>像素值全为0~255的整数值,并且坐标相邻的值容易相等</p><h2 id="3-L1范数，L2范数数学表达式，这两种度量分别适用于什么情况"><a href="#3-L1范数，L2范数数学表达式，这两种度量分别适用于什么情况" class="headerlink" title="3. L1范数，L2范数数学表达式，这两种度量分别适用于什么情况"></a>3. L1范数，L2范数数学表达式，这两种度量分别适用于什么情况</h2><p><strong>L1范数</strong>：$ d_1 (I_1, I_2) = \sum_p \lvert I_1^p - I_2^P\rvert$</p><p><strong>L2范数</strong>：$d_2(I_1, I_2 = \sqrt{\sum_p(I_1^p - I_2^p)^2}$</p><p>L1范数取决于你选择的坐标系统，当你转动坐标系统时将会改变它们之间的L1距离，而L2范式不会受此影响<br>因此，如果你输入的特征向量中的值对你的任务有一些重要的意义，则L1更适合一些<br>而对于比较通用的向量而言，L2会更自然一些</p><img src="/2019/12/03/CS231n-Week1-Assignment2/L1vsL2.png" class="" title="L1vsL2"><h2 id="4-描述近邻算法KNN，-NN算法的复杂度，为什么很少在图像中使用，以及它存在的问题"><a href="#4-描述近邻算法KNN，-NN算法的复杂度，为什么很少在图像中使用，以及它存在的问题" class="headerlink" title="4. 描述近邻算法KNN， NN算法的复杂度，为什么很少在图像中使用，以及它存在的问题"></a>4. 描述近邻算法KNN， NN算法的复杂度，为什么很少在图像中使用，以及它存在的问题</h2><p>NN算法复杂度：<br>训练时O(1)， 预测时O(N)</p><p>NN与KNN的区别：<br>KNN会投票与其最近的多个点，从而使得周围的决策边缘变得平滑，即对噪声产生更大的鲁棒性。</p><p>KNN算法在图像分类中很少用到的原因：</p><ol><li><p>它在测试时运算时间较长，与我们的需求不符</p></li><li><p>L1，L2范数的衡量标准建立在向量上的距离函数不太适合表示图像视觉上的相似度</p><p>例子中分别遮挡，平移和改变颜色了图片后，L2范数值不变，所以并不是很适合用于表示图像之间视觉感知的差异</p></li><li><p>维度灾难，KNN算法需要的数据分布较为密集，这就对数据集的要求较高，对于图片而言就需要图片的数量较多，当分布处于较高维度时需要的样本量呈指数倍增长</p></li></ol><h2 id="5-了解cifar-10数据集"><a href="#5-了解cifar-10数据集" class="headerlink" title="5. 了解cifar-10数据集"></a>5. 了解cifar-10数据集</h2><p>CIFAR-10数据集：<br>拥有10种类别(:airplane:, :car:，:bird:，:cat:, :deer:, :dog:, :frog:, :horse:,  :ship:, :truck:)<br>5w训练图片，1w测试图片</p><h2 id="6-超参数怎么选择合适-数据集如何划分"><a href="#6-超参数怎么选择合适-数据集如何划分" class="headerlink" title="6. 超参数怎么选择合适(数据集如何划分)"></a>6. 超参数怎么选择合适(数据集如何划分)</h2><p>分别提到四种想法：</p><ol><li><p>使用在数据集中准确率最好的超参数 </p><p>这样是不行的，因为会造成严重的过拟合，面对未知数据的性能会非常差</p></li><li><p>将数据分割成训练和测试集，并使用在测试集中性能最好的超参数</p><p>这样也是不好的，因为这样的超参数容易使分类器只在测试集中性能良好</p></li><li><p>将数据分成训练集，验证集和测试集，使用多组超参数在训练集中运行，并在验证集上进行验证， 并且使用在验证集中表现最好的，放入测试集中运行，看看效果(要保证验证集和测试集完全分隔)</p><p>这样得到的数据才是应该写入报告的测试结果，才能展现出训练的算法在未知数据中的表现</p></li><li><p>交叉验证法：(小数据集中常见)，将数据集分成多个小份，每次换超参数时使用不同的一份作为验证集</p></li></ol><img src="/2019/12/03/CS231n-Week1-Assignment2/cross-validation.png" class="" title="交叉验证集的设置">]]></content>
      
      
      <categories>
          
          <category> CS231n </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CS231n </tag>
            
            <tag> KNN </tag>
            
            <tag> L2 </tag>
            
            <tag> 超参数 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pyramid-GAN的代码结构</title>
      <link href="/2019/12/02/Pyramid-GAN%E7%9A%84%E4%BB%A3%E7%A0%81%E7%BB%93%E6%9E%84/"/>
      <url>/2019/12/02/Pyramid-GAN%E7%9A%84%E4%BB%A3%E7%A0%81%E7%BB%93%E6%9E%84/</url>
      
        <content type="html"><![CDATA[<h1 id="GAN的代码结构"><a href="#GAN的代码结构" class="headerlink" title="GAN的代码结构"></a>GAN的代码结构</h1><p>在GAN类外，是生成器类和判别器类的代码，而本篇中的GAN较为特殊，使用的是金字塔结构的判别器，有些地方需要额外注意。</p><h2 id="初始化函数"><a href="#初始化函数" class="headerlink" title="初始化函数"></a>初始化函数</h2><h3 id="参数解析器"><a href="#参数解析器" class="headerlink" title="参数解析器"></a>参数解析器</h3><p>首先是使用参数解析器(argparse)，方便直接在控制台使用Python命令来控制训练的部分超参数</p><p>本篇代码中含有的超参数包含：<br><a id="more"></a></p><div class="table-container"><table><thead><tr><th style="text-align:left">参数名</th><th style="text-align:center">类型</th><th style="text-align:left">默认值</th><th style="text-align:left">选择值</th><th style="text-align:left">帮助</th></tr></thead><tbody><tr><td style="text-align:left">description</td><td style="text-align:center">str</td><td style="text-align:left">none</td><td style="text-align:left">none</td><td style="text-align:left">对于本代码的描述</td></tr><tr><td style="text-align:left">dataset</td><td style="text-align:center">str</td><td style="text-align:left">CACD2000</td><td style="text-align:left">[‘CACD2000’,<br>‘UTKFace’,’FG_NET’,<br>‘wiki’,’MORPH’]</td><td style="text-align:left">训练使用的数据集</td></tr><tr><td style="text-align:left">epoch</td><td style="text-align:center">int</td><td style="text-align:left">50</td><td style="text-align:left"></td><td style="text-align:left">训练的遍数</td></tr><tr><td style="text-align:left">batch_size</td><td style="text-align:center">int</td><td style="text-align:left">8</td><td style="text-align:left">随显存提升而提高</td><td style="text-align:left">批量的大小</td></tr><tr><td style="text-align:left">input_size</td><td style="text-align:center">int</td><td style="text-align:left">224</td><td style="text-align:left"></td><td style="text-align:left">输入图片大小</td></tr><tr><td style="text-align:left">save_dir</td><td style="text-align:center">str</td><td style="text-align:left">‘D:/GAN/Pyramid-GAN/Dict/‘</td><td style="text-align:left"></td><td style="text-align:left">模型保存目录</td></tr><tr><td style="text-align:left">result_dir</td><td style="text-align:center">str</td><td style="text-align:left">‘D:/GAN/Pyramid-GAN/results’</td><td style="text-align:left"></td><td style="text-align:left">结果保存目录</td></tr><tr><td style="text-align:left">lrG</td><td style="text-align:center">float</td><td style="text-align:left">0.0001</td><td style="text-align:left">使用Adam时有人推荐3e-4</td><td style="text-align:left">生成器G的学习率</td></tr><tr><td style="text-align:left">lrD</td><td style="text-align:center">float</td><td style="text-align:left">0.0001</td><td style="text-align:left">同上</td><td style="text-align:left">判别器D的学习率</td></tr><tr><td style="text-align:left">beta1</td><td style="text-align:center">float</td><td style="text-align:left">0.5</td><td style="text-align:left"></td><td style="text-align:left">Adam中动量梯度下降超参数</td></tr><tr><td style="text-align:left">beta2</td><td style="text-align:center">float</td><td style="text-align:left">0.999</td><td style="text-align:left"></td><td style="text-align:left">Adam中根均方传播超参数</td></tr><tr><td style="text-align:left">gpu_mode</td><td style="text-align:center">bool</td><td style="text-align:left">True</td><td style="text-align:left"></td><td style="text-align:left">GPU模式</td></tr><tr><td style="text-align:left">benchmark_mode</td><td style="text-align:center">bool</td><td style="text-align:left">True</td><td style="text-align:left"></td><td style="text-align:left">标准模式</td></tr></tbody></table></div><h3 id="数据加载器"><a href="#数据加载器" class="headerlink" title="数据加载器"></a>数据加载器</h3><p>4个数据加载器dataloader0,1,2,3分别对应30岁以下，30~40，40~50和50以上的年龄图片</p><h3 id="网络模型"><a href="#网络模型" class="headerlink" title="网络模型"></a>网络模型</h3><p>生成器G，判别器D，G的Adam优化器和D的Adam优化器</p><h3 id="均方根误差"><a href="#均方根误差" class="headerlink" title="均方根误差"></a>均方根误差</h3><p>GPU模式使用CUDA训练G,D并且计算MSELoss(均方根误差)</p><h3 id="噪声样本z"><a href="#噪声样本z" class="headerlink" title="噪声样本z"></a>噪声样本z</h3><p>使用rand()生成batch_size个100*1*1的随机数，并且可以的话放入CUDA</p><h2 id="训练函数"><a href="#训练函数" class="headerlink" title="训练函数"></a>训练函数</h2><h3 id="训练历史"><a href="#训练历史" class="headerlink" title="训练历史"></a>训练历史</h3><p>使用train_hist元组，其中包含D_loss, G_loss, per_epoch_time和total_time</p><p>使用ones和zeros函数分别生成batch_size个真假标签，存入变量y_fake和y_real中</p><h3 id="训练D-1"><a href="#训练D-1" class="headerlink" title="训练D_1"></a>训练D_1</h3><p>使用enumerate函数遍历dataloader1中的数据，用iter表示迭代次数，数据形式为(x, y)</p><p>当使用GPU模式时，将x放入cuda，清除D优化器重的梯度，并将X分别投入4个path中，这四个path代表的是从输入图片开始，经过VGG的部分然后经过扩展的部分再形成3*3*1表示的结果</p><p>然后将四个path的结果投入D中，形成判别器D真实的训练结果，并以真实标签计算均方根误差，这里计算的是最小二乘公式的前半段，即$(D_w(\phi_{age}(x))-1)^2$，完整公式如下：</p><script type="math/tex; mode=display">\mathcal{L}_{GAN\_D} = {\frac{1}{2}\mathbb{E}_{x\sim P_{data}}(x)[(D_w(\phi_{age}(x)) -1)^2] +  \\ \frac{1}{2}\mathbb{E}_{x \sim P_{young}}(x)[D_w(\phi_{age}(G(x)))^2 + D_w(\phi_{age}(x))^2]}</script><p>这里计算了前半段的损失公式，并且进行反向传播。然后让D的优化器进行单次优化，即更新D的所有参数</p><h3 id="训练D-2"><a href="#训练D-2" class="headerlink" title="训练D_2"></a>训练D_2</h3><p>使用enumerate函数遍历dataloader0中的数据，即年轻样本</p><p>开始前，先将x的前8张进行重复形成fixed_noise，修正噪声，然后声明一个全局变量fixed_img_v，即图像修正值，将fixed_noise转换成变量放入并保存</p><p>然后将D优化器中的梯度清零</p><p>将x重新放入更新过参数的D的Path中，并得出新的判别结果并让其与0标签计算均方根误差，用于计算损失公式的$D_w(\phi_{age}(x))^2$部分</p><p>再将x投入G中，并将结果放入D中，得到的真实结果与0计算均方根误差，得到公式的$D_w(\phi_{age}(G(x)))^2$部分，并将两者相加得到的损失放入train_hist[‘D_loss’]中，然后进行反向传播并更新参数</p><h3 id="训练G"><a href="#训练G" class="headerlink" title="训练G"></a>训练G</h3><p>清除G优化器中的梯度，将x放入G中并将所得结果放入D，得到结果并计算均方根误差，得到：</p><script type="math/tex; mode=display">\mathcal{L}_{GAN\_G} = \mathbb{E}_{x \sim P_{young}(x)}[(D_w(\phi_{age}(G(x))) - 1)^2]</script><p>并且将生成结果G(x)与x计算均方根误差，得到pixel-level-loss，即：</p><script type="math/tex; mode=display">\mathcal{L}_{pixel} = \frac{1}{W\times H \times C}\lVert G(x)-x \rVert ^2_2</script><p>然后计算$L_G = 750 \times L_{GAN_G} + 0.2 \times L_{pixel}$</p><p>将其放入train_hist[‘G_loss’]中，并且进行反向传播并更新参数</p>]]></content>
      
      
      <categories>
          
          <category> deeplearning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deeplearning </tag>
            
            <tag> GAN </tag>
            
            <tag> code </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Adam的回顾</title>
      <link href="/2019/12/01/Adam%E7%9A%84%E5%9B%9E%E9%A1%BE/"/>
      <url>/2019/12/01/Adam%E7%9A%84%E5%9B%9E%E9%A1%BE/</url>
      
        <content type="html"><![CDATA[<h1 id="加权平均数"><a href="#加权平均数" class="headerlink" title="加权平均数"></a>加权平均数</h1><blockquote><p>  温馨提示：由于本篇latex公式较多，容易显示异常或者加载较慢</p></blockquote><p>公式：<br>$V_t = \beta_1V_{t-1} + (1-\beta_1)\theta_t$<br>其中$V_0 = 0$，所以展开后为：<br>$V_t = \beta_1^{t-1}(1-\beta_1)\theta_1 + \beta_1^{t-2}(1-\beta_1)\theta_2 + … + \beta_1^2(1-\beta_1)\theta_{t-2} + \beta_1(1-\beta_1)\theta_{t-1} + (1-\beta_1)\theta_t \\ = (1-\beta_1)\sum_{i=1}^{t} \beta_1^{t-i}\theta_{i}$</p><p>即$\frac{1}{1-\beta_1}V_t = \sum_{i=1}^{t}\beta_1^{t-i}\theta_i$，其中$\beta_1$是小于1的浮点数，所以在大约$\frac{1}{1-\beta_1}$次方后，$\beta_1^{\frac{1}{1-\beta_1}}$会小于1/e（这个数相当于是一个指数衰减函数，在权重小于1/e是算作较小值），可以逐渐算作忽略不计，则可视作  <strong>$V_t$为前$\frac{1}{1-\beta_1}$ 项的加权平均数</strong></p><p>另外，在数据的前期由于其数据比$\frac{1}{1-\beta_{1}}$小得多，所以不能算较多数的加权平均，为了弥补数据量还不足的问题，可以除以偏差修正量$1-\beta_1^t$，而这个量会随着t的增大而逐渐趋向于1，所以不会影响后面的值</p><p>所以最终取的值为：<br>$\frac{V_t}{1-\beta_1^t} = \frac{1-\beta_1}{1-\beta_1^t}\sum_{i=1}^t\beta_1^{t-i}\theta_i$</p><a id="more"></a><h1 id="动量梯度下降法-Momentum"><a href="#动量梯度下降法-Momentum" class="headerlink" title="动量梯度下降法(Momentum)"></a>动量梯度下降法(Momentum)</h1><p>我们一般把梯度下降法比作是在爬山中的下坡，不断地迭代就是为了找出当前更快的下山的路，梯度下降法的迭代计算式为：<br>$\left\{\begin{array}{l}{w = w - \alpha dw} \\ {b = b - \alpha db}\end{array}\right.$</p><p>w和b分别可以表示每次跨的步数多少，和一个跨步的固定最小的值，wx+b可以代表了下山的路线，所以，对于正确的下山路线而言，训练的过程可能是上下摆动较大的，而我们之前说过$\frac{1}{1-\beta_1}$代表了$V_t$表示着多少天的加权平均，所以$\beta_1$越大，代表着越多值的平均值，而取较大的$\beta_1$意味着得到更平坦的图像(路线)，所以使用加权平均数可以使得dw和db的变化变得平稳，我们分别用$V_{dw},V_{db}$表示他们使用加权平均后的值，这样可以使得w和b的变化更平稳，则wx+b也会变得更加平坦</p><p>流程：<br>On iteration t:<br>    计算当前小批量的dw, db<br>    $V_{dw} = \beta_1V_{dw} + (1-\beta_1)dw \\ V_{db} = \beta_1V_{db} + (1-\beta_1)db \\ w = w - \alpha V_{dw}, b = b - \alpha V_{db}$</p><p>其中的超参数：α(学习率)，$\beta_1$(控制指数加权平均，最常用0.9)</p><p>Note：由于数据量较大，一般$\beta_1$取10，而前10个数据对于整体数据而言几乎可以忽略不计，所以不必添加偏差修正量</p><h1 id="根均方传播法-RMS-prop"><a href="#根均方传播法-RMS-prop" class="headerlink" title="根均方传播法(RMS prop)"></a>根均方传播法(RMS prop)</h1><p>函数在向数据的正确分布拟合的时候，x的参数w越大，往往能更快的接近正确的分布(当然也不能过大)，而b较大的时候容易过大而走弯路，所以一般不能太大，所以一般来说我们鼓励在学习过程中w变得更大一点而b变得更小一点。由此我们可以使用RMSprop方法来平衡w较小而b较大的情况</p><p>流程：<br>On iteration t:<br>    计算当前小批量的dw, db<br>RMSprop: $S_{dw} = \beta_2 S_{dw} + (1 - \beta_2)(dw)^2 \\ S_{db} = \beta_2 S_{db} + (1- \beta_2)(dw)^2$<br>更新参数：$w = w - \alpha \frac{dw}{\sqrt{S_{dw}} + \epsilon} \\ b = b - \alpha \frac{db}{\sqrt{S_{db}} + \epsilon}$</p><p>其中公式中前后$S_{dw}, S_{db}$分别代表前t次，前t-1次迭代累计的dw的平方，相对本次db而言，它们是小的，所以更新后的也 $S_{db}$相对db而言是较小数，那么$\frac{db}{\sqrt{S_{db}} + \epsilon}$是一个较大的数，b减去一个较大的数后就可以更新为一个较小数，同理dw也可以更新为一个较大数</p><p>Note:ε的存在一般是为了防止$S_{dw}, S_{db}$过小导致算出来的值爆炸，所以加上一个几乎不影响结果的较小值，常用$10^{-8}、 10^{-6}$</p><h1 id="Adam优化算法-Adaptive-moment-Estimation"><a href="#Adam优化算法-Adaptive-moment-Estimation" class="headerlink" title="Adam优化算法(Adaptive moment Estimation)"></a>Adam优化算法(Adaptive moment Estimation)</h1><p>Adam优化算法就是将RMS传播算法和动量梯度下降法结合起来的算法</p><p>流程为：<br>$V_{dw} = 0,S_{dw} = 0, V_{db} = 0, S_{db} = 0$<br>On iteration t:<br>    计算当前小批量的dw, db<br>    $\left.\begin{array}{l}{V_{dw} = \beta_1 V_{dw} + (1-\beta_1)dw} \\<br>     {V_{db} = \beta_1 V_{db} + (1-\beta_1)db }\end{array}\right\}$”Momentum”动量梯度下降<br>    $\left.\begin{array}{l}{S_{dw}= \beta_2 S_{dw} + (1-\beta_2)(dw)^2} \\<br>     {S_{db}=\beta_2 S_{dw} + (1 -\beta_2)(db)^2}\end{array}\right\}$”RMSprop”根均方传播算法<br>偏差修正：<br>    $V^{corrected}_{dw} = \frac{V_{dw}}{1-\beta_1^t} \\<br>     V^{corrected}_{db} = \frac{V_{db}}{1-\beta_1^t} \\<br>     S^{corrected}_{dw} = \frac{S_{dw}}{1-\beta_2^t} \\<br>     S^{corrected}_{db} = \frac{S_{db}}{1-\beta_2^t}$<br>更新参数：$w = w - \alpha \frac{V^{corrected}_{dw}}{\sqrt{S^{corrected}_{dw}} + \epsilon} \\<br>         b = b-\alpha \frac{V^{corrected}_{db}}{\sqrt{S^{corrected}_{db}} +\epsilon}$</p><p>超参数：<br>α：学习率，需要调试<br>$\beta_1$：常用0.9            （dw的加权平均数，第一<em>矩（待了解）</em>）<br>$\beta_2$：推荐0.999        （$dw^2$的加权平均数，第二矩）<br>ε：$10^{-8}$                    （不需要太在意）</p><p>总而言之，Adam优化算法就是将RMS传播算法和动量梯度下降法结合在一起，前者可以减少偏差以加快学习，后者可以使图像更加平坦，从而加快学习。</p>]]></content>
      
      
      <categories>
          
          <category> deeplearning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deeplearning </tag>
            
            <tag> 优化器 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CS231n_Week1_Assignment1</title>
      <link href="/2019/11/30/CS231n-Week1-Assignment1/"/>
      <url>/2019/11/30/CS231n-Week1-Assignment1/</url>
      
        <content type="html"><![CDATA[<h1 id="作业："><a href="#作业：" class="headerlink" title="作业："></a>作业：</h1><p>(阅读理解题)</p><h2 id="1-图像的数据主要来源有哪些"><a href="#1-图像的数据主要来源有哪些" class="headerlink" title="1. 图像的数据主要来源有哪些"></a>1. 图像的数据主要来源有哪些</h2><p>照相机，手机，摄像头等一系列视觉传感器设备生产出了大量的图像数据</p><h2 id="2-sift-feature，金字塔匹配思想和hog特征分别是什么，可以用来干什么"><a href="#2-sift-feature，金字塔匹配思想和hog特征分别是什么，可以用来干什么" class="headerlink" title="2. sift feature，金字塔匹配思想和hog特征分别是什么，可以用来干什么"></a>2. sift feature，金字塔匹配思想和hog特征分别是什么，可以用来干什么</h2><ol><li><strong>sift feature：</strong>确认目标上在变化中具有表现型和不变性的特征，然后把这些特征与相似的目标进行匹配<a id="more"></a></li><li><strong>金字塔匹配思想：</strong>从图片的各部分的各个像素抽取特征并把它们放到一起作为一个特征描述符，然后再特征描述符上做一个支持向量机</li><li><strong>hog特征：</strong>histogram of gradients方向梯度直方图，将一堆特征放在一起后，研究如何在实际图片中比较合理的设计人体姿态和辨认人体姿态</li></ol><h2 id="3-神经网络早就存在为什么最近才兴起"><a href="#3-神经网络早就存在为什么最近才兴起" class="headerlink" title="3. 神经网络早就存在为什么最近才兴起"></a>3. 神经网络早就存在为什么最近才兴起</h2><ol><li>算力的提升</li><li>高质量的数据集的数量增多</li></ol><h2 id="4-图像任务有哪些，解决什么样的图像问题"><a href="#4-图像任务有哪些，解决什么样的图像问题" class="headerlink" title="4. 图像任务有哪些，解决什么样的图像问题"></a>4. 图像任务有哪些，解决什么样的图像问题</h2><p>图像分类，目标检测，图像描述</p>]]></content>
      
      
      <categories>
          
          <category> CS231n </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CS231n </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>用numpy实现一个双层的softmax神经网络</title>
      <link href="/2019/11/28/%E7%94%A8numpy%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E5%8F%8C%E5%B1%82%E7%9A%84softmax%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
      <url>/2019/11/28/%E7%94%A8numpy%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E5%8F%8C%E5%B1%82%E7%9A%84softmax%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<blockquote><p>  使用的数值化数据集：<br>  链接：<a href="https://pan.baidu.com/s/110dhDKA8eXYV4-kfmevo0g" target="_blank" rel="noopener">https://pan.baidu.com/s/110dhDKA8eXYV4-kfmevo0g</a><br>  提取码：x3wu</p></blockquote><h1 id="整体思路"><a href="#整体思路" class="headerlink" title="整体思路"></a>整体思路</h1><p>使用Numpy和pandas写一个神经网络，进行手写数字识别(MNIST)，样本图如下：</p><img src="/2019/11/28/%E7%94%A8numpy%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E5%8F%8C%E5%B1%82%E7%9A%84softmax%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/output_13_0.png" class="" title="手写数字5"><h1 id="总思路"><a href="#总思路" class="headerlink" title="总思路"></a>总思路</h1><p>整体神经网络代码只使用Numpy和pandas，根据额外需要使用time和matplotlib，设计思路如下</p><p>数据处理-&gt; 建立模型 -&gt; 进行训练 -&gt; 进行测试 -&gt; 评估时间和精度<br><a id="more"></a></p><h1 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h1><p>数据分为训练集和测试集，分别是785*6w和785*1w的数据，785列中前784列代表了28*28的图片的数值化数据，第785列为0~9的标签，并且第一行为0~784的索引</p><ol><li>使用pandas读取训练和测试数据</li><li>分别把训练和测试数据中的数据和标签分离</li><li>将标签进行one-hot编码</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练集有6w, 785列，前784为28*28,最后一列为标签</span></span><br><span class="line">train = pd.read_csv(<span class="string">"MNIST_Training_60K.csv"</span>,index_col=<span class="literal">False</span>,header=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 测试集有1w，784列，28*28的图片</span></span><br><span class="line">test = pd.read_csv(<span class="string">"MNIST_Test_10K.csv"</span>,index_col=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">label = train[:][<span class="string">"784"</span>]</span><br><span class="line">train = train.drop([<span class="string">"784"</span>],axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">X = train.T</span><br><span class="line">Y = np.zeros((<span class="number">10</span>, X.shape[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以下为成像代码</span></span><br><span class="line"><span class="comment"># import matplotlib.pyplot as plt</span></span><br><span class="line"><span class="comment"># x = np.array(X)</span></span><br><span class="line"><span class="comment"># X_train = x.reshape((60000,28,28))</span></span><br><span class="line"><span class="comment"># cur=X_train[0:1]</span></span><br><span class="line"><span class="comment"># plt.imshow(cur[0].reshape(h,w).T,cmap='gray')</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将标签one_hot编码</span></span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> range(X.shape[<span class="number">1</span>]):</span><br><span class="line">    Y[label[n]][n] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 处理test数据</span></span><br><span class="line">label_test = test[:][<span class="string">"784"</span>]</span><br><span class="line">test1 = test.drop([<span class="string">"784"</span>],axis=<span class="number">1</span>)</span><br><span class="line">X_test = test1.T</span><br><span class="line"><span class="comment"># Y_test = np.eye(10)[l]</span></span><br></pre></td></tr></table></figure><h1 id="建立模型"><a href="#建立模型" class="headerlink" title="建立模型"></a>建立模型</h1><h2 id="正向传播"><a href="#正向传播" class="headerlink" title="正向传播"></a>正向传播</h2><ol><li><p>参数初始化</p><p>采用”He”初始化，即使用标准化的服从高斯分布的随机数乘上 2/前一层网络单元数的根号幂初始化权重W，并且使用0初始化对偏差b进行初始化</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">(n_x, n_h, n_y)</span>:</span></span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    W1 = np.random.randn(n_h, n_x) * np.sqrt(<span class="number">2.</span> / n_x)</span><br><span class="line">    b1 = np.zeros((n_h, <span class="number">1</span>))</span><br><span class="line">    W2 = np.random.randn(n_y, n_h)  * np.sqrt(<span class="number">2.</span> / n_h)</span><br><span class="line">    b2 = np.zeros((n_y, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 断言保证这些参数的形状如下，否则报错</span></span><br><span class="line">    <span class="keyword">assert</span>(W1.shape == (n_h, n_x))</span><br><span class="line">    <span class="keyword">assert</span>(b1.shape == (n_h,   <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">assert</span>(W2.shape == (n_y, n_h))</span><br><span class="line">    <span class="keyword">assert</span>(b2.shape == (n_y,   <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将参数封装</span></span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>:W1,</span><br><span class="line">                  <span class="string">"b1"</span>:b1,</span><br><span class="line">                  <span class="string">"W2"</span>:W2,</span><br><span class="line">                  <span class="string">"b2"</span>:b2&#125;</span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>注意：</p><p>权重w本来的维度应该是(前一层神经元数，本层神经元数)，但是为了方便后续计算省略掉此处的转置操作，在一开始就选择将W的维度设置为(本层神经元数，前一层大小神经元数)</p><p>而偏差b由于需要整层网络共享，我们基于Python的广播机制，选择让它的大小设置为(本层神经元数，1)</p></li></ol><ol><li><p>正向线性传播</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_forward</span><span class="params">(A0, W1, b1)</span>:</span></span><br><span class="line">    Z1 = np.dot(W1, A0) + b1</span><br><span class="line">    <span class="comment"># 即Z1 = W1 * A0 + b1</span></span><br><span class="line">    <span class="keyword">assert</span>(Z1.shape == (W1.shape[<span class="number">0</span>], A0.shape[<span class="number">1</span>]))</span><br><span class="line">    </span><br><span class="line">    cache = (A0, W1, b1)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> Z1, cache</span><br></pre></td></tr></table></figure><p>无特别说明</p></li></ol><ol><li><p>正向激活</p><p>隐藏层使用ReLU(整流线性单元)函数，输出层使用softmax函数，期望它的值符合0~9十个分类。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu</span><span class="params">(Z1)</span>:</span></span><br><span class="line">    A1 = np.maximum(<span class="number">0</span>, Z1) </span><br><span class="line">    cache = Z1</span><br><span class="line">    <span class="keyword">return</span> A1, cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(Z2)</span>:</span></span><br><span class="line">    A2 = np.exp(Z2) / sum(np.exp(Z2))</span><br><span class="line">    </span><br><span class="line">    cache = Z2</span><br><span class="line">    <span class="keyword">return</span> A2, cache</span><br></pre></td></tr></table></figure></li></ol><ol><li><p>正向传播函数</p><p>将线性传播部分和激活函数合并</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_forward_relu</span><span class="params">(A0, W1, b1)</span>:</span></span><br><span class="line">    Z1, linear_cache = linear_forward(A0, W1, b1)</span><br><span class="line">    A1, activation_cache = relu(Z1)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(A1.shape == (W.shape[<span class="number">0</span>], A0.shape[<span class="number">1</span>]))</span><br><span class="line">    cache = (linear_cache, activation_cache)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> A1, cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_forward_softmax</span><span class="params">(A1, W2, b2)</span>:</span></span><br><span class="line">    Z2, linear_cache = linear_forward(A1, W2, b2)</span><br><span class="line">    A2, activation_cache = softmax(Z2)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(A2.shape == (W2.shape[<span class="number">0</span>], A1.shape[<span class="number">1</span>]))</span><br><span class="line">    cache = (linear_cache, activation_cache)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> A2, cache</span><br></pre></td></tr></table></figure></li></ol><h2 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h2><ol><li><p>计算成本</p><p>使用交叉熵作为成本函数，其函数的公式为：</p><script type="math/tex; mode=display">\sum_{i}^{m} y_i\log{\hat{y_{i}}}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(A2, Y)</span>:</span></span><br><span class="line">    <span class="comment"># 计算交叉熵</span></span><br><span class="line">    m = Y.shape[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算loss函数</span></span><br><span class="line">    logprobs = <span class="number">-1</span>* sum(np.multiply(np.log(A2), Y))</span><br><span class="line">    <span class="comment"># 成本函数就是将整个训练集上的损失相加取相反数</span></span><br><span class="line">    cost = <span class="number">-1</span>/m * np.sum(logprobs)</span><br><span class="line">    cost = np.squeeze(cost)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cost, logprobs</span><br></pre></td></tr></table></figure></li></ol><ol><li><p>反向传播</p><p>计算各个参数基于成本的梯度，用于更新参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_propagation</span><span class="params">(parameters, cache, X, Y)</span>:</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    W1 = parameters[<span class="string">'W1'</span>]</span><br><span class="line">    W2 = parameters[<span class="string">'W2'</span>]</span><br><span class="line">    </span><br><span class="line">    A1 = cache[<span class="string">'A1'</span>]</span><br><span class="line">    A2 = cache[<span class="string">'A2'</span>]</span><br><span class="line">    </span><br><span class="line">    dZ2 = A2 - Y</span><br><span class="line">    dW2 = <span class="number">1</span>/m * np.dot(dZ2, A1.T)</span><br><span class="line">    db2 = <span class="number">1</span>/m * np.sum(dZ2, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    dA1 = np.dot(W2.T, dZ2)</span><br><span class="line">    dZ1 = np.array(dA1, copy=<span class="literal">True</span>)</span><br><span class="line">    dZ1[Z1 &lt;=<span class="number">0</span> ] = <span class="number">0</span> <span class="comment"># 标记一下，操作十分神奇,牛逼！</span></span><br><span class="line">    dW1 = <span class="number">1</span>/m * np.dot(dZ1, X.T)</span><br><span class="line">    db1 = <span class="number">1</span>/m * np.sum(dZ1, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    grads=&#123;<span class="string">"dW1"</span>:dW1,</span><br><span class="line">           <span class="string">"db1"</span>:db1,</span><br><span class="line">           <span class="string">"dW2"</span>:dW2,</span><br><span class="line">           <span class="string">"db2"</span>:db2&#125;</span><br><span class="line">    <span class="keyword">return</span> grads</span><br></pre></td></tr></table></figure><p>各参数传播的顺序和算式如下：</p><img src="/2019/11/28/%E7%94%A8numpy%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E5%8F%8C%E5%B1%82%E7%9A%84softmax%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/propagation.jpg" class="" title="参数传播"></li></ol><pre><code>softmax对输入求导的公式见[CS231n-Week3-Assignment4](https://1305936314.github.io/2019/12/09/CS231n-Week3-Assignment4/)</code></pre><ol><li><p>更新参数</p><p>使用mini-batch梯度下降法更新参数，目前仅实现了梯度下降法，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span><span class="params">(parameters, grads, learning_rate=<span class="number">1.2</span>)</span>:</span></span><br><span class="line">    W1 = parameters[<span class="string">'W1'</span>]</span><br><span class="line">    b1 = parameters[<span class="string">'b1'</span>]</span><br><span class="line">    W2 = parameters[<span class="string">'W2'</span>]</span><br><span class="line">    b2 = parameters[<span class="string">'b2'</span>]</span><br><span class="line">    </span><br><span class="line">    dW1 = grads[<span class="string">'dW1'</span>]</span><br><span class="line">    db1 = grads[<span class="string">'db1'</span>]</span><br><span class="line">    dW2 = grads[<span class="string">'dW2'</span>]</span><br><span class="line">    db2 = grads[<span class="string">'db2'</span>]</span><br><span class="line">    </span><br><span class="line">    W1 = W1 - learning_rate* dW1</span><br><span class="line">    b1 = b1 - learning_rate* db1</span><br><span class="line">    W2 = W2 - learning_rate* dW2</span><br><span class="line">    b2 = b2 - learning_rate* db2</span><br><span class="line">    </span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</span><br><span class="line">                  <span class="string">"b1"</span>: b1,</span><br><span class="line">                  <span class="string">"W2"</span>: W2,</span><br><span class="line">                  <span class="string">"b2"</span>: b2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure></li></ol><h2 id="整体模型"><a href="#整体模型" class="headerlink" title="整体模型"></a>整体模型</h2><p>将上述函数等无缝合并，方便转换</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nn_model</span><span class="params">(X, Y, n_h=<span class="number">1000</span>, learning_rate=<span class="number">0.12</span> ,num_iterations=<span class="number">10000</span>, print_cost=False)</span>:</span></span><br><span class="line">    n_x = <span class="number">784</span></span><br><span class="line">    n_h = <span class="number">1000</span></span><br><span class="line">    n_y = <span class="number">10</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 随机初始化参数</span></span><br><span class="line">    np.random.seed(<span class="number">3</span>)</span><br><span class="line">    W1 = np.random.randn(n_h, n_x) * np.sqrt(<span class="number">2.</span> / n_x)</span><br><span class="line">    b1 = np.zeros((n_h, <span class="number">1</span>))</span><br><span class="line">    W2 = np.random.randn(n_y, n_h)  * np.sqrt(<span class="number">2.</span> / n_h)</span><br><span class="line">    b2 = np.zeros((n_y, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line">        <span class="comment"># 正向传播: 线性-&gt; ReLU -&gt; 线性-&gt; softmax</span></span><br><span class="line">        Z1 = np.dot(W1, X) + b1</span><br><span class="line">        A1 = np.maximum(<span class="number">0</span>, Z1)</span><br><span class="line">        Z2 = np.dot(W2, A1) + b2</span><br><span class="line">        A2 = np.exp(Z2) / sum(np.exp(Z2))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算成本：计算交叉熵成本</span></span><br><span class="line">        logprobs = <span class="number">-1</span>* sum(np.multiply(np.log(A2), Y))</span><br><span class="line">        cost = <span class="number">-1</span>/m * np.sum(logprobs)</span><br><span class="line">        cost = np.squeeze(cost)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 反向传播：梯度下降法</span></span><br><span class="line">        dZ2 = A2 - Y</span><br><span class="line">        dW2 = <span class="number">1</span>/m * np.dot(dZ2, A1.T)</span><br><span class="line">        db2 = <span class="number">1</span>/m * np.sum(dZ2, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">        dA1 = np.dot(W2.T, dZ2)</span><br><span class="line">        dZ1 = np.array(dA1, copy=<span class="literal">True</span>)</span><br><span class="line">        dZ1[Z1 &lt;=<span class="number">0</span> ] = <span class="number">0</span> <span class="comment"># 标记一下，操作十分神奇,牛逼！</span></span><br><span class="line">        dW1 = <span class="number">1</span>/m * np.dot(dZ1, X.T)</span><br><span class="line">        db1 = <span class="number">1</span>/m * np.sum(dZ1, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 更新参数</span></span><br><span class="line">        W1 -= learning_rate * dW1</span><br><span class="line">        b1 -= learning_rate * db1</span><br><span class="line">        W2 -= learning_rate * dW2</span><br><span class="line">        b2 -= learning_rate * db2</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Cost after iteration %i: %f"</span> %(i, cost))</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> W1, b1, W2, b2</span><br></pre></td></tr></table></figure><p>此处只为保留一下未mini-batch版本</p><h1 id="训练测试"><a href="#训练测试" class="headerlink" title="训练测试"></a>训练测试</h1><p>预测函数如下，即正向传播过程</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(X, W1, b1, W2, b2)</span>:</span></span><br><span class="line">    Z1 = np.dot(W1, X) + b1</span><br><span class="line">    A1 = np.maximum(<span class="number">0</span>, Z1)</span><br><span class="line">    Z2 = np.dot(W2, A1) + b2</span><br><span class="line">    A2 = np.exp(Z2) / sum(np.exp(Z2))</span><br><span class="line">    predictions = A2.argmax(axis=<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 使用argmax使得返回值predictions为和label相同的0~9标签</span></span><br><span class="line">    <span class="keyword">return</span> predictions</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line">tic = time.process_time()</span><br><span class="line">W1, b1, W2, b2= nn_model(X, Y,n_h=<span class="number">300</span>, learning_rate=<span class="number">0.08</span>, num_iterations=<span class="number">10000</span>, print_cost=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line">predictions = predict(X_test, W1, b1, W2, b2)</span><br><span class="line">toc = time.process_time()</span><br><span class="line"><span class="comment"># 时间</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Computation time = "</span> + str((toc - tic)) + <span class="string">"s"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 评估精确度</span></span><br><span class="line">correct_predictions = np.equal(predictions, label_test)</span><br><span class="line">accuracy = np.mean(correct_predictions.astype(np.float32))</span><br><span class="line">print(<span class="string">'Test Accuracy:%f'</span>%(accuracy*<span class="number">100</span>) + <span class="string">'%'</span>)</span><br></pre></td></tr></table></figure><h1 id="所有代码合并"><a href="#所有代码合并" class="headerlink" title="所有代码合并"></a>所有代码合并</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="comment"># import matplotlib.pyplot as plt</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练集有6w, 785列，前784为28*28,最后一列为标签</span></span><br><span class="line">train = pd.read_csv(<span class="string">"MNIST_Training_60K.csv"</span>,index_col=<span class="literal">False</span>,header=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 测试集有1w，784列，28*28的图片</span></span><br><span class="line">test = pd.read_csv(<span class="string">"MNIST_Test_10K.csv"</span>,index_col=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">label = train[:][<span class="string">"784"</span>]</span><br><span class="line">train = train.drop([<span class="string">"784"</span>],axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">X = train.T</span><br><span class="line">Y = np.zeros((<span class="number">10</span>, X.shape[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> range(X.shape[<span class="number">1</span>]):</span><br><span class="line">    Y[label[n]][n] = <span class="number">1</span></span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nn_model</span><span class="params">(X, Y, n_h=<span class="number">1000</span>, learning_rate=<span class="number">0.12</span> ,num_iterations=<span class="number">10000</span>, print_cost=False)</span>:</span></span><br><span class="line">    n_x = <span class="number">784</span></span><br><span class="line">    n_y = <span class="number">10</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 随机初始化参数</span></span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    W1 = np.random.randn(n_h, n_x) * np.sqrt(<span class="number">2.</span> / n_x)</span><br><span class="line">    b1 = np.zeros((n_h, <span class="number">1</span>))</span><br><span class="line">    W2 = np.random.randn(n_y, n_h)  * np.sqrt(<span class="number">2.</span> / n_h)</span><br><span class="line">    b2 = np.zeros((n_y, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line">        <span class="comment"># 正向传播: 线性-&gt; ReLU -&gt; 线性-&gt; softmax</span></span><br><span class="line">        Z1 = np.dot(W1, X) + b1</span><br><span class="line">        A1 = np.maximum(<span class="number">0</span>, Z1)</span><br><span class="line">        Z2 = np.dot(W2, A1) + b2</span><br><span class="line">        A2 = np.exp(Z2) / sum(np.exp(Z2))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算成本：计算交叉熵成本</span></span><br><span class="line">        logprobs = <span class="number">-1</span>* sum(np.multiply(np.log(A2), Y))</span><br><span class="line">        cost = <span class="number">-1</span>/m * np.sum(logprobs)</span><br><span class="line">        cost = np.squeeze(cost)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 反向传播：梯度下降法</span></span><br><span class="line">        dZ2 = A2 - Y</span><br><span class="line">        dW2 = <span class="number">1</span>/m * np.dot(dZ2, A1.T)</span><br><span class="line">        db2 = <span class="number">1</span>/m * np.sum(dZ2, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">        dA1 = np.dot(W2.T, dZ2)</span><br><span class="line">        dZ1 = np.array(dA1, copy=<span class="literal">True</span>)</span><br><span class="line">        dZ1[Z1 &lt;=<span class="number">0</span> ] = <span class="number">0</span> <span class="comment"># 标记一下，操作十分神奇,牛逼！</span></span><br><span class="line">        dW1 = <span class="number">1</span>/m * np.dot(dZ1, X.T)</span><br><span class="line">        db1 = <span class="number">1</span>/m * np.sum(dZ1, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 更新参数</span></span><br><span class="line">        W1 -= learning_rate * dW1</span><br><span class="line">        b1 -= learning_rate * db1</span><br><span class="line">        W2 -= learning_rate * dW2</span><br><span class="line">        b2 -= learning_rate * db2</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Cost after iteration %i: %f"</span> %(i, cost))</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> W1, b1, W2, b2</span><br><span class="line"></span><br><span class="line">label_test = test[:][<span class="string">"784"</span>]</span><br><span class="line">test1 = test.drop([<span class="string">"784"</span>],axis=<span class="number">1</span>)</span><br><span class="line">X_test = test1.T</span><br><span class="line"><span class="comment"># Y_test = np.eye(10)[l]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line">tic = time.process_time()</span><br><span class="line">W1, b1, W2, b2= nn_model(X, Y,n_h=<span class="number">300</span>, learning_rate=<span class="number">0.08</span>, num_iterations=<span class="number">10000</span>, print_cost=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line">Z1 = np.dot(W1, X_test) + b1</span><br><span class="line">A1 = np.maximum(<span class="number">0</span>, Z1)</span><br><span class="line">Z2 = np.dot(W2, A1) + b2</span><br><span class="line">A2 = np.exp(Z2) / sum(np.exp(Z2))</span><br><span class="line">predictions = A2.argmax(axis=<span class="number">0</span>)</span><br><span class="line">toc = time.process_time()</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Computation time = "</span> + str((toc - tic)) + <span class="string">"s"</span>)</span><br><span class="line"></span><br><span class="line">correct_predictions = np.equal(predictions, label_test)</span><br><span class="line">accuracy = np.mean(correct_predictions.astype(np.float32))</span><br><span class="line">print(<span class="string">'Test Accuracy:%f'</span>%(accuracy*<span class="number">100</span>) + <span class="string">'%'</span>)</span><br></pre></td></tr></table></figure><h1 id="添加mini-batch"><a href="#添加mini-batch" class="headerlink" title="添加mini-batch"></a>添加mini-batch</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练集有6w, 785列，前784为28*28,最后一列为标签</span></span><br><span class="line">train = pd.read_csv(<span class="string">"./MNIST_Training_60K.csv"</span>, index_col=<span class="literal">False</span>, header=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 测试集有1w，784列，28*28的图片</span></span><br><span class="line">test = pd.read_csv(<span class="string">"./MNIST_Test_10K.csv"</span>, index_col=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">label = train[:][<span class="string">"784"</span>]</span><br><span class="line">train = train.drop([<span class="string">"784"</span>], axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">X = train.T</span><br><span class="line">Y = np.zeros((<span class="number">10</span>, X.shape[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> range(X.shape[<span class="number">1</span>]):</span><br><span class="line">    Y[label[n]][n] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nn_model</span><span class="params">(X, Y, n_h=<span class="number">1000</span>, mini_batch_size=<span class="number">100</span>, learning_rate=<span class="number">0.08</span>, epoch=<span class="number">1000</span>, print_cost=False)</span>:</span></span><br><span class="line">    n_x = <span class="number">784</span></span><br><span class="line">    n_y = <span class="number">10</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 随机初始化参数</span></span><br><span class="line">    np.random.seed(<span class="number">3</span>)</span><br><span class="line">    W1 = np.random.randn(n_h, n_x) * np.sqrt(<span class="number">2.</span> / n_x)</span><br><span class="line">    b1 = np.zeros((n_h, <span class="number">1</span>))</span><br><span class="line">    W2 = np.random.randn(n_y, n_h) * np.sqrt(<span class="number">2.</span> / n_h)</span><br><span class="line">    b2 = np.zeros((n_y, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    costs=[]</span><br><span class="line">    complete_numof_mini_batch = m // mini_batch_size</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, epoch):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, complete_numof_mini_batch):</span><br><span class="line">            <span class="comment"># 正向传播: 线性-&gt; ReLU -&gt; 线性-&gt; softmax</span></span><br><span class="line">            mini_batch_X = X.values[:, (j * mini_batch_size): (j + <span class="number">1</span>) * mini_batch_size]</span><br><span class="line">            mini_batch_Y = Y[:, (j * mini_batch_size): (j + <span class="number">1</span>) * mini_batch_size]</span><br><span class="line">            Z1 = np.dot(W1, mini_batch_X) + b1</span><br><span class="line">            A1 = np.maximum(<span class="number">0</span>, Z1)</span><br><span class="line">            Z2 = np.dot(W2, A1) + b2</span><br><span class="line">            A2 = np.exp(Z2) / sum(np.exp(Z2))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 计算成本：计算交叉熵成本</span></span><br><span class="line">            logprobs = <span class="number">-1</span> * sum(np.multiply(np.log(A2), mini_batch_Y))</span><br><span class="line">            cost = <span class="number">-1</span> / mini_batch_size * np.sum(logprobs)</span><br><span class="line">            cost = np.squeeze(cost)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 反向传播：梯度下降法</span></span><br><span class="line">            dZ2 = A2 - mini_batch_Y</span><br><span class="line">            dW2 = <span class="number">1</span> / mini_batch_size * np.dot(dZ2, A1.T)</span><br><span class="line">            db2 = <span class="number">1</span> / mini_batch_size * np.sum(dZ2, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">            dA1 = np.dot(W2.T, dZ2)</span><br><span class="line">            dZ1 = np.array(dA1, copy=<span class="literal">True</span>)</span><br><span class="line">            dZ1[Z1 &lt;= <span class="number">0</span>] = <span class="number">0</span>  <span class="comment"># 标记一下，操作十分神奇,牛逼！</span></span><br><span class="line">            dW1 = <span class="number">1</span> / mini_batch_size * np.dot(dZ1, mini_batch_X.T)</span><br><span class="line">            db1 = <span class="number">1</span> / mini_batch_size * np.sum(dZ1, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 更新参数</span></span><br><span class="line">            W1 -= learning_rate * dW1</span><br><span class="line">            b1 -= learning_rate * db1</span><br><span class="line">            W2 -= learning_rate * dW2</span><br><span class="line">            b2 -= learning_rate * db2</span><br><span class="line">            <span class="keyword">if</span> print_cost <span class="keyword">and</span> (j+<span class="number">1</span>) % <span class="number">500</span>:</span><br><span class="line">                costs.append(cost)</span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> (i+<span class="number">1</span>) % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Cost after epoch [%3d/%3d]: %f"</span> % (i+<span class="number">1</span>, epoch, cost))</span><br><span class="line"></span><br><span class="line">    plt.plot(costs)</span><br><span class="line">    plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'iterations (per 500)'</span>)</span><br><span class="line">    plt.title(<span class="string">"Learning rate = "</span> + str(learning_rate))</span><br><span class="line">    plt.show()</span><br><span class="line">    <span class="keyword">return</span> W1, b1, W2, b2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">label_test = test[:][<span class="string">"784"</span>]</span><br><span class="line">test1 = test.drop([<span class="string">"784"</span>], axis=<span class="number">1</span>)</span><br><span class="line">X_test = test1.T</span><br><span class="line"><span class="comment"># Y_test = np.eye(10)[l]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line">mini_batch_size = <span class="number">100</span></span><br><span class="line">tic = time.process_time()</span><br><span class="line">W1, b1, W2, b2 = nn_model(X, Y, n_h=<span class="number">1000</span>, mini_batch_size=mini_batch_size, learning_rate=<span class="number">0.1</span>, epoch=<span class="number">100</span>, print_cost=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line">complete_numof_mini_batch_test = X_test.shape[<span class="number">1</span>] // mini_batch_size</span><br><span class="line">accuracy = []</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">0</span>, complete_numof_mini_batch_test):</span><br><span class="line">    mini_batch_X_test = X_test.values[:, (k * mini_batch_size):((k+<span class="number">1</span>) * mini_batch_size)]</span><br><span class="line">    mini_batch_label = label_test.values[(k * mini_batch_size): (k+<span class="number">1</span>) * mini_batch_size]</span><br><span class="line">    Z1 = np.dot(W1, mini_batch_X_test) + b1</span><br><span class="line">    A1 = np.maximum(<span class="number">0</span>, Z1)</span><br><span class="line">    Z2 = np.dot(W2, A1) + b2</span><br><span class="line">    A2 = np.exp(Z2) / sum(np.exp(Z2))</span><br><span class="line">    predictions = A2.argmax(axis=<span class="number">0</span>)</span><br><span class="line">    correct_predictions = np.equal(predictions, mini_batch_label)</span><br><span class="line">    accuracy.append(np.mean(correct_predictions.astype(np.float32)))</span><br><span class="line"></span><br><span class="line">accuracy = np.mean(accuracy)</span><br><span class="line">toc = time.process_time()</span><br><span class="line">print(<span class="string">"Computation time = "</span> + str((toc - tic)) + <span class="string">"s"</span>)</span><br><span class="line">print(<span class="string">'Test Accuracy:%f'</span> % (accuracy * <span class="number">100</span>) + <span class="string">'%'</span>)</span><br></pre></td></tr></table></figure><img src="/2019/11/28/%E7%94%A8numpy%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E5%8F%8C%E5%B1%82%E7%9A%84softmax%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/costs.png" class="" title="cost图"><p>代码与cost如上图，目前改进就是如此了，将X分成100样本一小份，然后逐份进行训练，由于参数共享，所以只有反向传播时涉及到了Y，别处都不需要修改太多。后续具体参数还需要继续调整，敬请期待！</p><p>最后调整好的准确率和时间如下：</p><img src="/2019/11/28/%E7%94%A8numpy%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E5%8F%8C%E5%B1%82%E7%9A%84softmax%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/accuracy.png" class="" title="准确率和时间"><h1 id="说说我对神经网络的理解"><a href="#说说我对神经网络的理解" class="headerlink" title="说说我对神经网络的理解"></a>说说我对神经网络的理解</h1><p>再看一眼这个参数传播的图：</p><img src="/2019/11/28/%E7%94%A8numpy%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E5%8F%8C%E5%B1%82%E7%9A%84softmax%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/propagation.jpg" class="" title="参数传播图"><h2 id="宏观理解"><a href="#宏观理解" class="headerlink" title="宏观理解"></a>宏观理解</h2><p>根据上面的参数传播图来宏观的理解一下就是：</p><p>我们所需要做的分类在数值化后，可以通过拟合几个函数(在这个全连接层里起码是这样的)，通过它们得到最终的值并进行取舍和改进，慢慢的能够得出一个网络，包含了多个拟合好的函数进行计算和选择，以此来做到分类</p><p>从X开始，每层选择一个节点，一直到输出层，这样就是一个函数，全连接层就是每层节点与上一层都有关系，而通过上一层的权重与上上层的节点也保持一定的关联，依次类推。</p><h2 id="正向传播-1"><a href="#正向传播-1" class="headerlink" title="正向传播"></a>正向传播</h2><p>正向传播中，每次求Z都是一次线性运算，相当于将x放入一个逐次增高的函数中，让x位于的维度越来越高，而w就是其每层幂中对于x的权重，在图像中，相当于是w在决定该维度中的决策边线的角度，b是因为单纯的旋转不足以让需要拟合的曲线到达需要的地方，还需要进行上下调整，因为决策边线是无限长的，所以上下调整配合角度旋转就可以到达任何需要拟合的位置。</p><p>而随着网络层数的增加，这个输出会到达较高的维度，这样的维度中决策边线就是曲线了。</p><p>ReLU函数(A = max(0, Z))的修正作用一方面使得被激活的函数呈非对称(偶)结构，另一方面优化了计算的步骤，对于Z1是负数的情况不需要考虑，由别的地方来给它解答。</p><p>softmax($S_i = \frac{e^{z_i}}{\sum_{j}^{N}e^{z_j}}$)的作用不言而喻，对于最可能的情况给予他最大的权重值，但是别的情况也有相似值可以考虑，这就好像模拟了人的认识过程，随着你越来越懂怎么辨认，你就越来越不会觉得可能是别的情况，但在一开始还是不清楚的。softmax还有一个好处就是它自带标准化，得出的值的和为1，这样我们不妨可以将每个值认作对应类的概率，从而与one-hot后的Y值不谋而合。只要我们优化函数使得最终对应类的值尽可能的接近1即可~</p><h2 id="计算成本"><a href="#计算成本" class="headerlink" title="计算成本"></a>计算成本</h2><p>计算成本的函数为交叉熵函数($L = - \sum_{i}^{N} y_i \log{a_{2_i}}$)，其中N指的是需要分辨的类的总数，也是softmax中得出的结果数量。这个函数与KL散度密切相关，当且仅当分布P和Q在离散变量的情况下是相同的分布，或者连续性变量的情况下是几乎“处处相同”时，KL散度为0。最小化成本即最小化交叉熵的过程就是拟合神经网络中的分布与给定数据的分布的过程，使得它们尽量的相似。在数据足够多的情况下，基本可以认为它服从真实的分布。</p><p>(原公式中是$y_i\log{\hat{y_i}}$，而在softmax的结果中我们基本可以认为$\hat{y} == a_{2_i}$)</p><p>每一批量的成本取值为该批量中的样本计算出的交叉熵的平均值，这样大体上可以模拟该批量中的样本所代表的分布与拟合出的分布之间的差距</p><h2 id="反向传播-1"><a href="#反向传播-1" class="headerlink" title="反向传播"></a>反向传播</h2><p>反向传播的意义在于什么呢，在于计算出成本函数关于各个参数(尤其是W，b)的梯度，而W和b的梯度需要借助其他参数得到(因为链式求导法则)，因此，我们需要沿着成本函数反向的计算各个参数关于成本的梯度，从而借此更方便的计算出W和b的梯度</p><p>关于这个梯度的作用的话，大家都知道，梯度方向是函数在该店沿着梯度方向变化最快的方向，所以借助梯度我们能更快的改变参数的值，让网络的性能更好。</p><h2 id="更新参数"><a href="#更新参数" class="headerlink" title="　更新参数"></a>　更新参数</h2><p>更新参数的过程才是使用梯度下降法的关键，更新参数需要参数减去学习率＊成本关于该参数的梯度。我们将梯度认作是一个让我们下降最快的方向的话，那么学习率就是我们下降的时候一步跨出的步长，比如在一个碗状的函数中，高度值代表着我们的误差值，那么沿着当前点的梯度方向跨出一步，理论上相对而言就是下降较快的一步，而不同的点的梯度不一样，所以并不能说明它是最快的。</p><p>并且随着我们越来越接近碗底，我们跨出的步长应该变得越来越小，从而使得我们能够更“小心地”接近最低点。所以学习率衰减是一个好办法。从物理上得出这样的理解，同样从物理上得到灵感的还有<a href="[https://1305936314.github.io/2019/12/01/Adam%E7%9A%84%E5%9B%9E%E9%A1%BE/](https://1305936314.github.io/2019/12/01/Adam的回顾/">动量梯度下降法</a>)，取其加权平均数，一般取$\beta_1$ = 0.9较多，即十个样本的加权平均值。</p><h2 id="循环批量-batch-，遍数-epoch"><a href="#循环批量-batch-，遍数-epoch" class="headerlink" title="循环批量(batch)，遍数(epoch)"></a>循环批量(batch)，遍数(epoch)</h2><p>我这里使用小批量(mini-batch)的是为了照顾后面对应使用x86汇编实现，暂时想的是mini-batch取20，因为10及以下容易在计算log的时候算到log0从而后面的值都是无效值。小批量循环至一遍结束后要继续遍历epoch次，我这里由于将第一层的神经元数调小了，遍历次数需要多一些，从100升到了120，有时150</p><p>并且，由于使用了小批量时每次向量化计算减少了，for循环需要的次数增多了，计算时间也会被明显拉长</p><h1 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h1><p>先训练后得出模型的重要参数W，b后，我们要进行测试，时间的评估以及准确度的评估</p><p>测试时将测试集放入参数中进行正向传播后得出A2，即模型的预测值，然后看预测值正确的数量来评估准确度。</p><h2 id="调参收获"><a href="#调参收获" class="headerlink" title="调参收获"></a>调参收获</h2><img src="/2019/11/28/%E7%94%A8numpy%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E5%8F%8C%E5%B1%82%E7%9A%84softmax%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/cost.png" class="" title="成本">    <p>这个是我在调小了第一层的神经元数1000=&gt;50后的成本和准确率，调小了神经元数之后呢，相应的这个需要遍历的次数变的高了不少，我觉得可能第一层的权重w初始化需要小一点比较好，本身初始化大小与节点数成反相关的。并且呢需要拟合的复杂度变高了，毕竟与每个节点的相关度变高了，这就相当于是反向正则化了(并且也确认了一个观点“增加网络的规模有一定正则化作用”是对的)，然后的话mini-batch不能太小，而在cost图中如果阴影部分比较分散的话，我个人觉得可能是需要学习率小一点，如果是多片分开的阴影的话我觉得可能需要神经元数多一点更好。</p>]]></content>
      
      
      <categories>
          
          <category> deeplearning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 微机课设 </tag>
            
            <tag> numpy </tag>
            
            <tag> 神经网络 </tag>
            
            <tag> MNIST </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>第一篇博客</title>
      <link href="/2019/11/21/%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/"/>
      <url>/2019/11/21/%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/</url>
      
        <content type="html"><![CDATA[<h1 id="0-最近事务"><a href="#0-最近事务" class="headerlink" title="0. 最近事务"></a>0. 最近事务</h1><h2 id="0-1-平时课程上"><a href="#0-1-平时课程上" class="headerlink" title="0.1 平时课程上"></a>0.1 平时课程上</h2><h3 id="0-1-1-微机原理"><a href="#0-1-1-微机原理" class="headerlink" title="0.1.1 微机原理"></a>0.1.1 微机原理</h3><p>[ ] 需要我<u>读完CAAE的论文</u>简介然后完善好最后的<strong>文献报告</strong><br>[x] 写一下softmax神经网络的python代码<br>[ ] 考完试开始康康汇编，学着用它写神经网络<br>[ ] CS231n的日常学习</p><h3 id="0-1-2-考试开始了"><a href="#0-1-2-考试开始了" class="headerlink" title="0.1.2 考试开始了"></a>0.1.2 考试开始了</h3><p>[ ] 编译原理期末考试</p><p>[x]概率论期末考试</p><hr><h2 id="0-2-项目里面"><a href="#0-2-项目里面" class="headerlink" title="0.2 项目里面"></a>0.2 项目里面</h2><h3 id="0-2-1-专利"><a href="#0-2-1-专利" class="headerlink" title="0.2.1 专利"></a>0.2.1 专利</h3><h3 id="0-2-2-原本代码的复现完善"><a href="#0-2-2-原本代码的复现完善" class="headerlink" title="0.2.2 原本代码的复现完善"></a>0.2.2 原本代码的复现完善</h3><h3 id="0-2-3-读新的论文，并结合来做失踪人口回归的项目"><a href="#0-2-3-读新的论文，并结合来做失踪人口回归的项目" class="headerlink" title="0.2.3 读新的论文，并结合来做失踪人口回归的项目"></a>0.2.3 读新的论文，并结合来做失踪人口回归的项目</h3>]]></content>
      
      
      <categories>
          
          <category> Todo </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 随便写写 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
