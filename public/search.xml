<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>CS231n_Week1_Assignment2</title>
      <link href="/2019/12/03/CS231n-Week1-Assignment2/"/>
      <url>/2019/12/03/CS231n-Week1-Assignment2/</url>
      
        <content type="html"><![CDATA[<h1 id="学习："><a href="#学习：" class="headerlink" title="学习："></a>学习：</h1><h2 id="数据驱动方法"><a href="#数据驱动方法" class="headerlink" title="数据驱动方法"></a>数据驱动方法</h2><p>一张图像在计算机的眼中就是一堆数字矩阵，大小为长x宽x通道数，其中每个数字在0~255之间代表像素。<br>图像即训练数据，而在分类任务中，图像的种类即label标签。<br>图像分类存在的问题与挑战：角度，光照，形变和遮挡，图片背景混乱，类内差异等</p><p>过去的尝试：<br>硬编码：计算图像边缘然后将边角进行分类，然后写一些规则对猫进行分类</p><p>现在：数据驱动方法(比深度学习更加广义)：</p><ol><li>收集大量的图片和标签数据集</li><li>使用机器学习来训练一个分类器</li><li>在新的图片上对分类器进行评估<a id="more"></a>CIFAR-10数据集：<br>拥有10种类别(:airplane:, :car:，:bird:，:cat:, :deer:, :dog:, :frog:, :horse:,  :ship:, :truck:)<br>5w训练图片，1w测试图片</li></ol><p>距离度量方法：</p><p><strong>L1范数</strong>：$ d_1 (I_1, I_2) = \sum_p \lvert I_1^p - I_2^P\rvert$</p><img src="/2019/12/03/CS231n-Week1-Assignment2/L1distance.png" class="" title="L1距离的运算演示"><p><strong>L2范数</strong>：$d_2(I_1, I_2 = \sqrt{\sum_p(I_1^p - I_2^p)^2}$</p><p>L1范数取决于你选择的坐标系统，当你转动坐标系统时将会改变它们之间的L1距离，而L2范式不会受此影响<br>因此，如果你输入的特征向量中的值对你的任务有一些重要的意义，则L1更适合一些<br>而对于比较通用的向量而言，L2会更自然一些</p><p>在实际演示中，分别使用L1，L2范数的结果中，L1的结果更受坐标轴的影响，而L2中只需要将坐标轴放在较为自然的地方即可<br><img src="/2019/12/03/CS231n-Week1-Assignment2/L1vsL2.png" class="" title="L1vsL2"></p><h2 id="K-NN-K-最近邻算法"><a href="#K-NN-K-最近邻算法" class="headerlink" title="K-NN: K-最近邻算法"></a>K-NN: K-最近邻算法</h2><p>NN算法复杂度：<br>训练时O(1)， 预测时O(N)<br>缺点：</p><ol><li>实用中由于部署设备不同，我们更希望分类器在预测时快，训练时慢是可以的</li><li>最邻近算法根据最近的点来计算决策边缘，这非常容易受到噪声等因素的影响</li></ol><p>NN与KNN的区别：<br>KNN会投票与其最近的多个点，从而使得周围的决策边缘变得平滑，即对噪声产生更大的鲁棒性。</p><p>设置超参数</p><p>分别提到四种想法：</p><ol><li><p>使用在数据集中准确率最好的超参数 </p><p>这样是不行的，因为会造成严重的过拟合，面对未知数据的性能会非常差</p></li><li><p>将数据分割成训练和测试集，并使用在测试集中性能最好的超参数</p><p>这样也是不好的，因为这样的超参数容易使分类器只在测试集中性能良好</p></li><li><p>将数据分成训练集，验证集和测试集，使用多组超参数在训练集中运行，并在验证集上进行验证， 并且使用在验证集中表现最好的，放入测试集中运行，看看效果(要保证验证集和测试集完全分隔)</p><p>这样得到的数据才是应该写入报告的测试结果，才能展现出训练的算法在未知数据中的表现</p></li><li><p>交叉验证法：(小数据集中常见)，将数据集分成多个小份，每次换超参数时使用不同的一份作为验证集</p><img src="/2019/12/03/CS231n-Week1-Assignment2/cross-validation.png" class="" title="交叉验证集的设置"></li></ol><p>KNN算法在图像分类中很少用到的原因：</p><ol><li><p>它在测试时运算时间较长，与我们的需求不符</p></li><li><p>L1，L2范数的衡量标准建立在向量上的距离函数不太适合表示图像视觉上的相似度</p><p>例子中分别遮挡，平移和改变颜色了图片后，L2范数值不变，所以并不是很适合用于表示图像之间视觉感知的差异</p></li><li><p>维度灾难，KNN算法需要的数据分布较为密集，这就对数据集的要求较高，对于图片而言就需要图片的数量较多，当分布处于较高维度时需要的样本量呈指数倍增长</p></li></ol><h1 id="作业："><a href="#作业：" class="headerlink" title="作业："></a>作业：</h1><h2 id="1-图像分类数据和label分别是什么，图像分类存在的问题与挑战"><a href="#1-图像分类数据和label分别是什么，图像分类存在的问题与挑战" class="headerlink" title="1. 图像分类数据和label分别是什么，图像分类存在的问题与挑战"></a>1. 图像分类数据和label分别是什么，图像分类存在的问题与挑战</h2><p>一张图像在计算机的眼中就是一堆数字矩阵，大小为长*宽*通道数，其中每个数字在0~255之间代表像素。<br>图像即训练数据，而在分类任务中，图像的种类即label标签。<br>图像分类存在的问题与挑战：角度，光照，形变和遮挡，图片背景混乱，类内差异等</p><h2 id="2-使用python加载一张彩色图片，观察像素值"><a href="#2-使用python加载一张彩色图片，观察像素值" class="headerlink" title="2. 使用python加载一张彩色图片，观察像素值"></a>2. 使用python加载一张彩色图片，观察像素值</h2><p>图片原图为：<br><img src="/2019/12/03/CS231n-Week1-Assignment2/prettygirl.jpg" class="" title="原图"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">img_v = plt.imread(<span class="string">'prettygirl.jpg'</span>)</span><br><span class="line">plt.imshow(img_v)</span><br><span class="line">print(img_v)</span><br></pre></td></tr></table></figure><img src="/2019/12/03/CS231n-Week1-Assignment2/problem2.png" class="" title="效果如图"><p>像素值全为0~255的整数值,并且坐标相邻的值容易相等</p><h2 id="3-L1范数，L2范数数学表达式，这两种度量分别适用于什么情况"><a href="#3-L1范数，L2范数数学表达式，这两种度量分别适用于什么情况" class="headerlink" title="3. L1范数，L2范数数学表达式，这两种度量分别适用于什么情况"></a>3. L1范数，L2范数数学表达式，这两种度量分别适用于什么情况</h2><p><strong>L1范数</strong>：$ d_1 (I_1, I_2) = \sum_p \lvert I_1^p - I_2^P\rvert$</p><p><strong>L2范数</strong>：$d_2(I_1, I_2 = \sqrt{\sum_p(I_1^p - I_2^p)^2}$</p><p>L1范数取决于你选择的坐标系统，当你转动坐标系统时将会改变它们之间的L1距离，而L2范式不会受此影响<br>因此，如果你输入的特征向量中的值对你的任务有一些重要的意义，则L1更适合一些<br>而对于比较通用的向量而言，L2会更自然一些</p><img src="/2019/12/03/CS231n-Week1-Assignment2/L1vsL2.png" class="" title="L1vsL2"><h2 id="4-描述近邻算法KNN，-NN算法的复杂度，为什么很少在图像中使用，以及它存在的问题"><a href="#4-描述近邻算法KNN，-NN算法的复杂度，为什么很少在图像中使用，以及它存在的问题" class="headerlink" title="4. 描述近邻算法KNN， NN算法的复杂度，为什么很少在图像中使用，以及它存在的问题"></a>4. 描述近邻算法KNN， NN算法的复杂度，为什么很少在图像中使用，以及它存在的问题</h2><p>NN算法复杂度：<br>训练时O(1)， 预测时O(N)</p><p>NN与KNN的区别：<br>KNN会投票与其最近的多个点，从而使得周围的决策边缘变得平滑，即对噪声产生更大的鲁棒性。</p><p>KNN算法在图像分类中很少用到的原因：</p><ol><li><p>它在测试时运算时间较长，与我们的需求不符</p></li><li><p>L1，L2范数的衡量标准建立在向量上的距离函数不太适合表示图像视觉上的相似度</p><p>例子中分别遮挡，平移和改变颜色了图片后，L2范数值不变，所以并不是很适合用于表示图像之间视觉感知的差异</p></li><li><p>维度灾难，KNN算法需要的数据分布较为密集，这就对数据集的要求较高，对于图片而言就需要图片的数量较多，当分布处于较高维度时需要的样本量呈指数倍增长</p></li></ol><h2 id="5-了解cifar-10数据集"><a href="#5-了解cifar-10数据集" class="headerlink" title="5. 了解cifar-10数据集"></a>5. 了解cifar-10数据集</h2><p>CIFAR-10数据集：<br>拥有10种类别(:airplane:, :car:，:bird:，:cat:, :deer:, :dog:, :frog:, :horse:,  :ship:, :truck:)<br>5w训练图片，1w测试图片</p><h2 id="6-超参数怎么选择合适-数据集如何划分"><a href="#6-超参数怎么选择合适-数据集如何划分" class="headerlink" title="6. 超参数怎么选择合适(数据集如何划分)"></a>6. 超参数怎么选择合适(数据集如何划分)</h2><p>分别提到四种想法：</p><ol><li><p>使用在数据集中准确率最好的超参数 </p><p>这样是不行的，因为会造成严重的过拟合，面对未知数据的性能会非常差</p></li><li><p>将数据分割成训练和测试集，并使用在测试集中性能最好的超参数</p><p>这样也是不好的，因为这样的超参数容易使分类器只在测试集中性能良好</p></li><li><p>将数据分成训练集，验证集和测试集，使用多组超参数在训练集中运行，并在验证集上进行验证， 并且使用在验证集中表现最好的，放入测试集中运行，看看效果(要保证验证集和测试集完全分隔)</p><p>这样得到的数据才是应该写入报告的测试结果，才能展现出训练的算法在未知数据中的表现</p></li><li><p>交叉验证法：(小数据集中常见)，将数据集分成多个小份，每次换超参数时使用不同的一份作为验证集</p></li></ol><img src="/2019/12/03/CS231n-Week1-Assignment2/cross-validation.png" class="" title="交叉验证集的设置">]]></content>
      
      
      <categories>
          
          <category> CS231n </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CS231n </tag>
            
            <tag> KNN </tag>
            
            <tag> L2 </tag>
            
            <tag> 超参数 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pyramid-GAN的代码结构</title>
      <link href="/2019/12/02/Pyramid-GAN%E7%9A%84%E4%BB%A3%E7%A0%81%E7%BB%93%E6%9E%84/"/>
      <url>/2019/12/02/Pyramid-GAN%E7%9A%84%E4%BB%A3%E7%A0%81%E7%BB%93%E6%9E%84/</url>
      
        <content type="html"><![CDATA[<h1 id="GAN的代码结构"><a href="#GAN的代码结构" class="headerlink" title="GAN的代码结构"></a>GAN的代码结构</h1><p>在GAN类外，是生成器类和判别器类的代码，而本篇中的GAN较为特殊，使用的是金字塔结构的判别器，有些地方需要额外注意。</p><h2 id="初始化函数"><a href="#初始化函数" class="headerlink" title="初始化函数"></a>初始化函数</h2><h3 id="参数解析器"><a href="#参数解析器" class="headerlink" title="参数解析器"></a>参数解析器</h3><p>首先是使用参数解析器(argparse)，方便直接在控制台使用Python命令来控制训练的部分超参数</p><p>本篇代码中含有的超参数包含：<br><a id="more"></a></p><div class="table-container"><table><thead><tr><th style="text-align:left">参数名</th><th style="text-align:center">类型</th><th style="text-align:left">默认值</th><th style="text-align:left">选择值</th><th style="text-align:left">帮助</th></tr></thead><tbody><tr><td style="text-align:left">description</td><td style="text-align:center">str</td><td style="text-align:left">none</td><td style="text-align:left">none</td><td style="text-align:left">对于本代码的描述</td></tr><tr><td style="text-align:left">dataset</td><td style="text-align:center">str</td><td style="text-align:left">CACD2000</td><td style="text-align:left">[‘CACD2000’,<br>‘UTKFace’,’FG_NET’,<br>‘wiki’,’MORPH’]</td><td style="text-align:left">训练使用的数据集</td></tr><tr><td style="text-align:left">epoch</td><td style="text-align:center">int</td><td style="text-align:left">50</td><td style="text-align:left"></td><td style="text-align:left">训练的遍数</td></tr><tr><td style="text-align:left">batch_size</td><td style="text-align:center">int</td><td style="text-align:left">8</td><td style="text-align:left">随显存提升而提高</td><td style="text-align:left">批量的大小</td></tr><tr><td style="text-align:left">input_size</td><td style="text-align:center">int</td><td style="text-align:left">224</td><td style="text-align:left"></td><td style="text-align:left">输入图片大小</td></tr><tr><td style="text-align:left">save_dir</td><td style="text-align:center">str</td><td style="text-align:left">‘D:/GAN/Pyramid-GAN/Dict/‘</td><td style="text-align:left"></td><td style="text-align:left">模型保存目录</td></tr><tr><td style="text-align:left">result_dir</td><td style="text-align:center">str</td><td style="text-align:left">‘D:/GAN/Pyramid-GAN/results’</td><td style="text-align:left"></td><td style="text-align:left">结果保存目录</td></tr><tr><td style="text-align:left">lrG</td><td style="text-align:center">float</td><td style="text-align:left">0.0001</td><td style="text-align:left">使用Adam时有人推荐3e-4</td><td style="text-align:left">生成器G的学习率</td></tr><tr><td style="text-align:left">lrD</td><td style="text-align:center">float</td><td style="text-align:left">0.0001</td><td style="text-align:left">同上</td><td style="text-align:left">判别器D的学习率</td></tr><tr><td style="text-align:left">beta1</td><td style="text-align:center">float</td><td style="text-align:left">0.5</td><td style="text-align:left"></td><td style="text-align:left">Adam中动量梯度下降超参数</td></tr><tr><td style="text-align:left">beta2</td><td style="text-align:center">float</td><td style="text-align:left">0.999</td><td style="text-align:left"></td><td style="text-align:left">Adam中根均方传播超参数</td></tr><tr><td style="text-align:left">gpu_mode</td><td style="text-align:center">bool</td><td style="text-align:left">True</td><td style="text-align:left"></td><td style="text-align:left">GPU模式</td></tr><tr><td style="text-align:left">benchmark_mode</td><td style="text-align:center">bool</td><td style="text-align:left">True</td><td style="text-align:left"></td><td style="text-align:left">标准模式</td></tr></tbody></table></div><h3 id="数据加载器"><a href="#数据加载器" class="headerlink" title="数据加载器"></a>数据加载器</h3><p>4个数据加载器dataloader0,1,2,3分别对应30岁以下，30~40，40~50和50以上的年龄图片</p><h3 id="网络模型"><a href="#网络模型" class="headerlink" title="网络模型"></a>网络模型</h3><p>生成器G，判别器D，G的Adam优化器和D的Adam优化器</p><h3 id="均方根误差"><a href="#均方根误差" class="headerlink" title="均方根误差"></a>均方根误差</h3><p>GPU模式使用CUDA训练G,D并且计算MSELoss(均方根误差)</p><h3 id="噪声样本z"><a href="#噪声样本z" class="headerlink" title="噪声样本z"></a>噪声样本z</h3><p>使用rand()生成batch_size个100*1*1的随机数，并且可以的话放入CUDA</p><h2 id="训练函数"><a href="#训练函数" class="headerlink" title="训练函数"></a>训练函数</h2><h3 id="训练历史"><a href="#训练历史" class="headerlink" title="训练历史"></a>训练历史</h3><p>使用train_hist元组，其中包含D_loss, G_loss, per_epoch_time和total_time</p><p>使用ones和zeros函数分别生成batch_size个真假标签，存入变量y_fake和y_real中</p><h3 id="训练D-1"><a href="#训练D-1" class="headerlink" title="训练D_1"></a>训练D_1</h3><p>使用enumerate函数遍历dataloader1中的数据，用iter表示迭代次数，数据形式为(x, y)</p><p>当使用GPU模式时，将x放入cuda，清除D优化器重的梯度，并将X分别投入4个path中，这四个path代表的是从输入图片开始，经过VGG的部分然后经过扩展的部分再形成3*3*1表示的结果</p><p>然后将四个path的结果投入D中，形成判别器D真实的训练结果，并以真实标签计算均方根误差，这里计算的是最小二乘公式的前半段，即$(D_w(\phi_{age}(x))-1)^2$，完整公式如下：</p><script type="math/tex; mode=display">\mathcal{L}_{GAN\_D} = {\frac{1}{2}\mathbb{E}_{x\sim P_{data}}(x)[(D_w(\phi_{age}(x)) -1)^2] +  \\ \frac{1}{2}\mathbb{E}_{x \sim P_{young}}(x)[D_w(\phi_{age}(G(x)))^2 + D_w(\phi_{age}(x))^2]}</script><p>这里计算了前半段的损失公式，并且进行反向传播。然后让D的优化器进行单次优化，即更新D的所有参数</p><h3 id="训练D-2"><a href="#训练D-2" class="headerlink" title="训练D_2"></a>训练D_2</h3><p>使用enumerate函数遍历dataloader0中的数据，即年轻样本</p><p>开始前，先将x的前8张进行重复形成fixed_noise，修正噪声，然后声明一个全局变量fixed_img_v，即图像修正值，将fixed_noise转换成变量放入并保存</p><p>然后将D优化器中的梯度清零</p><p>将x重新放入更新过参数的D的Path中，并得出新的判别结果并让其与0标签计算均方根误差，用于计算损失公式的$D_w(\phi_{age}(x))^2$部分</p><p>再将x投入G中，并将结果放入D中，得到的真实结果与0计算均方根误差，得到公式的$D_w(\phi_{age}(G(x)))^2$部分，并将两者相加得到的损失放入train_hist[‘D_loss’]中，然后进行反向传播并更新参数</p><h3 id="训练G"><a href="#训练G" class="headerlink" title="训练G"></a>训练G</h3><p>清除G优化器中的梯度，将x放入G中并将所得结果放入D，得到结果并计算均方根误差，得到：</p><script type="math/tex; mode=display">\mathcal{L}_{GAN\_G} = \mathbb{E}_{x \sim P_{young}(x)}[(D_w(\phi_{age}(G(x))) - 1)^2]</script><p>并且将生成结果G(x)与x计算均方根误差，得到pixel-level-loss，即：</p><script type="math/tex; mode=display">\mathcal{L}_{pixel} = \frac{1}{W\times H \times C}\lVert G(x)-x \rVert ^2_2</script><p>然后计算$L_G = 750 \times L_{GAN_G} + 0.2 \times L_{pixel}$</p><p>将其放入train_hist[‘G_loss’]中，并且进行反向传播并更新参数</p>]]></content>
      
      
      <categories>
          
          <category> deeplearning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deeplearning </tag>
            
            <tag> GAN </tag>
            
            <tag> code </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Adam的回顾</title>
      <link href="/2019/12/01/Adam%E7%9A%84%E5%9B%9E%E9%A1%BE/"/>
      <url>/2019/12/01/Adam%E7%9A%84%E5%9B%9E%E9%A1%BE/</url>
      
        <content type="html"><![CDATA[<h1 id="加权平均数"><a href="#加权平均数" class="headerlink" title="加权平均数"></a>加权平均数</h1><blockquote><p>  温馨提示：由于本篇latex公式较多，容易显示异常或者加载较慢</p></blockquote><p>公式：<br>$V_t = \beta_1V_{t-1} + (1-\beta_1)\theta_t$<br>其中$V_0 = 0$，所以展开后为：<br>$V_t = \beta_1^{t-1}(1-\beta_1)\theta_1 + \beta_1^{t-2}(1-\beta_1)\theta_2 + … + \beta_1^2(1-\beta_1)\theta_{t-2} + \beta_1(1-\beta_1)\theta_{t-1} + (1-\beta_1)\theta_t \\ = (1-\beta_1)\sum_{i=1}^{t} \beta_1^{t-i}\theta_{i}$</p><p>即$\frac{1}{1-\beta_1}V_t = \sum_{i=1}^{t}\beta_1^{t-i}\theta_i$，其中$\beta_1$是小于1的浮点数，所以在大约$\frac{1}{1-\beta_1}$次方后，$\beta_1^{\frac{1}{1-\beta_1}}$会小于1/e（这个数相当于是一个指数衰减函数，在权重小于1/e是算作较小值），可以逐渐算作忽略不计，则可视作  <strong>$V_t$为前$\frac{1}{1-\beta_1}$ 项的加权平均数</strong></p><p>另外，在数据的前期由于其数据比$\frac{1}{1-\beta_{1}}$小得多，所以不能算较多数的加权平均，为了弥补数据量还不足的问题，可以除以偏差修正量$1-\beta_1^t$，而这个量会随着t的增大而逐渐趋向于1，所以不会影响后面的值</p><p>所以最终取的值为：<br>$\frac{V_t}{1-\beta_1^t} = \frac{1-\beta_1}{1-\beta_1^t}\sum_{i=1}^t\beta_1^{t-i}\theta_i$</p><a id="more"></a><h1 id="动量梯度下降法-Momentum"><a href="#动量梯度下降法-Momentum" class="headerlink" title="动量梯度下降法(Momentum)"></a>动量梯度下降法(Momentum)</h1><p>我们一般把梯度下降法比作是在爬山中的下坡，不断地迭代就是为了找出当前更快的下山的路，梯度下降法的迭代计算式为：<br>$\left\{\begin{array}{l}{w = w - \alpha dw} \\ {b = b - \alpha db}\end{array}\right.$</p><p>w和b分别可以表示每次跨的步数多少，和一个跨步的固定最小的值，wx+b可以代表了下山的路线，所以，对于正确的下山路线而言，训练的过程可能是上下摆动较大的，而我们之前说过$\frac{1}{1-\beta_1}$代表了$V_t$表示着多少天的加权平均，所以$\beta_1$越大，代表着越多值的平均值，而取较大的$\beta_1$意味着得到更平坦的图像(路线)，所以使用加权平均数可以使得dw和db的变化变得平稳，我们分别用$V_{dw},V_{db}$表示他们使用加权平均后的值，这样可以使得w和b的变化更平稳，则wx+b也会变得更加平坦</p><p>流程：<br>On iteration t:<br>    计算当前小批量的dw, db<br>    $V_{dw} = \beta_1V_{dw} + (1-\beta_1)dw \\ V_{db} = \beta_1V_{db} + (1-\beta_1)db \\ w = w - \alpha V_{dw}, b = b - \alpha V_{db}$</p><p>其中的超参数：α(学习率)，$\beta_1$(控制指数加权平均，最常用0.9)</p><p>Note：由于数据量较大，一般$\beta_1$取10，而前10个数据对于整体数据而言几乎可以忽略不计，所以不必添加偏差修正量</p><h1 id="根均方传播法-RMS-prop"><a href="#根均方传播法-RMS-prop" class="headerlink" title="根均方传播法(RMS prop)"></a>根均方传播法(RMS prop)</h1><p>函数在向数据的正确分布拟合的时候，x的参数w越大，往往能更快的接近正确的分布(当然也不能过大)，而b较大的时候容易过大而走弯路，所以一般不能太大，所以一般来说我们鼓励在学习过程中w变得更大一点而b变得更小一点。由此我们可以使用RMSprop方法来平衡w较小而b较大的情况</p><p>流程：<br>On iteration t:<br>    计算当前小批量的dw, db<br>RMSprop: $S_{dw} = \beta_2 S_{dw} + (1 - \beta_2)(dw)^2 \\ S_{db} = \beta_2 S_{db} + (1- \beta_2)(dw)^2$<br>更新参数：$w = w - \alpha \frac{dw}{\sqrt{S_{dw}} + \epsilon} \\ b = b - \alpha \frac{db}{\sqrt{S_{db}} + \epsilon}$</p><p>其中公式中前后$S_{dw}, S_{db}$分别代表前t次，前t-1次迭代累计的dw的平方，相对本次db而言，它们是小的，所以更新后的也 $S_{db}$相对db而言是较小数，那么$\frac{db}{\sqrt{S_{db}} + \epsilon}$是一个较大的数，b减去一个较大的数后就可以更新为一个较小数，同理dw也可以更新为一个较大数</p><p>Note:ε的存在一般是为了防止$S_{dw}, S_{db}$过小导致算出来的值爆炸，所以加上一个几乎不影响结果的较小值，常用$10^{-8}、 10^{-6}$</p><h1 id="Adam优化算法-Adaptive-moment-Estimation"><a href="#Adam优化算法-Adaptive-moment-Estimation" class="headerlink" title="Adam优化算法(Adaptive moment Estimation)"></a>Adam优化算法(Adaptive moment Estimation)</h1><p>Adam优化算法就是将RMS传播算法和动量梯度下降法结合起来的算法</p><p>流程为：<br>$V_{dw} = 0,S_{dw} = 0, V_{db} = 0, S_{db} = 0$<br>On iteration t:<br>    计算当前小批量的dw, db<br>    $\left.\begin{array}{l}{V_{dw} = \beta_1 V_{dw} + (1-\beta_1)dw} \\<br>     {V_{db} = \beta_1 V_{db} + (1-\beta_1)db }\end{array}\right\}$”Momentum”动量梯度下降<br>    $\left.\begin{array}{l}{S_{dw}= \beta_2 S_{dw} + (1-\beta_2)(dw)^2} \\<br>     {S_{db}=\beta_2 S_{dw} + (1 -\beta_2)(db)^2}\end{array}\right\}$”RMSprop”根均方传播算法<br>偏差修正：<br>    $V^{corrected}_{dw} = \frac{V_{dw}}{1-\beta_1^t} \\<br>     V^{corrected}_{db} = \frac{V_{db}}{1-\beta_1^t} \\<br>     S^{corrected}_{dw} = \frac{S_{dw}}{1-\beta_2^t} \\<br>     S^{corrected}_{db} = \frac{S_{db}}{1-\beta_2^t}$<br>更新参数：$w = w - \alpha \frac{V^{corrected}_{dw}}{\sqrt{S^{corrected}_{dw}} + \epsilon} \\<br>         b = b-\alpha \frac{V^{corrected}_{db}}{\sqrt{S^{corrected}_{db}} +\epsilon}$</p><p>超参数：<br>α：学习率，需要调试<br>$\beta_1$：常用0.9            （dw的加权平均数，第一<em>矩（待了解）</em>）<br>$\beta_2$：推荐0.999        （$dw^2$的加权平均数，第二矩）<br>ε：$10^{-8}$                    （不需要太在意）</p><p>总而言之，Adam优化算法就是将RMS传播算法和动量梯度下降法结合在一起，前者可以减少偏差以加快学习，后者可以使图像更加平坦，从而加快学习。</p>]]></content>
      
      
      <categories>
          
          <category> deeplearning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deeplearning </tag>
            
            <tag> 优化器 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CS231n_Week1_Assignment1</title>
      <link href="/2019/11/30/CS231n-Week1-Assignment1/"/>
      <url>/2019/11/30/CS231n-Week1-Assignment1/</url>
      
        <content type="html"><![CDATA[<h1 id="作业："><a href="#作业：" class="headerlink" title="作业："></a>作业：</h1><p>(阅读理解题)</p><h2 id="1-图像的数据主要来源有哪些"><a href="#1-图像的数据主要来源有哪些" class="headerlink" title="1. 图像的数据主要来源有哪些"></a>1. 图像的数据主要来源有哪些</h2><p>照相机，手机，摄像头等一系列视觉传感器设备生产出了大量的图像数据</p><h2 id="2-sift-feature，金字塔匹配思想和hog特征分别是什么，可以用来干什么"><a href="#2-sift-feature，金字塔匹配思想和hog特征分别是什么，可以用来干什么" class="headerlink" title="2. sift feature，金字塔匹配思想和hog特征分别是什么，可以用来干什么"></a>2. sift feature，金字塔匹配思想和hog特征分别是什么，可以用来干什么</h2><ol><li><strong>sift feature：</strong>确认目标上在变化中具有表现型和不变性的特征，然后把这些特征与相似的目标进行匹配<a id="more"></a></li><li><strong>金字塔匹配思想：</strong>从图片的各部分的各个像素抽取特征并把它们放到一起作为一个特征描述符，然后再特征描述符上做一个支持向量机</li><li><strong>hog特征：</strong>histogram of gradients方向梯度直方图，将一堆特征放在一起后，研究如何在实际图片中比较合理的设计人体姿态和辨认人体姿态</li></ol><h2 id="3-神经网络早就存在为什么最近才兴起"><a href="#3-神经网络早就存在为什么最近才兴起" class="headerlink" title="3. 神经网络早就存在为什么最近才兴起"></a>3. 神经网络早就存在为什么最近才兴起</h2><ol><li>算力的提升</li><li>高质量的数据集的数量增多</li></ol><h2 id="4-图像任务有哪些，解决什么样的图像问题"><a href="#4-图像任务有哪些，解决什么样的图像问题" class="headerlink" title="4. 图像任务有哪些，解决什么样的图像问题"></a>4. 图像任务有哪些，解决什么样的图像问题</h2><p>图像分类，目标检测，图像描述</p>]]></content>
      
      
      <categories>
          
          <category> CS231n </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CS231n </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>用numpy实现一个双层的softmax神经网络</title>
      <link href="/2019/11/28/%E7%94%A8numpy%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E5%8F%8C%E5%B1%82%E7%9A%84softmax%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
      <url>/2019/11/28/%E7%94%A8numpy%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E5%8F%8C%E5%B1%82%E7%9A%84softmax%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<blockquote><p>  使用的数值化数据集：<br>  链接：<a href="https://pan.baidu.com/s/110dhDKA8eXYV4-kfmevo0g" target="_blank" rel="noopener">https://pan.baidu.com/s/110dhDKA8eXYV4-kfmevo0g</a><br>  提取码：x3wu</p></blockquote><h1 id="整体思路"><a href="#整体思路" class="headerlink" title="整体思路"></a>整体思路</h1><p>使用Numpy和pandas写一个神经网络，进行手写数字识别(MNIST)，样本图如下：</p><img src="/2019/11/28/%E7%94%A8numpy%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E5%8F%8C%E5%B1%82%E7%9A%84softmax%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/output_13_0.png" class="" title="手写数字5"><h1 id="总思路"><a href="#总思路" class="headerlink" title="总思路"></a>总思路</h1><p>整体神经网络代码只使用Numpy和pandas，根据额外需要使用time和matplotlib，设计思路如下</p><p>数据处理-&gt; 建立模型 -&gt; 进行训练 -&gt; 进行测试 -&gt; 评估时间和精度<br><a id="more"></a></p><h1 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h1><p>数据分为训练集和测试集，分别是785*6w和785*1w的数据，785列中前784列代表了28*28的图片的数值化数据，第785列为0~9的标签，并且第一行为0~784的索引</p><ol><li>使用pandas读取训练和测试数据</li><li>分别把训练和测试数据中的数据和标签分离</li><li>将标签进行one-hot编码</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练集有6w, 785列，前784为28*28,最后一列为标签</span></span><br><span class="line">train = pd.read_csv(<span class="string">"MNIST_Training_60K.csv"</span>,index_col=<span class="literal">False</span>,header=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 测试集有1w，784列，28*28的图片</span></span><br><span class="line">test = pd.read_csv(<span class="string">"MNIST_Test_10K.csv"</span>,index_col=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">label = train[:][<span class="string">"784"</span>]</span><br><span class="line">train = train.drop([<span class="string">"784"</span>],axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">X = train.T</span><br><span class="line">Y = np.zeros((<span class="number">10</span>, X.shape[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以下为成像代码</span></span><br><span class="line"><span class="comment"># import matplotlib.pyplot as plt</span></span><br><span class="line"><span class="comment"># x = np.array(X)</span></span><br><span class="line"><span class="comment"># X_train = x.reshape((60000,28,28))</span></span><br><span class="line"><span class="comment"># cur=X_train[0:1]</span></span><br><span class="line"><span class="comment"># plt.imshow(cur[0].reshape(h,w).T,cmap='gray')</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将标签one_hot编码</span></span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> range(X.shape[<span class="number">1</span>]):</span><br><span class="line">    Y[label[n]][n] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 处理test数据</span></span><br><span class="line">label_test = test[:][<span class="string">"784"</span>]</span><br><span class="line">test1 = test.drop([<span class="string">"784"</span>],axis=<span class="number">1</span>)</span><br><span class="line">X_test = test1.T</span><br><span class="line"><span class="comment"># Y_test = np.eye(10)[l]</span></span><br></pre></td></tr></table></figure><h1 id="建立模型"><a href="#建立模型" class="headerlink" title="建立模型"></a>建立模型</h1><h2 id="正向传播"><a href="#正向传播" class="headerlink" title="正向传播"></a>正向传播</h2><ol><li><p>参数初始化</p><p>采用”He”初始化，即使用标准化的服从高斯分布的随机数乘上 2/前一层网络单元数的根号幂初始化权重W，并且使用0初始化对偏差b进行初始化</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">(n_x, n_h, n_y)</span>:</span></span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    W1 = np.random.randn(n_h, n_x) * np.sqrt(<span class="number">2.</span> / n_x)</span><br><span class="line">    b1 = np.zeros((n_h, <span class="number">1</span>))</span><br><span class="line">    W2 = np.random.randn(n_y, n_h)  * np.sqrt(<span class="number">2.</span> / n_h)</span><br><span class="line">    b2 = np.zeros((n_y, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 断言保证这些参数的形状如下，否则报错</span></span><br><span class="line">    <span class="keyword">assert</span>(W1.shape == (n_h, n_x))</span><br><span class="line">    <span class="keyword">assert</span>(b1.shape == (n_h,   <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">assert</span>(W2.shape == (n_y, n_h))</span><br><span class="line">    <span class="keyword">assert</span>(b2.shape == (n_y,   <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将参数封装</span></span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>:W1,</span><br><span class="line">                  <span class="string">"b1"</span>:b1,</span><br><span class="line">                  <span class="string">"W2"</span>:W2,</span><br><span class="line">                  <span class="string">"b2"</span>:b2&#125;</span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>注意：</p><p>权重w本来的维度应该是(前一层神经元数，本层神经元数)，但是为了方便后续计算省略掉此处的转置操作，在一开始就选择将W的维度设置为(本层神经元数，前一层大小神经元数)</p><p>而偏差b由于需要整层网络共享，我们基于Python的广播机制，选择让它的大小设置为(本层神经元数，1)</p></li></ol><ol><li><p>正向线性传播</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_forward</span><span class="params">(A0, W1, b1)</span>:</span></span><br><span class="line">    Z1 = np.dot(W1, A0) + b1</span><br><span class="line">    <span class="comment"># 即Z1 = W1 * A0 + b1</span></span><br><span class="line">    <span class="keyword">assert</span>(Z1.shape == (W1.shape[<span class="number">0</span>], A0.shape[<span class="number">1</span>]))</span><br><span class="line">    </span><br><span class="line">    cache = (A0, W1, b1)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> Z1, cache</span><br></pre></td></tr></table></figure><p>无特别说明</p></li></ol><ol><li><p>正向激活</p><p>隐藏层使用ReLU(整流线性单元)函数，输出层使用softmax函数，期望它的值符合0~9十个分类。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu</span><span class="params">(Z1)</span>:</span></span><br><span class="line">    A1 = np.maximum(<span class="number">0</span>, Z1) </span><br><span class="line">    cache = Z1</span><br><span class="line">    <span class="keyword">return</span> A1, cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(Z2)</span>:</span></span><br><span class="line">    A2 = np.exp(Z2) / sum(np.exp(Z2))</span><br><span class="line">    </span><br><span class="line">    cache = Z2</span><br><span class="line">    <span class="keyword">return</span> A2, cache</span><br></pre></td></tr></table></figure></li></ol><ol><li><p>正向传播函数</p><p>将线性传播部分和激活函数合并</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_forward_relu</span><span class="params">(A0, W1, b1)</span>:</span></span><br><span class="line">    Z1, linear_cache = linear_forward(A0, W1, b1)</span><br><span class="line">    A1, activation_cache = relu(Z1)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(A1.shape == (W.shape[<span class="number">0</span>], A0.shape[<span class="number">1</span>]))</span><br><span class="line">    cache = (linear_cache, activation_cache)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> A1, cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_forward_softmax</span><span class="params">(A1, W2, b2)</span>:</span></span><br><span class="line">    Z2, linear_cache = linear_forward(A1, W2, b2)</span><br><span class="line">    A2, activation_cache = softmax(Z2)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(A2.shape == (W2.shape[<span class="number">0</span>], A1.shape[<span class="number">1</span>]))</span><br><span class="line">    cache = (linear_cache, activation_cache)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> A2, cache</span><br></pre></td></tr></table></figure></li></ol><h2 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h2><ol><li><p>计算成本</p><p>使用交叉熵作为成本函数，其函数的公式为：</p><script type="math/tex; mode=display">\sum_{i}^{m} y_i\log{\hat{y_{i}}}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(A2, Y)</span>:</span></span><br><span class="line">    <span class="comment"># 计算交叉熵</span></span><br><span class="line">    m = Y.shape[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算loss函数</span></span><br><span class="line">    logprobs = <span class="number">-1</span>* sum(np.multiply(np.log(A2), Y))</span><br><span class="line">    <span class="comment"># 成本函数就是将整个训练集上的损失相加取相反数</span></span><br><span class="line">    cost = <span class="number">-1</span>/m * np.sum(logprobs)</span><br><span class="line">    cost = np.squeeze(cost)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cost, logprobs</span><br></pre></td></tr></table></figure></li></ol><ol><li><p>反向传播</p><p>计算各个参数基于成本的梯度，用于更新参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_propagation</span><span class="params">(parameters, cache, X, Y)</span>:</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    W1 = parameters[<span class="string">'W1'</span>]</span><br><span class="line">    W2 = parameters[<span class="string">'W2'</span>]</span><br><span class="line">    </span><br><span class="line">    A1 = cache[<span class="string">'A1'</span>]</span><br><span class="line">    A2 = cache[<span class="string">'A2'</span>]</span><br><span class="line">    </span><br><span class="line">    dZ2 = A2 - Y</span><br><span class="line">    dW2 = <span class="number">1</span>/m * np.dot(dZ2, A1.T)</span><br><span class="line">    db2 = <span class="number">1</span>/m * np.sum(dZ2, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    dA1 = np.dot(W2.T, dZ2)</span><br><span class="line">    dZ1 = np.array(dA1, copy=<span class="literal">True</span>)</span><br><span class="line">    dZ1[Z1 &lt;=<span class="number">0</span> ] = <span class="number">0</span> <span class="comment"># 标记一下，操作十分神奇,牛逼！</span></span><br><span class="line">    dW1 = <span class="number">1</span>/m * np.dot(dZ1, X.T)</span><br><span class="line">    db1 = <span class="number">1</span>/m * np.sum(dZ1, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    grads=&#123;<span class="string">"dW1"</span>:dW1,</span><br><span class="line">           <span class="string">"db1"</span>:db1,</span><br><span class="line">           <span class="string">"dW2"</span>:dW2,</span><br><span class="line">           <span class="string">"db2"</span>:db2&#125;</span><br><span class="line">    <span class="keyword">return</span> grads</span><br></pre></td></tr></table></figure><p>各参数传播的顺序和算式如下：</p><img src="/2019/11/28/%E7%94%A8numpy%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E5%8F%8C%E5%B1%82%E7%9A%84softmax%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/propagation.jpg" class="" title="参数传播"></li><li><p>更新参数</p><p>使用mini-batch梯度下降法更新参数，目前仅实现了梯度下降法，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span><span class="params">(parameters, grads, learning_rate=<span class="number">1.2</span>)</span>:</span></span><br><span class="line">    W1 = parameters[<span class="string">'W1'</span>]</span><br><span class="line">    b1 = parameters[<span class="string">'b1'</span>]</span><br><span class="line">    W2 = parameters[<span class="string">'W2'</span>]</span><br><span class="line">    b2 = parameters[<span class="string">'b2'</span>]</span><br><span class="line">    </span><br><span class="line">    dW1 = grads[<span class="string">'dW1'</span>]</span><br><span class="line">    db1 = grads[<span class="string">'db1'</span>]</span><br><span class="line">    dW2 = grads[<span class="string">'dW2'</span>]</span><br><span class="line">    db2 = grads[<span class="string">'db2'</span>]</span><br><span class="line">    </span><br><span class="line">    W1 = W1 - learning_rate* dW1</span><br><span class="line">    b1 = b1 - learning_rate* db1</span><br><span class="line">    W2 = W2 - learning_rate* dW2</span><br><span class="line">    b2 = b2 - learning_rate* db2</span><br><span class="line">    </span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</span><br><span class="line">                  <span class="string">"b1"</span>: b1,</span><br><span class="line">                  <span class="string">"W2"</span>: W2,</span><br><span class="line">                  <span class="string">"b2"</span>: b2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure></li></ol><h2 id="整体模型"><a href="#整体模型" class="headerlink" title="整体模型"></a>整体模型</h2><p>将上述函数等无缝合并，方便转换</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nn_model</span><span class="params">(X, Y, n_h=<span class="number">1000</span>, learning_rate=<span class="number">0.12</span> ,num_iterations=<span class="number">10000</span>, print_cost=False)</span>:</span></span><br><span class="line">    n_x = <span class="number">784</span></span><br><span class="line">    n_h = <span class="number">1000</span></span><br><span class="line">    n_y = <span class="number">10</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 随机初始化参数</span></span><br><span class="line">    np.random.seed(<span class="number">3</span>)</span><br><span class="line">    W1 = np.random.randn(n_h, n_x) * np.sqrt(<span class="number">2.</span> / n_x)</span><br><span class="line">    b1 = np.zeros((n_h, <span class="number">1</span>))</span><br><span class="line">    W2 = np.random.randn(n_y, n_h)  * np.sqrt(<span class="number">2.</span> / n_h)</span><br><span class="line">    b2 = np.zeros((n_y, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line">        <span class="comment"># 正向传播: 线性-&gt; ReLU -&gt; 线性-&gt; softmax</span></span><br><span class="line">        Z1 = np.dot(W1, X) + b1</span><br><span class="line">        A1 = np.maximum(<span class="number">0</span>, Z1)</span><br><span class="line">        Z2 = np.dot(W2, A1) + b2</span><br><span class="line">        A2 = np.exp(Z2) / sum(np.exp(Z2))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算成本：计算交叉熵成本</span></span><br><span class="line">        logprobs = <span class="number">-1</span>* sum(np.multiply(np.log(A2), Y))</span><br><span class="line">        cost = <span class="number">-1</span>/m * np.sum(logprobs)</span><br><span class="line">        cost = np.squeeze(cost)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 反向传播：梯度下降法</span></span><br><span class="line">        dZ2 = A2 - Y</span><br><span class="line">        dW2 = <span class="number">1</span>/m * np.dot(dZ2, A1.T)</span><br><span class="line">        db2 = <span class="number">1</span>/m * np.sum(dZ2, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">        dA1 = np.dot(W2.T, dZ2)</span><br><span class="line">        dZ1 = np.array(dA1, copy=<span class="literal">True</span>)</span><br><span class="line">        dZ1[Z1 &lt;=<span class="number">0</span> ] = <span class="number">0</span> <span class="comment"># 标记一下，操作十分神奇,牛逼！</span></span><br><span class="line">        dW1 = <span class="number">1</span>/m * np.dot(dZ1, X.T)</span><br><span class="line">        db1 = <span class="number">1</span>/m * np.sum(dZ1, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 更新参数</span></span><br><span class="line">        W1 -= learning_rate * dW1</span><br><span class="line">        b1 -= learning_rate * db1</span><br><span class="line">        W2 -= learning_rate * dW2</span><br><span class="line">        b2 -= learning_rate * db2</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Cost after iteration %i: %f"</span> %(i, cost))</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> W1, b1, W2, b2</span><br></pre></td></tr></table></figure><p>此处只为保留一下未mini-batch版本</p><h1 id="训练测试"><a href="#训练测试" class="headerlink" title="训练测试"></a>训练测试</h1><p>预测函数如下，即正向传播过程</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(X, W1, b1, W2, b2)</span>:</span></span><br><span class="line">    Z1 = np.dot(W1, X) + b1</span><br><span class="line">    A1 = np.maximum(<span class="number">0</span>, Z1)</span><br><span class="line">    Z2 = np.dot(W2, A1) + b2</span><br><span class="line">    A2 = np.exp(Z2) / sum(np.exp(Z2))</span><br><span class="line">    predictions = A2.argmax(axis=<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 使用argmax使得返回值predictions为和label相同的0~9标签</span></span><br><span class="line">    <span class="keyword">return</span> predictions</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line">tic = time.process_time()</span><br><span class="line">W1, b1, W2, b2= nn_model(X, Y,n_h=<span class="number">300</span>, learning_rate=<span class="number">0.08</span>, num_iterations=<span class="number">10000</span>, print_cost=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line">predictions = predict(X_test, W1, b1, W2, b2)</span><br><span class="line">toc = time.process_time()</span><br><span class="line"><span class="comment"># 时间</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Computation time = "</span> + str((toc - tic)) + <span class="string">"s"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 评估精确度</span></span><br><span class="line">correct_predictions = np.equal(predictions, label_test)</span><br><span class="line">accuracy = np.mean(correct_predictions.astype(np.float32))</span><br><span class="line">print(<span class="string">'Test Accuracy:%f'</span>%(accuracy*<span class="number">100</span>) + <span class="string">'%'</span>)</span><br></pre></td></tr></table></figure><h1 id="所有代码合并"><a href="#所有代码合并" class="headerlink" title="所有代码合并"></a>所有代码合并</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="comment"># import matplotlib.pyplot as plt</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练集有6w, 785列，前784为28*28,最后一列为标签</span></span><br><span class="line">train = pd.read_csv(<span class="string">"MNIST_Training_60K.csv"</span>,index_col=<span class="literal">False</span>,header=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 测试集有1w，784列，28*28的图片</span></span><br><span class="line">test = pd.read_csv(<span class="string">"MNIST_Test_10K.csv"</span>,index_col=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">label = train[:][<span class="string">"784"</span>]</span><br><span class="line">train = train.drop([<span class="string">"784"</span>],axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">X = train.T</span><br><span class="line">Y = np.zeros((<span class="number">10</span>, X.shape[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> range(X.shape[<span class="number">1</span>]):</span><br><span class="line">    Y[label[n]][n] = <span class="number">1</span></span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nn_model</span><span class="params">(X, Y, n_h=<span class="number">1000</span>, learning_rate=<span class="number">0.12</span> ,num_iterations=<span class="number">10000</span>, print_cost=False)</span>:</span></span><br><span class="line">    n_x = <span class="number">784</span></span><br><span class="line">    n_y = <span class="number">10</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 随机初始化参数</span></span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    W1 = np.random.randn(n_h, n_x) * np.sqrt(<span class="number">2.</span> / n_x)</span><br><span class="line">    b1 = np.zeros((n_h, <span class="number">1</span>))</span><br><span class="line">    W2 = np.random.randn(n_y, n_h)  * np.sqrt(<span class="number">2.</span> / n_h)</span><br><span class="line">    b2 = np.zeros((n_y, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line">        <span class="comment"># 正向传播: 线性-&gt; ReLU -&gt; 线性-&gt; softmax</span></span><br><span class="line">        Z1 = np.dot(W1, X) + b1</span><br><span class="line">        A1 = np.maximum(<span class="number">0</span>, Z1)</span><br><span class="line">        Z2 = np.dot(W2, A1) + b2</span><br><span class="line">        A2 = np.exp(Z2) / sum(np.exp(Z2))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算成本：计算交叉熵成本</span></span><br><span class="line">        logprobs = <span class="number">-1</span>* sum(np.multiply(np.log(A2), Y))</span><br><span class="line">        cost = <span class="number">-1</span>/m * np.sum(logprobs)</span><br><span class="line">        cost = np.squeeze(cost)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 反向传播：梯度下降法</span></span><br><span class="line">        dZ2 = A2 - Y</span><br><span class="line">        dW2 = <span class="number">1</span>/m * np.dot(dZ2, A1.T)</span><br><span class="line">        db2 = <span class="number">1</span>/m * np.sum(dZ2, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">        dA1 = np.dot(W2.T, dZ2)</span><br><span class="line">        dZ1 = np.array(dA1, copy=<span class="literal">True</span>)</span><br><span class="line">        dZ1[Z1 &lt;=<span class="number">0</span> ] = <span class="number">0</span> <span class="comment"># 标记一下，操作十分神奇,牛逼！</span></span><br><span class="line">        dW1 = <span class="number">1</span>/m * np.dot(dZ1, X.T)</span><br><span class="line">        db1 = <span class="number">1</span>/m * np.sum(dZ1, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 更新参数</span></span><br><span class="line">        W1 -= learning_rate * dW1</span><br><span class="line">        b1 -= learning_rate * db1</span><br><span class="line">        W2 -= learning_rate * dW2</span><br><span class="line">        b2 -= learning_rate * db2</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Cost after iteration %i: %f"</span> %(i, cost))</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> W1, b1, W2, b2</span><br><span class="line"></span><br><span class="line">label_test = test[:][<span class="string">"784"</span>]</span><br><span class="line">test1 = test.drop([<span class="string">"784"</span>],axis=<span class="number">1</span>)</span><br><span class="line">X_test = test1.T</span><br><span class="line"><span class="comment"># Y_test = np.eye(10)[l]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line">tic = time.process_time()</span><br><span class="line">W1, b1, W2, b2= nn_model(X, Y,n_h=<span class="number">300</span>, learning_rate=<span class="number">0.08</span>, num_iterations=<span class="number">10000</span>, print_cost=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line">Z1 = np.dot(W1, X_test) + b1</span><br><span class="line">A1 = np.maximum(<span class="number">0</span>, Z1)</span><br><span class="line">Z2 = np.dot(W2, A1) + b2</span><br><span class="line">A2 = np.exp(Z2) / sum(np.exp(Z2))</span><br><span class="line">predictions = A2.argmax(axis=<span class="number">0</span>)</span><br><span class="line">toc = time.process_time()</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Computation time = "</span> + str((toc - tic)) + <span class="string">"s"</span>)</span><br><span class="line"></span><br><span class="line">correct_predictions = np.equal(predictions, label_test)</span><br><span class="line">accuracy = np.mean(correct_predictions.astype(np.float32))</span><br><span class="line">print(<span class="string">'Test Accuracy:%f'</span>%(accuracy*<span class="number">100</span>) + <span class="string">'%'</span>)</span><br></pre></td></tr></table></figure><h1 id="添加mini-batch"><a href="#添加mini-batch" class="headerlink" title="添加mini-batch"></a>添加mini-batch</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练集有6w, 785列，前784为28*28,最后一列为标签</span></span><br><span class="line">train = pd.read_csv(<span class="string">"./MNIST_Training_60K.csv"</span>, index_col=<span class="literal">False</span>, header=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 测试集有1w，784列，28*28的图片</span></span><br><span class="line">test = pd.read_csv(<span class="string">"./MNIST_Test_10K.csv"</span>, index_col=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">label = train[:][<span class="string">"784"</span>]</span><br><span class="line">train = train.drop([<span class="string">"784"</span>], axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">X = train.T</span><br><span class="line">Y = np.zeros((<span class="number">10</span>, X.shape[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> range(X.shape[<span class="number">1</span>]):</span><br><span class="line">    Y[label[n]][n] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nn_model</span><span class="params">(X, Y, n_h=<span class="number">1000</span>, mini_batch_size=<span class="number">100</span>, learning_rate=<span class="number">0.08</span>, epoch=<span class="number">1000</span>, print_cost=False)</span>:</span></span><br><span class="line">    n_x = <span class="number">784</span></span><br><span class="line">    n_y = <span class="number">10</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 随机初始化参数</span></span><br><span class="line">    np.random.seed(<span class="number">3</span>)</span><br><span class="line">    W1 = np.random.randn(n_h, n_x) * np.sqrt(<span class="number">2.</span> / n_x)</span><br><span class="line">    b1 = np.zeros((n_h, <span class="number">1</span>))</span><br><span class="line">    W2 = np.random.randn(n_y, n_h) * np.sqrt(<span class="number">2.</span> / n_h)</span><br><span class="line">    b2 = np.zeros((n_y, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    costs=[]</span><br><span class="line">    complete_numof_mini_batch = m // mini_batch_size</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, epoch):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, complete_numof_mini_batch):</span><br><span class="line">            <span class="comment"># 正向传播: 线性-&gt; ReLU -&gt; 线性-&gt; softmax</span></span><br><span class="line">            mini_batch_X = X.values[:, (j * mini_batch_size): (j + <span class="number">1</span>) * mini_batch_size]</span><br><span class="line">            mini_batch_Y = Y[:, (j * mini_batch_size): (j + <span class="number">1</span>) * mini_batch_size]</span><br><span class="line">            Z1 = np.dot(W1, mini_batch_X) + b1</span><br><span class="line">            A1 = np.maximum(<span class="number">0</span>, Z1)</span><br><span class="line">            Z2 = np.dot(W2, A1) + b2</span><br><span class="line">            A2 = np.exp(Z2) / sum(np.exp(Z2))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 计算成本：计算交叉熵成本</span></span><br><span class="line">            logprobs = <span class="number">-1</span> * sum(np.multiply(np.log(A2), mini_batch_Y))</span><br><span class="line">            cost = <span class="number">-1</span> / mini_batch_size * np.sum(logprobs)</span><br><span class="line">            cost = np.squeeze(cost)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 反向传播：梯度下降法</span></span><br><span class="line">            dZ2 = A2 - mini_batch_Y</span><br><span class="line">            dW2 = <span class="number">1</span> / mini_batch_size * np.dot(dZ2, A1.T)</span><br><span class="line">            db2 = <span class="number">1</span> / mini_batch_size * np.sum(dZ2, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">            dA1 = np.dot(W2.T, dZ2)</span><br><span class="line">            dZ1 = np.array(dA1, copy=<span class="literal">True</span>)</span><br><span class="line">            dZ1[Z1 &lt;= <span class="number">0</span>] = <span class="number">0</span>  <span class="comment"># 标记一下，操作十分神奇,牛逼！</span></span><br><span class="line">            dW1 = <span class="number">1</span> / mini_batch_size * np.dot(dZ1, mini_batch_X.T)</span><br><span class="line">            db1 = <span class="number">1</span> / mini_batch_size * np.sum(dZ1, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 更新参数</span></span><br><span class="line">            W1 -= learning_rate * dW1</span><br><span class="line">            b1 -= learning_rate * db1</span><br><span class="line">            W2 -= learning_rate * dW2</span><br><span class="line">            b2 -= learning_rate * db2</span><br><span class="line">            <span class="keyword">if</span> print_cost <span class="keyword">and</span> (j+<span class="number">1</span>) % <span class="number">500</span>:</span><br><span class="line">                costs.append(cost)</span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> (i+<span class="number">1</span>) % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Cost after epoch [%3d/%3d]: %f"</span> % (i+<span class="number">1</span>, epoch, cost))</span><br><span class="line"></span><br><span class="line">    plt.plot(costs)</span><br><span class="line">    plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'iterations (per 500)'</span>)</span><br><span class="line">    plt.title(<span class="string">"Learning rate = "</span> + str(learning_rate))</span><br><span class="line">    plt.show()</span><br><span class="line">    <span class="keyword">return</span> W1, b1, W2, b2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">label_test = test[:][<span class="string">"784"</span>]</span><br><span class="line">test1 = test.drop([<span class="string">"784"</span>], axis=<span class="number">1</span>)</span><br><span class="line">X_test = test1.T</span><br><span class="line"><span class="comment"># Y_test = np.eye(10)[l]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line">mini_batch_size = <span class="number">100</span></span><br><span class="line">tic = time.process_time()</span><br><span class="line">W1, b1, W2, b2 = nn_model(X, Y, n_h=<span class="number">1000</span>, mini_batch_size=mini_batch_size, learning_rate=<span class="number">0.1</span>, epoch=<span class="number">100</span>, print_cost=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line">complete_numof_mini_batch_test = X_test.shape[<span class="number">1</span>] // mini_batch_size</span><br><span class="line">accuracy = []</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">0</span>, complete_numof_mini_batch_test):</span><br><span class="line">    mini_batch_X_test = X_test.values[:, (k * mini_batch_size):((k+<span class="number">1</span>) * mini_batch_size)]</span><br><span class="line">    mini_batch_label = label_test.values[(k * mini_batch_size): (k+<span class="number">1</span>) * mini_batch_size]</span><br><span class="line">    Z1 = np.dot(W1, mini_batch_X_test) + b1</span><br><span class="line">    A1 = np.maximum(<span class="number">0</span>, Z1)</span><br><span class="line">    Z2 = np.dot(W2, A1) + b2</span><br><span class="line">    A2 = np.exp(Z2) / sum(np.exp(Z2))</span><br><span class="line">    predictions = A2.argmax(axis=<span class="number">0</span>)</span><br><span class="line">    correct_predictions = np.equal(predictions, mini_batch_label)</span><br><span class="line">    accuracy.append(np.mean(correct_predictions.astype(np.float32)))</span><br><span class="line"></span><br><span class="line">accuracy = np.mean(accuracy)</span><br><span class="line">toc = time.process_time()</span><br><span class="line">print(<span class="string">"Computation time = "</span> + str((toc - tic)) + <span class="string">"s"</span>)</span><br><span class="line">print(<span class="string">'Test Accuracy:%f'</span> % (accuracy * <span class="number">100</span>) + <span class="string">'%'</span>)</span><br></pre></td></tr></table></figure><img src="/2019/11/28/%E7%94%A8numpy%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E5%8F%8C%E5%B1%82%E7%9A%84softmax%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/costs.png" class="" title="cost图"><p>代码与cost如上图，目前改进就是如此了，将X分成100样本一小份，然后逐份进行训练，由于参数共享，所以只有反向传播时涉及到了Y，别处都不需要修改太多。后续具体参数还需要继续调整，敬请期待！</p><p>最后调整好的准确率和时间如下：</p><img src="/2019/11/28/%E7%94%A8numpy%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E5%8F%8C%E5%B1%82%E7%9A%84softmax%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/accuracy.png" class="" title="准确率和时间">]]></content>
      
      
      <categories>
          
          <category> deeplearning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 微机课设 </tag>
            
            <tag> numpy </tag>
            
            <tag> 神经网络 </tag>
            
            <tag> MNIST </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>第一篇博客</title>
      <link href="/2019/11/21/%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/"/>
      <url>/2019/11/21/%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/</url>
      
        <content type="html"><![CDATA[<h1 id="0-最近事务"><a href="#0-最近事务" class="headerlink" title="0. 最近事务"></a>0. 最近事务</h1><h2 id="0-1-平时课程上"><a href="#0-1-平时课程上" class="headerlink" title="0.1 平时课程上"></a>0.1 平时课程上</h2><h3 id="0-1-1-微机原理"><a href="#0-1-1-微机原理" class="headerlink" title="0.1.1 微机原理"></a>0.1.1 微机原理</h3><p>[ ] 需要我<u>读完CAAE的论文</u>简介然后完善好最后的<strong>文献报告</strong><br>[x] 写一下softmax神经网络的python代码<br>[ ] 考完试开始康康汇编，学着用它写神经网络<br>[ ] CS231n的日常学习</p><h3 id="0-1-2-考试开始了"><a href="#0-1-2-考试开始了" class="headerlink" title="0.1.2 考试开始了"></a>0.1.2 考试开始了</h3><p>[ ] 编译原理期末考试</p><p>[x]概率论期末考试</p><hr><h2 id="0-2-项目里面"><a href="#0-2-项目里面" class="headerlink" title="0.2 项目里面"></a>0.2 项目里面</h2><h3 id="0-2-1-专利"><a href="#0-2-1-专利" class="headerlink" title="0.2.1 专利"></a>0.2.1 专利</h3><h3 id="0-2-2-原本代码的复现完善"><a href="#0-2-2-原本代码的复现完善" class="headerlink" title="0.2.2 原本代码的复现完善"></a>0.2.2 原本代码的复现完善</h3><h3 id="0-2-3-读新的论文，并结合来做失踪人口回归的项目"><a href="#0-2-3-读新的论文，并结合来做失踪人口回归的项目" class="headerlink" title="0.2.3 读新的论文，并结合来做失踪人口回归的项目"></a>0.2.3 读新的论文，并结合来做失踪人口回归的项目</h3>]]></content>
      
      
      <categories>
          
          <category> Todo </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 随便写写 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
