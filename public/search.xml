<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Adam的回顾</title>
      <link href="/2019/12/01/Adam%E7%9A%84%E5%9B%9E%E9%A1%BE/"/>
      <url>/2019/12/01/Adam%E7%9A%84%E5%9B%9E%E9%A1%BE/</url>
      
        <content type="html"><![CDATA[<h1 id="加权平均数"><a href="#加权平均数" class="headerlink" title="加权平均数"></a>加权平均数</h1><p>公式：<br>$V_t = \beta_1V_{t-1} + (1-\beta_1)\theta_t$<br>其中V~0~ = 0，所以展开后为：<br>$V_t = \beta_1^{t-1}(1-\beta_1)\theta_1 + \beta_1^{t-2}(1-\beta_1)\theta_2 + … + \beta_1^2(1-\beta_1)\theta_{t-2} + \beta_1(1-\beta_1)\theta_{t-1} + (1-\beta_1)\theta_t \\ = (1-\beta_1)\sum_{i=1}^{t} \beta_1^{t-i}\theta_{i}$</p><p>即$\frac{1}{1-\beta_1}V_t = \sum_{i=1}^{t}\beta_1^{t-i}\theta_i$，其中β~1~是小于1的浮点数，所以在大约$\frac{1}{1-\beta_1}$次方后，$\beta_1^{\frac{1}{1-\beta_1}}$会小于1/e（这个数相当于是一个指数衰减函数，在权重小于1/e是算作较小值），可以逐渐算作忽略不计，则可视作  <strong>V~t~为前$\frac{1}{1-\beta_1}$ 项的加权平均数</strong></p><p>另外，在数据的前期由于其数据比$\frac{1}{1-\beta_{1}}$小得多，所以不能算较多数的加权平均，为了弥补数据量还不足的问题，可以除以偏差修正量$1-\beta_1^t$，而这个量会随着t的增大而逐渐趋向于1，所以不会影响后面的值</p><p>所以最终取的值为：<br>$\frac{V_t}{1-\beta_1^t} = \frac{1-\beta_1}{1-\beta_1^t}\sum_{i=1}^t\beta_1^{t-i}\theta_i$</p><a id="more"></a><h1 id="动量梯度下降法-Momentum"><a href="#动量梯度下降法-Momentum" class="headerlink" title="动量梯度下降法(Momentum)"></a>动量梯度下降法(Momentum)</h1><p>我们一般把梯度下降法比作是在爬山中的下坡，不断地迭代就是为了找出当前更快的下山的路，梯度下降法的迭代计算式为：<br>$\left\{\begin{array}{l}{w = w - \alpha dw} \\ {b = b - \alpha db}\end{array}\right.$</p><p>w和b分别可以表示每次跨的步数多少，和一个跨步的固定最小的值，wx+b可以代表了下山的路线，所以，对于正确的下山路线而言，训练的过程可能是上下摆动较大的，而我们之前说过$\frac{1}{1-\beta_1}$代表了V~t~表示着多少天的加权平均，所以$\beta_1$越大，代表着越多值的平均值，而取较大的$\beta_1$意味着得到更平坦的图像(路线)，所以使用加权平均数可以使得dw和db的变化变得平稳，我们分别用V~dw~、V~db~表示他们使用加权平均后的值，这样可以使得w和b的变化更平稳，则wx+b也会变得更加平坦</p><p>流程：<br>On iteration t:<br>    计算当前小批量的dw, db<br>    $V_{dw} = \beta_1V_{dw} + (1-\beta_1)dw \\ V_{db} = \beta_1V_{db} + (1-\beta_1)db \\ w = w - \alpha V_{dw}, b = b - \alpha V_{db}$</p><p>其中的超参数：α(学习率)，β~1~(控制指数加权平均，最常用0.9)</p><p>Note：由于数据量较大，一般β~1~取10，而前10个数据对于整体数据而言几乎可以忽略不计，所以不必添加偏差修正量</p><h1 id="根均方传播法-RMS-prop"><a href="#根均方传播法-RMS-prop" class="headerlink" title="根均方传播法(RMS prop)"></a>根均方传播法(RMS prop)</h1><p>函数在向数据的正确分布拟合的时候，x的参数w越大，往往能更快的接近正确的分布(当然也不能过大)，而b较大的时候容易过大而走弯路，所以一般不能太大，所以一般来说我们鼓励在学习过程中w变得更大一点而b变得更小一点。由此我们可以使用RMSprop方法来平衡w较小而b较大的情况</p><p>流程：<br>On iteration t:<br>    计算当前小批量的dw, db<br>RMSprop: $S_{dw} = \beta_2 S_{dw} + (1 - \beta_2)(dw)^2 \\ S_{db} = \beta_2 S_{db} + (1- \beta_2)(dw)^2$<br>更新参数：$w = w - \alpha \frac{dw}{\sqrt{S_{dw}} + \epsilon} \\ b = b - \alpha \frac{db}{\sqrt{S_{db}} + \epsilon}$</p><p>其中公式中前后S~dw~， S~db~分别代表前t次，前t-1次迭代累计的dw的平方，相对本次db而言，它们是小的，所以更新后的S~db~也相对db而言是较小数，那么$\frac{db}{\sqrt{S_{db}} + \epsilon}$是一个较大的数，b减去一个较大的数后就可以更新为一个较小数，同理dw也可以更新为一个较大数</p><p>Note:ε的存在一般是为了防止S~dw~、S~db~过小导致算出来的值爆炸，所以加上一个几乎不影响结果的较小值，常用10^-8^、10^-6^</p><h1 id="Adam优化算法-Adaptive-moment-Estimation"><a href="#Adam优化算法-Adaptive-moment-Estimation" class="headerlink" title="Adam优化算法(Adaptive moment Estimation)"></a>Adam优化算法(Adaptive moment Estimation)</h1><p>Adam优化算法就是将RMS传播算法和动量梯度下降法结合起来的算法</p><p>流程为：<br>V~dw~=0, S~dw~=0, V~db~=0,S~db~=0<br>On iteration t:<br>    计算当前小批量的dw, db<br>    $\left.\begin{array}{l}{V_{dw} = \beta_1 V_{dw} + (1-\beta_1)dw} \\<br>     {V_{db} = \beta_1 V_{db} + (1-\beta_1)db }\end{array}\right\}$”Momentum”动量梯度下降<br>    $\left.\begin{array}{l}{S_{dw}= \beta_2 S_{dw} + (1-\beta_2)(dw)^2} \\<br>     {S_{db}=\beta_2 S_{dw} + (1 -\beta_2)(db)^2}\end{array}\right\}$”RMSprop”根均方传播算法<br>偏差修正：<br>    $V^{corrected}_{dw} = \frac{V_{dw}}{1-\beta_1^t} \\<br>     V^{corrected}_{db} = \frac{V_{db}}{1-\beta_1^t} \\<br>     S^{corrected}_{dw} = \frac{S_{dw}}{1-\beta_2^t} \\<br>     S^{corrected}_{db} = \frac{S_{db}}{1-\beta_2^t}$<br>更新参数：$w = w - \alpha \frac{V^{corrected}_{dw}}{\sqrt{S^{corrected}_{dw}} + \epsilon} \\<br>         b = b-\alpha \frac{V^{corrected}_{db}}{\sqrt{S^{corrected}_{db}} +\epsilon}$</p><p>超参数：<br>α：学习率，需要调试<br>β~1~：常用0.9            （dw的加权平均数，第一<em>矩（待了解）</em>）<br>β~2~：推荐0.999        （dw^2^的加权平均数，第二矩）<br>ε：10^-8^                    （不需要太在意）</p><p>总而言之，Adam优化算法就是将RMS传播算法和动量梯度下降法结合在一起，前者可以减少偏差以加快学习，后者可以使图像更加平坦，从而加快学习。</p>]]></content>
      
      
      <categories>
          
          <category> deeplearning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deeplearning </tag>
            
            <tag> 优化器 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CS231n_Week1_Assignment1</title>
      <link href="/2019/11/30/CS231n-Week1-Assignment1/"/>
      <url>/2019/11/30/CS231n-Week1-Assignment1/</url>
      
        <content type="html"><![CDATA[<h1 id="作业："><a href="#作业：" class="headerlink" title="作业："></a>作业：</h1><p>(阅读理解题)</p><h2 id="1-图像的数据主要来源有哪些"><a href="#1-图像的数据主要来源有哪些" class="headerlink" title="1. 图像的数据主要来源有哪些"></a>1. 图像的数据主要来源有哪些</h2><p>照相机，手机，摄像头等一系列视觉传感器设备生产出了大量的图像数据</p><h2 id="2-sift-feature，金字塔匹配思想和hog特征分别是什么，可以用来干什么"><a href="#2-sift-feature，金字塔匹配思想和hog特征分别是什么，可以用来干什么" class="headerlink" title="2. sift feature，金字塔匹配思想和hog特征分别是什么，可以用来干什么"></a>2. sift feature，金字塔匹配思想和hog特征分别是什么，可以用来干什么</h2><ol><li><strong>sift feature：</strong>确认目标上在变化中具有表现型和不变性的特征，然后把这些特征与相似的目标进行匹配<a id="more"></a></li><li><strong>金字塔匹配思想：</strong>从图片的各部分的各个像素抽取特征并把它们放到一起作为一个特征描述符，然后再特征描述符上做一个支持向量机</li><li><strong>hog特征：</strong>histogram of gradients方向梯度直方图，将一堆特征放在一起后，研究如何在实际图片中比较合理的设计人体姿态和辨认人体姿态</li></ol><h2 id="3-神经网络早就存在为什么最近才兴起"><a href="#3-神经网络早就存在为什么最近才兴起" class="headerlink" title="3. 神经网络早就存在为什么最近才兴起"></a>3. 神经网络早就存在为什么最近才兴起</h2><ol><li>算力的提升</li><li>高质量的数据集的数量增多</li></ol><h2 id="4-图像任务有哪些，解决什么样的图像问题"><a href="#4-图像任务有哪些，解决什么样的图像问题" class="headerlink" title="4. 图像任务有哪些，解决什么样的图像问题"></a>4. 图像任务有哪些，解决什么样的图像问题</h2><p>图像分类，目标检测，图像描述</p>]]></content>
      
      
      <categories>
          
          <category> CS231n </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CS231n </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>用numpy实现一个双层的softmax神经网络</title>
      <link href="/2019/11/28/%E7%94%A8numpy%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E5%8F%8C%E5%B1%82%E7%9A%84softmax%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
      <url>/2019/11/28/%E7%94%A8numpy%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E5%8F%8C%E5%B1%82%E7%9A%84softmax%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<h1 id="整体思路"><a href="#整体思路" class="headerlink" title="整体思路"></a>整体思路</h1><p>使用Numpy和pandas写一个神经网络，进行手写数字识别(MNIST)，样本图如下：</p><img src="/2019/11/28/%E7%94%A8numpy%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E5%8F%8C%E5%B1%82%E7%9A%84softmax%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/output_13_0.png" class="" title="手写数字5"><h1 id="总思路"><a href="#总思路" class="headerlink" title="总思路"></a>总思路</h1><p>整体神经网络代码只使用Numpy和pandas，根据额外需要使用time和matplotlib，设计思路如下</p><p>数据处理-&gt; 建立模型 -&gt; 进行训练 -&gt; 进行测试 -&gt; 评估时间和精度<br><a id="more"></a></p><h1 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h1><p>数据分为训练集和测试集，分别是785*6w和785*1w的数据，785列中前784列代表了28*28的图片的数值化数据，第785列为0~9的标签，并且第一行为0~784的索引</p><ol><li>使用pandas读取训练和测试数据</li><li>分别把训练和测试数据中的数据和标签分离</li><li>将标签进行one-hot编码</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练集有6w, 785列，前784为28*28,最后一列为标签</span></span><br><span class="line">train = pd.read_csv(<span class="string">"MNIST_Training_60K.csv"</span>,index_col=<span class="literal">False</span>,header=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 测试集有1w，784列，28*28的图片</span></span><br><span class="line">test = pd.read_csv(<span class="string">"MNIST_Test_10K.csv"</span>,index_col=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">label = train[:][<span class="string">"784"</span>]</span><br><span class="line">train = train.drop([<span class="string">"784"</span>],axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">X = train.T</span><br><span class="line">Y = np.zeros((<span class="number">10</span>, X.shape[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以下为成像代码</span></span><br><span class="line"><span class="comment"># import matplotlib.pyplot as plt</span></span><br><span class="line"><span class="comment"># x = np.array(X)</span></span><br><span class="line"><span class="comment"># X_train = x.reshape((60000,28,28))</span></span><br><span class="line"><span class="comment"># w,h=28,28</span></span><br><span class="line"><span class="comment"># totalNum=X_train.shape[0]</span></span><br><span class="line"><span class="comment"># cur=X_train[0:1]</span></span><br><span class="line"><span class="comment"># cnt=0</span></span><br><span class="line"><span class="comment"># fig, axs = plt.subplots(1)</span></span><br><span class="line"><span class="comment"># axs.imshow(cur[cnt].reshape(h,w).T,cmap='gray')</span></span><br><span class="line"><span class="comment"># axs.axis('off')</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将标签one_hot编码</span></span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> range(X.shape[<span class="number">1</span>]):</span><br><span class="line">    Y[label[n]][n] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 处理test数据</span></span><br><span class="line">label_test = test[:][<span class="string">"784"</span>]</span><br><span class="line">test1 = test.drop([<span class="string">"784"</span>],axis=<span class="number">1</span>)</span><br><span class="line">X_test = test1.T</span><br><span class="line"><span class="comment"># Y_test = np.eye(10)[l]</span></span><br></pre></td></tr></table></figure><h1 id="建立模型"><a href="#建立模型" class="headerlink" title="建立模型"></a>建立模型</h1><h2 id="正向传播"><a href="#正向传播" class="headerlink" title="正向传播"></a>正向传播</h2><ol><li><p>参数初始化</p><p>采用”He”初始化，即使用标准化的服从高斯分布的随机数乘上 2/前一层网络单元数的根号幂初始化权重W，并且使用0初始化对偏差b进行初始化</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">(n_x, n_h, n_y)</span>:</span></span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    W1 = np.random.randn(n_h, n_x) * np.sqrt(<span class="number">2.</span> / n_x)</span><br><span class="line">    b1 = np.zeros((n_h, <span class="number">1</span>))</span><br><span class="line">    W2 = np.random.randn(n_y, n_h)  * np.sqrt(<span class="number">2.</span> / n_h)</span><br><span class="line">    b2 = np.zeros((n_y, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 断言保证这些参数的形状如下，否则报错</span></span><br><span class="line">    <span class="keyword">assert</span>(W1.shape == (n_h, n_x))</span><br><span class="line">    <span class="keyword">assert</span>(b1.shape == (n_h,   <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">assert</span>(W2.shape == (n_y, n_h))</span><br><span class="line">    <span class="keyword">assert</span>(b2.shape == (n_y,   <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将参数封装</span></span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>:W1,</span><br><span class="line">                  <span class="string">"b1"</span>:b1,</span><br><span class="line">                  <span class="string">"W2"</span>:W2,</span><br><span class="line">                  <span class="string">"b2"</span>:b2&#125;</span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>注意：</p><p>权重w本来的维度应该是(前一层神经元数，本层神经元数)，但是为了方便后续计算省略掉此处的转置操作，在一开始就选择将W的维度设置为(本层神经元数，前一层大小神经元数)</p><p>而偏差b由于需要整层网络共享，我们基于Python的广播机制，选择让它的大小设置为(本层神经元数，1)</p></li></ol><ol><li><p>正向线性传播</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_forward</span><span class="params">(A0, W1, b1)</span>:</span></span><br><span class="line">    Z1 = np.dot(W1, A0) + b1</span><br><span class="line">    <span class="comment"># 即Z1 = W1 * A0 + b1</span></span><br><span class="line">    <span class="keyword">assert</span>(Z1.shape == (W1.shape[<span class="number">0</span>], A0.shape[<span class="number">1</span>]))</span><br><span class="line">    </span><br><span class="line">    cache = (A0, W1, b1)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> Z1, cache</span><br></pre></td></tr></table></figure><p>无特别说明</p></li></ol><ol><li><p>正向激活</p><p>隐藏层使用ReLU(整流线性单元)函数，输出层使用softmax函数，期望它的值符合0~9十个分类。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu</span><span class="params">(Z1)</span>:</span></span><br><span class="line">    A1 = np.maximum(<span class="number">0</span>, Z1) </span><br><span class="line">    cache = Z1</span><br><span class="line">    <span class="keyword">return</span> A1, cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(Z2)</span>:</span></span><br><span class="line">    A2 = np.exp(Z2) / sum(np.exp(Z2))</span><br><span class="line">    </span><br><span class="line">    cache = Z2</span><br><span class="line">    <span class="keyword">return</span> A2, cache</span><br></pre></td></tr></table></figure></li></ol><ol><li><p>正向传播函数</p><p>将线性传播部分和激活函数合并</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_forward_relu</span><span class="params">(A0, W1, b1)</span>:</span></span><br><span class="line">    Z1, linear_cache = linear_forward(A0, W1, b1)</span><br><span class="line">    A1, activation_cache = relu(Z1)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(A1.shape == (W.shape[<span class="number">0</span>], A0.shape[<span class="number">1</span>]))</span><br><span class="line">    cache = (linear_cache, activation_cache)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> A1, cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_forward_softmax</span><span class="params">(A1, W2, b2)</span>:</span></span><br><span class="line">    Z2, linear_cache = linear_forward(A1, W2, b2)</span><br><span class="line">    A2, activation_cache = softmax(Z2)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(A2.shape == (W2.shape[<span class="number">0</span>], A1.shape[<span class="number">1</span>]))</span><br><span class="line">    cache = (linear_cache, activation_cache)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> A2, cache</span><br></pre></td></tr></table></figure></li></ol><h2 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h2><ol><li><p>计算成本</p><p>使用交叉熵作为成本函数，其函数的公式为：</p><script type="math/tex; mode=display">\sum_{i}^{m} y_i\log{\hat{y_{i}}}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(A2, Y)</span>:</span></span><br><span class="line">    <span class="comment"># 计算交叉熵</span></span><br><span class="line">    m = Y.shape[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算loss函数</span></span><br><span class="line">    logprobs = <span class="number">-1</span>* sum(np.multiply(np.log(A2), Y))</span><br><span class="line">    <span class="comment"># 成本函数就是将整个训练集上的损失相加取相反数</span></span><br><span class="line">    cost = <span class="number">-1</span>/m * np.sum(logprobs)</span><br><span class="line">    cost = np.squeeze(cost)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cost, logprobs</span><br></pre></td></tr></table></figure></li></ol><ol><li><p>反向传播</p><p>计算各个参数基于成本的梯度，用于更新参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_propagation</span><span class="params">(parameters, cache, X, Y)</span>:</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    W1 = parameters[<span class="string">'W1'</span>]</span><br><span class="line">    W2 = parameters[<span class="string">'W2'</span>]</span><br><span class="line">    </span><br><span class="line">    A1 = cache[<span class="string">'A1'</span>]</span><br><span class="line">    A2 = cache[<span class="string">'A2'</span>]</span><br><span class="line">    </span><br><span class="line">    dZ2 = A2 - Y</span><br><span class="line">    dW2 = <span class="number">1</span>/m * np.dot(dZ2, A1.T)</span><br><span class="line">    db2 = <span class="number">1</span>/m * np.sum(dZ2, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    dA1 = np.dot(W2.T, dZ2)</span><br><span class="line">    dZ1 = np.array(dA1, copy=<span class="literal">True</span>)</span><br><span class="line">    dZ1[Z1 &lt;=<span class="number">0</span> ] = <span class="number">0</span> <span class="comment"># 标记一下，操作十分神奇,牛逼！</span></span><br><span class="line">    dW1 = <span class="number">1</span>/m * np.dot(dZ1, X.T)</span><br><span class="line">    db1 = <span class="number">1</span>/m * np.sum(dZ1, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    grads=&#123;<span class="string">"dW1"</span>:dW1,</span><br><span class="line">           <span class="string">"db1"</span>:db1,</span><br><span class="line">           <span class="string">"dW2"</span>:dW2,</span><br><span class="line">           <span class="string">"db2"</span>:db2&#125;</span><br><span class="line">    <span class="keyword">return</span> grads</span><br></pre></td></tr></table></figure><p>各参数传播的顺序和算式如下：</p><img src="/2019/11/28/%E7%94%A8numpy%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E5%8F%8C%E5%B1%82%E7%9A%84softmax%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/propagation.jpg" class="" title="参数传播"></li><li><p>更新参数</p><p>使用mini-batch梯度下降法更新参数，目前仅实现了梯度下降法，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span><span class="params">(parameters, grads, learning_rate=<span class="number">1.2</span>)</span>:</span></span><br><span class="line">    W1 = parameters[<span class="string">'W1'</span>]</span><br><span class="line">    b1 = parameters[<span class="string">'b1'</span>]</span><br><span class="line">    W2 = parameters[<span class="string">'W2'</span>]</span><br><span class="line">    b2 = parameters[<span class="string">'b2'</span>]</span><br><span class="line">    </span><br><span class="line">    dW1 = grads[<span class="string">'dW1'</span>]</span><br><span class="line">    db1 = grads[<span class="string">'db1'</span>]</span><br><span class="line">    dW2 = grads[<span class="string">'dW2'</span>]</span><br><span class="line">    db2 = grads[<span class="string">'db2'</span>]</span><br><span class="line">    </span><br><span class="line">    W1 = W1 - learning_rate* dW1</span><br><span class="line">    b1 = b1 - learning_rate* db1</span><br><span class="line">    W2 = W2 - learning_rate* dW2</span><br><span class="line">    b2 = b2 - learning_rate* db2</span><br><span class="line">    </span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</span><br><span class="line">                  <span class="string">"b1"</span>: b1,</span><br><span class="line">                  <span class="string">"W2"</span>: W2,</span><br><span class="line">                  <span class="string">"b2"</span>: b2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure></li></ol><h2 id="整体模型"><a href="#整体模型" class="headerlink" title="整体模型"></a>整体模型</h2><p>将上述函数等无缝合并，方便转换</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nn_model</span><span class="params">(X, Y, n_h=<span class="number">1000</span>, learning_rate=<span class="number">0.12</span> ,num_iterations=<span class="number">10000</span>, print_cost=False)</span>:</span></span><br><span class="line">    n_x = <span class="number">784</span></span><br><span class="line">    n_h = <span class="number">1000</span></span><br><span class="line">    n_y = <span class="number">10</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 随机初始化参数</span></span><br><span class="line">    np.random.seed(<span class="number">3</span>)</span><br><span class="line">    W1 = np.random.randn(n_h, n_x) * np.sqrt(<span class="number">2.</span> / n_x)</span><br><span class="line">    b1 = np.zeros((n_h, <span class="number">1</span>))</span><br><span class="line">    W2 = np.random.randn(n_y, n_h)  * np.sqrt(<span class="number">2.</span> / n_h)</span><br><span class="line">    b2 = np.zeros((n_y, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line">        <span class="comment"># 正向传播: 线性-&gt; ReLU -&gt; 线性-&gt; softmax</span></span><br><span class="line">        Z1 = np.dot(W1, X) + b1</span><br><span class="line">        A1 = np.maximum(<span class="number">0</span>, Z1)</span><br><span class="line">        Z2 = np.dot(W2, A1) + b2</span><br><span class="line">        A2 = np.exp(Z2) / sum(np.exp(Z2))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算成本：计算交叉熵成本</span></span><br><span class="line">        logprobs = <span class="number">-1</span>* sum(np.multiply(np.log(A2), Y))</span><br><span class="line">        cost = <span class="number">-1</span>/m * np.sum(logprobs)</span><br><span class="line">        cost = np.squeeze(cost)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 反向传播：梯度下降法</span></span><br><span class="line">        dZ2 = A2 - Y</span><br><span class="line">        dW2 = <span class="number">1</span>/m * np.dot(dZ2, A1.T)</span><br><span class="line">        db2 = <span class="number">1</span>/m * np.sum(dZ2, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">        dA1 = np.dot(W2.T, dZ2)</span><br><span class="line">        dZ1 = np.array(dA1, copy=<span class="literal">True</span>)</span><br><span class="line">        dZ1[Z1 &lt;=<span class="number">0</span> ] = <span class="number">0</span> <span class="comment"># 标记一下，操作十分神奇,牛逼！</span></span><br><span class="line">        dW1 = <span class="number">1</span>/m * np.dot(dZ1, X.T)</span><br><span class="line">        db1 = <span class="number">1</span>/m * np.sum(dZ1, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 更新参数</span></span><br><span class="line">        W1 -= learning_rate * dW1</span><br><span class="line">        b1 -= learning_rate * db1</span><br><span class="line">        W2 -= learning_rate * dW2</span><br><span class="line">        b2 -= learning_rate * db2</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Cost after iteration %i: %f"</span> %(i, cost))</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> W1, b1, W2, b2</span><br></pre></td></tr></table></figure><p>此处只为保留一下未mini-batch版本</p><h1 id="训练测试"><a href="#训练测试" class="headerlink" title="训练测试"></a>训练测试</h1><p>预测函数如下，即正向传播过程</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(X, W1, b1, W2, b2)</span>:</span></span><br><span class="line">    Z1 = np.dot(W1, X) + b1</span><br><span class="line">    A1 = np.maximum(<span class="number">0</span>, Z1)</span><br><span class="line">    Z2 = np.dot(W2, A1) + b2</span><br><span class="line">    A2 = np.exp(Z2) / sum(np.exp(Z2))</span><br><span class="line">    predictions = A2.argmax(axis=<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 使用argmax使得返回值predictions为和label相同的0~9标签</span></span><br><span class="line">    <span class="keyword">return</span> predictions</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line">tic = time.process_time()</span><br><span class="line">W1, b1, W2, b2= nn_model(X, Y,n_h=<span class="number">300</span>, learning_rate=<span class="number">0.08</span>, num_iterations=<span class="number">10000</span>, print_cost=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line">predictions = predict(X_test, W1, b1, W2, b2)</span><br><span class="line">toc = time.process_time()</span><br><span class="line"><span class="comment"># 时间</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Computation time = "</span> + str((toc - tic)) + <span class="string">"s"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 评估精确度</span></span><br><span class="line">correct_predictions = np.equal(predictions, label_test)</span><br><span class="line">accuracy = np.mean(correct_predictions.astype(np.float32))</span><br><span class="line">print(<span class="string">'Test Accuracy:%f'</span>%(accuracy*<span class="number">100</span>) + <span class="string">'%'</span>)</span><br></pre></td></tr></table></figure><h1 id="所有代码合并"><a href="#所有代码合并" class="headerlink" title="所有代码合并"></a>所有代码合并</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="comment"># import matplotlib.pyplot as plt</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练集有6w, 785列，前784为28*28,最后一列为标签</span></span><br><span class="line">train = pd.read_csv(<span class="string">"MNIST_Training_60K.csv"</span>,index_col=<span class="literal">False</span>,header=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 测试集有1w，784列，28*28的图片</span></span><br><span class="line">test = pd.read_csv(<span class="string">"MNIST_Test_10K.csv"</span>,index_col=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">label = train[:][<span class="string">"784"</span>]</span><br><span class="line">train = train.drop([<span class="string">"784"</span>],axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">X = train.T</span><br><span class="line">Y = np.zeros((<span class="number">10</span>, X.shape[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> range(X.shape[<span class="number">1</span>]):</span><br><span class="line">    Y[label[n]][n] = <span class="number">1</span></span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nn_model</span><span class="params">(X, Y, n_h=<span class="number">1000</span>, learning_rate=<span class="number">0.12</span> ,num_iterations=<span class="number">10000</span>, print_cost=False)</span>:</span></span><br><span class="line">    n_x = <span class="number">784</span></span><br><span class="line">    n_y = <span class="number">10</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 随机初始化参数</span></span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    W1 = np.random.randn(n_h, n_x) * np.sqrt(<span class="number">2.</span> / n_x)</span><br><span class="line">    b1 = np.zeros((n_h, <span class="number">1</span>))</span><br><span class="line">    W2 = np.random.randn(n_y, n_h)  * np.sqrt(<span class="number">2.</span> / n_h)</span><br><span class="line">    b2 = np.zeros((n_y, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line">        <span class="comment"># 正向传播: 线性-&gt; ReLU -&gt; 线性-&gt; softmax</span></span><br><span class="line">        Z1 = np.dot(W1, X) + b1</span><br><span class="line">        A1 = np.maximum(<span class="number">0</span>, Z1)</span><br><span class="line">        Z2 = np.dot(W2, A1) + b2</span><br><span class="line">        A2 = np.exp(Z2) / sum(np.exp(Z2))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算成本：计算交叉熵成本</span></span><br><span class="line">        logprobs = <span class="number">-1</span>* sum(np.multiply(np.log(A2), Y))</span><br><span class="line">        cost = <span class="number">-1</span>/m * np.sum(logprobs)</span><br><span class="line">        cost = np.squeeze(cost)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 反向传播：梯度下降法</span></span><br><span class="line">        dZ2 = A2 - Y</span><br><span class="line">        dW2 = <span class="number">1</span>/m * np.dot(dZ2, A1.T)</span><br><span class="line">        db2 = <span class="number">1</span>/m * np.sum(dZ2, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">        dA1 = np.dot(W2.T, dZ2)</span><br><span class="line">        dZ1 = np.array(dA1, copy=<span class="literal">True</span>)</span><br><span class="line">        dZ1[Z1 &lt;=<span class="number">0</span> ] = <span class="number">0</span> <span class="comment"># 标记一下，操作十分神奇,牛逼！</span></span><br><span class="line">        dW1 = <span class="number">1</span>/m * np.dot(dZ1, X.T)</span><br><span class="line">        db1 = <span class="number">1</span>/m * np.sum(dZ1, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 更新参数</span></span><br><span class="line">        W1 -= learning_rate * dW1</span><br><span class="line">        b1 -= learning_rate * db1</span><br><span class="line">        W2 -= learning_rate * dW2</span><br><span class="line">        b2 -= learning_rate * db2</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Cost after iteration %i: %f"</span> %(i, cost))</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> W1, b1, W2, b2</span><br><span class="line"></span><br><span class="line">label_test = test[:][<span class="string">"784"</span>]</span><br><span class="line">test1 = test.drop([<span class="string">"784"</span>],axis=<span class="number">1</span>)</span><br><span class="line">X_test = test1.T</span><br><span class="line"><span class="comment"># Y_test = np.eye(10)[l]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line">tic = time.process_time()</span><br><span class="line">W1, b1, W2, b2= nn_model(X, Y,n_h=<span class="number">300</span>, learning_rate=<span class="number">0.08</span>, num_iterations=<span class="number">10000</span>, print_cost=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line">Z1 = np.dot(W1, X_test) + b1</span><br><span class="line">A1 = np.maximum(<span class="number">0</span>, Z1)</span><br><span class="line">Z2 = np.dot(W2, A1) + b2</span><br><span class="line">A2 = np.exp(Z2) / sum(np.exp(Z2))</span><br><span class="line">predictions = A2.argmax(axis=<span class="number">0</span>)</span><br><span class="line">toc = time.process_time()</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Computation time = "</span> + str((toc - tic)) + <span class="string">"s"</span>)</span><br><span class="line"></span><br><span class="line">correct_predictions = np.equal(predictions, label_test)</span><br><span class="line">accuracy = np.mean(correct_predictions.astype(np.float32))</span><br><span class="line">print(<span class="string">'Test Accuracy:%f'</span>%(accuracy*<span class="number">100</span>) + <span class="string">'%'</span>)</span><br></pre></td></tr></table></figure><h1 id="添加mini-batch"><a href="#添加mini-batch" class="headerlink" title="添加mini-batch"></a>添加mini-batch</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练集有6w, 785列，前784为28*28,最后一列为标签</span></span><br><span class="line">train = pd.read_csv(<span class="string">"./MNIST_Training_60K.csv"</span>, index_col=<span class="literal">False</span>, header=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 测试集有1w，784列，28*28的图片</span></span><br><span class="line">test = pd.read_csv(<span class="string">"./MNIST_Test_10K.csv"</span>, index_col=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">label = train[:][<span class="string">"784"</span>]</span><br><span class="line">train = train.drop([<span class="string">"784"</span>], axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">X = train.T</span><br><span class="line">Y = np.zeros((<span class="number">10</span>, X.shape[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> range(X.shape[<span class="number">1</span>]):</span><br><span class="line">    Y[label[n]][n] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nn_model</span><span class="params">(X, Y, n_h=<span class="number">1000</span>, mini_batch_size=<span class="number">100</span>, learning_rate=<span class="number">0.08</span>, epoch=<span class="number">1000</span>, print_cost=False)</span>:</span></span><br><span class="line">    n_x = <span class="number">784</span></span><br><span class="line">    n_y = <span class="number">10</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 随机初始化参数</span></span><br><span class="line">    np.random.seed(<span class="number">3</span>)</span><br><span class="line">    W1 = np.random.randn(n_h, n_x) * np.sqrt(<span class="number">2.</span> / n_x)</span><br><span class="line">    b1 = np.zeros((n_h, <span class="number">1</span>))</span><br><span class="line">    W2 = np.random.randn(n_y, n_h) * np.sqrt(<span class="number">2.</span> / n_h)</span><br><span class="line">    b2 = np.zeros((n_y, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    costs=[]</span><br><span class="line">    complete_numof_mini_batch = m // mini_batch_size</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, epoch):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, complete_numof_mini_batch):</span><br><span class="line">            <span class="comment"># 正向传播: 线性-&gt; ReLU -&gt; 线性-&gt; softmax</span></span><br><span class="line">            mini_batch_X = X.values[:, (j * mini_batch_size): (j + <span class="number">1</span>) * mini_batch_size]</span><br><span class="line">            mini_batch_Y = Y[:, (j * mini_batch_size): (j + <span class="number">1</span>) * mini_batch_size]</span><br><span class="line">            Z1 = np.dot(W1, mini_batch_X) + b1</span><br><span class="line">            A1 = np.maximum(<span class="number">0</span>, Z1)</span><br><span class="line">            Z2 = np.dot(W2, A1) + b2</span><br><span class="line">            A2 = np.exp(Z2) / sum(np.exp(Z2))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 计算成本：计算交叉熵成本</span></span><br><span class="line">            logprobs = <span class="number">-1</span> * sum(np.multiply(np.log(A2), mini_batch_Y))</span><br><span class="line">            cost = <span class="number">-1</span> / mini_batch_size * np.sum(logprobs)</span><br><span class="line">            cost = np.squeeze(cost)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 反向传播：梯度下降法</span></span><br><span class="line">            dZ2 = A2 - mini_batch_Y</span><br><span class="line">            dW2 = <span class="number">1</span> / mini_batch_size * np.dot(dZ2, A1.T)</span><br><span class="line">            db2 = <span class="number">1</span> / mini_batch_size * np.sum(dZ2, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">            dA1 = np.dot(W2.T, dZ2)</span><br><span class="line">            dZ1 = np.array(dA1, copy=<span class="literal">True</span>)</span><br><span class="line">            dZ1[Z1 &lt;= <span class="number">0</span>] = <span class="number">0</span>  <span class="comment"># 标记一下，操作十分神奇,牛逼！</span></span><br><span class="line">            dW1 = <span class="number">1</span> / mini_batch_size * np.dot(dZ1, mini_batch_X.T)</span><br><span class="line">            db1 = <span class="number">1</span> / mini_batch_size * np.sum(dZ1, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 更新参数</span></span><br><span class="line">            W1 -= learning_rate * dW1</span><br><span class="line">            b1 -= learning_rate * db1</span><br><span class="line">            W2 -= learning_rate * dW2</span><br><span class="line">            b2 -= learning_rate * db2</span><br><span class="line">            <span class="keyword">if</span> print_cost <span class="keyword">and</span> (j+<span class="number">1</span>) % <span class="number">500</span>:</span><br><span class="line">                costs.append(cost)</span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> (i+<span class="number">1</span>) % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Cost after epoch [%3d/%3d]: %f"</span> % (i+<span class="number">1</span>, epoch, cost))</span><br><span class="line"></span><br><span class="line">    plt.plot(costs)</span><br><span class="line">    plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'iterations (per 500)'</span>)</span><br><span class="line">    plt.title(<span class="string">"Learning rate = "</span> + str(learning_rate))</span><br><span class="line">    plt.show()</span><br><span class="line">    <span class="keyword">return</span> W1, b1, W2, b2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">label_test = test[:][<span class="string">"784"</span>]</span><br><span class="line">test1 = test.drop([<span class="string">"784"</span>], axis=<span class="number">1</span>)</span><br><span class="line">X_test = test1.T</span><br><span class="line"><span class="comment"># Y_test = np.eye(10)[l]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line">mini_batch_size = <span class="number">100</span></span><br><span class="line">tic = time.process_time()</span><br><span class="line">W1, b1, W2, b2 = nn_model(X, Y, n_h=<span class="number">1000</span>, mini_batch_size=mini_batch_size, learning_rate=<span class="number">0.1</span>, epoch=<span class="number">100</span>, print_cost=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line">complete_numof_mini_batch_test = X_test.shape[<span class="number">1</span>] // mini_batch_size</span><br><span class="line">accuracy = []</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">0</span>, complete_numof_mini_batch_test):</span><br><span class="line">    mini_batch_X_test = X_test.values[:, (k * mini_batch_size):((k+<span class="number">1</span>) * mini_batch_size)]</span><br><span class="line">    mini_batch_label = label_test.values[(k * mini_batch_size): (k+<span class="number">1</span>) * mini_batch_size]</span><br><span class="line">    Z1 = np.dot(W1, mini_batch_X_test) + b1</span><br><span class="line">    A1 = np.maximum(<span class="number">0</span>, Z1)</span><br><span class="line">    Z2 = np.dot(W2, A1) + b2</span><br><span class="line">    A2 = np.exp(Z2) / sum(np.exp(Z2))</span><br><span class="line">    predictions = A2.argmax(axis=<span class="number">0</span>)</span><br><span class="line">    correct_predictions = np.equal(predictions, mini_batch_label)</span><br><span class="line">    accuracy.append(np.mean(correct_predictions.astype(np.float32)))</span><br><span class="line"></span><br><span class="line">accuracy = np.mean(accuracy)</span><br><span class="line">toc = time.process_time()</span><br><span class="line">print(<span class="string">"Computation time = "</span> + str((toc - tic)) + <span class="string">"s"</span>)</span><br><span class="line">print(<span class="string">'Test Accuracy:%f'</span> % (accuracy * <span class="number">100</span>) + <span class="string">'%'</span>)</span><br></pre></td></tr></table></figure><img src="/2019/11/28/%E7%94%A8numpy%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E5%8F%8C%E5%B1%82%E7%9A%84softmax%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/costs.png" class="" title="cost图"><p>代码与cost如上图，目前改进就是如此了，将X分成100样本一小份，然后逐份进行训练，由于参数共享，所以只有反向传播时涉及到了Y，别处都不需要修改太多。后续具体参数还需要继续调整，敬请期待！</p><p>最后调整好的准确率和时间如下：</p><img src="/2019/11/28/%E7%94%A8numpy%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E5%8F%8C%E5%B1%82%E7%9A%84softmax%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/accuracy.png" class="" title="准确率和时间">]]></content>
      
      
      <categories>
          
          <category> deeplearning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 微机课设 </tag>
            
            <tag> numpy </tag>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>第一篇博客</title>
      <link href="/2019/11/21/%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/"/>
      <url>/2019/11/21/%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/</url>
      
        <content type="html"><![CDATA[<h1 id="0-最近事务"><a href="#0-最近事务" class="headerlink" title="0. 最近事务"></a>0. 最近事务</h1><h2 id="0-1-平时课程上"><a href="#0-1-平时课程上" class="headerlink" title="0.1 平时课程上"></a>0.1 平时课程上</h2><h3 id="0-1-1-微机原理"><a href="#0-1-1-微机原理" class="headerlink" title="0.1.1 微机原理"></a>0.1.1 微机原理</h3><p>[ ] 需要我<u>读完CAAE的论文</u>简介然后完善好最后的<strong>文献报告</strong><br>[x] 写一下softmax神经网络的python代码<br>[ ] 考完试开始康康汇编，学着用它写神经网络<br>[ ] CS231n的日常学习</p><h3 id="0-1-2-考试开始了"><a href="#0-1-2-考试开始了" class="headerlink" title="0.1.2 考试开始了"></a>0.1.2 考试开始了</h3><p>[ ] 编译原理期末考试</p><p>[x]概率论期末考试</p><hr><h2 id="0-2-项目里面"><a href="#0-2-项目里面" class="headerlink" title="0.2 项目里面"></a>0.2 项目里面</h2><h3 id="0-2-1-专利"><a href="#0-2-1-专利" class="headerlink" title="0.2.1 专利"></a>0.2.1 专利</h3><h3 id="0-2-2-原本代码的复现完善"><a href="#0-2-2-原本代码的复现完善" class="headerlink" title="0.2.2 原本代码的复现完善"></a>0.2.2 原本代码的复现完善</h3><h3 id="0-2-3-读新的论文，并结合来做失踪人口回归的项目"><a href="#0-2-3-读新的论文，并结合来做失踪人口回归的项目" class="headerlink" title="0.2.3 读新的论文，并结合来做失踪人口回归的项目"></a>0.2.3 读新的论文，并结合来做失踪人口回归的项目</h3>]]></content>
      
      
      <categories>
          
          <category> Todo </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 随便写写 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
